{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PortfolioConstructor import PortfolioConstructor\n",
    "from ExchnageEnv import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda') \n",
    "    torch.get_default_device()\n",
    "    device = 'cuda'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/naradaw/dev/Charles_Schwab/data/symbol_universe/snp_unique_100_2019\", \"rb\") as fp:\n",
    "    symbol_universe = pickle.load(fp)\n",
    "    \n",
    "symbol_universe = symbol_universe[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_path = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v1/2024_10_31/feature_set_2024_10_31_11_18.pkl\"\n",
    "\n",
    "with open(feature_set_path, 'rb') as f:\n",
    "    feature_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v1/2024_10_31/dataset_sqs_2024_10_31_11_18.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/naradaw/dev/Charles_Schwab/code/RnD/v2/mlflow_experiments/930648686917041142', creation_time=1730715551772, experiment_id='930648686917041142', last_update_time=1730715551772, lifecycle_stage='active', name='/portfolio-contructor-v2', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('file:/home/naradaw/dev/Charles_Schwab/code/RnD/v2/mlflow_experiments')\n",
    "\n",
    "mlflow.set_experiment(\"/portfolio-contructor-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "eval_step = 8\n",
    "train_step = 4\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "symbol_universe= symbol_universe\n",
    "num_features= len(feature_set)\n",
    "d_model = 88\n",
    "nheads = 2\n",
    "num_transformer_layers = 2\n",
    "\n",
    "episode_duration= 12   \n",
    "holding_period = 1\n",
    "train_test_split= 0.8\n",
    "symbol_universe = symbol_universe\n",
    "feature_set= feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_universe = random.choices(symbol_universe, k = 20)\n",
    "# symbol_universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "sharpe ratio measures the excess return of the portfolio over the \n",
    "volatility of it -> risk adjusted performance\n",
    "'''\n",
    "\n",
    "\n",
    "def sharp_ratio_(rewards, tran_costs):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharp_ratio_loss_(rewards, tran_costs, allocations):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env):\n",
    "    model.eval()\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    tran_costs = []\n",
    "    \n",
    "    env.reset(mode = \"test\")\n",
    "    state = env.get_state()\n",
    "\n",
    "    while not is_end:\n",
    "        _, allocations = model(state)\n",
    "        state, reward, is_end, tran_cost = env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return sharp_ratio, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# mlflow.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "training model --\n",
      "Step 3: last loss = 31.80803\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 7: last loss = 44.78043\n",
      "eval step --\n",
      "Step 7: val_rewards = 0.7061796559060425\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 11: last loss = -75.17106\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 15: last loss = -114.44054\n",
      "eval step --\n",
      "Step 15: val_rewards = 0.6941040935797264\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 19: last loss = 160.50276\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 23: last loss = 31.41937\n",
      "eval step --\n",
      "Step 23: val_rewards = 0.6974197242524538\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 27: last loss = 258.90515\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 31: last loss = 198.75691\n",
      "eval step --\n",
      "Step 31: val_rewards = 0.7303077090284806\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 35: last loss = 59.89495\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 39: last loss = 138.69815\n",
      "eval step --\n",
      "Step 39: val_rewards = 0.7452634564622732\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 43: last loss = 32.80138\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 47: last loss = 62.26991\n",
      "eval step --\n",
      "Step 47: val_rewards = 0.7671608215614102\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 51: last loss = 55.26288\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 55: last loss = 95.45531\n",
      "eval step --\n",
      "Step 55: val_rewards = 0.7594780159001161\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 59: last loss = 40.86234\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 63: last loss = 43.04759\n",
      "eval step --\n",
      "Step 63: val_rewards = 0.6961630491709441\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 67: last loss = 68.19038\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 71: last loss = 25.06576\n",
      "eval step --\n",
      "Step 71: val_rewards = 0.7937183883084797\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 75: last loss = 103.89832\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 79: last loss = 143.05783\n",
      "eval step --\n",
      "Step 79: val_rewards = 0.8667102748911775\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 83: last loss = 16.52072\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 87: last loss = 32.76505\n",
      "eval step --\n",
      "Step 87: val_rewards = 0.8452766190154077\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 91: last loss = 173.67305\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 95: last loss = 156.20531\n",
      "eval step --\n",
      "Step 95: val_rewards = 0.8294367081104899\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 99: last loss = 57.86942\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 103: last loss = 163.47392\n",
      "eval step --\n",
      "Step 103: val_rewards = 0.7945548943914058\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 107: last loss = 284.80807\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 111: last loss = 39.58979\n",
      "eval step --\n",
      "Step 111: val_rewards = 0.7486765470692552\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 115: last loss = 38.99466\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 119: last loss = 150.55991\n",
      "eval step --\n",
      "Step 119: val_rewards = 0.8402820683726929\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 123: last loss = 21.49175\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 127: last loss = -51.84828\n",
      "eval step --\n",
      "Step 127: val_rewards = 0.769251490337885\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 131: last loss = 38.46150\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 135: last loss = 71.87733\n",
      "eval step --\n",
      "Step 135: val_rewards = 0.8251754613268171\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 139: last loss = 119.73210\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 143: last loss = 29.94318\n",
      "eval step --\n",
      "Step 143: val_rewards = 0.8083398972346048\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 147: last loss = -11.40540\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 151: last loss = 50.01254\n",
      "eval step --\n",
      "Step 151: val_rewards = 0.8435350085420591\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 155: last loss = 9.85860\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 159: last loss = 8.90221\n",
      "eval step --\n",
      "Step 159: val_rewards = 0.7770303094236822\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 163: last loss = 185.06525\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 167: last loss = 42.62103\n",
      "eval step --\n",
      "Step 167: val_rewards = 0.7817441557970103\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 171: last loss = 96.04565\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 175: last loss = 140.27965\n",
      "eval step --\n",
      "Step 175: val_rewards = 0.8261138619168876\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 179: last loss = 140.38890\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 183: last loss = -2.90834\n",
      "eval step --\n",
      "Step 183: val_rewards = 0.7987089251310062\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 187: last loss = 197.04733\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 191: last loss = 67.81586\n",
      "eval step --\n",
      "Step 191: val_rewards = 0.7771907910767679\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 195: last loss = 36.06502\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 199: last loss = 42.97736\n",
      "eval step --\n",
      "Step 199: val_rewards = 0.7817441676466765\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 203: last loss = 209.06477\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 207: last loss = 172.28970\n",
      "eval step --\n",
      "Step 207: val_rewards = 0.7817441843686231\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 211: last loss = 132.86586\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 215: last loss = 46.53628\n",
      "eval step --\n",
      "Step 215: val_rewards = 0.7817440967945144\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 219: last loss = 140.27818\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 223: last loss = 49.78804\n",
      "eval step --\n",
      "Step 223: val_rewards = 0.7901815712538731\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 227: last loss = 66.43220\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 231: last loss = 80.11509\n",
      "eval step --\n",
      "Step 231: val_rewards = 0.7669534827652456\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 235: last loss = -27.16504\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 239: last loss = 13.46412\n",
      "eval step --\n",
      "Step 239: val_rewards = 0.7740660754453527\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 243: last loss = 7.33960\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 247: last loss = 66.71635\n",
      "eval step --\n",
      "Step 247: val_rewards = 0.7682960271556452\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 251: last loss = -7.74857\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 255: last loss = 190.35793\n",
      "eval step --\n",
      "Step 255: val_rewards = 0.7770302808214793\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 259: last loss = -27.24337\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 263: last loss = 128.75261\n",
      "eval step --\n",
      "Step 263: val_rewards = 0.7817441510081917\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 267: last loss = 89.06741\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 271: last loss = -25.35185\n",
      "eval step --\n",
      "Step 271: val_rewards = 0.8587742521130924\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 275: last loss = 11.89352\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 279: last loss = 39.10426\n",
      "eval step --\n",
      "Step 279: val_rewards = 0.7817441041885251\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 283: last loss = 58.33817\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 287: last loss = 60.52982\n",
      "eval step --\n",
      "Step 287: val_rewards = 0.777030228581207\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 291: last loss = 14.38289\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 295: last loss = 181.93840\n",
      "eval step --\n",
      "Step 295: val_rewards = 0.8064256614288273\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 299: last loss = -6.72188\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 303: last loss = -6.09728\n",
      "eval step --\n",
      "Step 303: val_rewards = 0.8587742521888763\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 307: last loss = 194.82915\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 311: last loss = 106.26202\n",
      "eval step --\n",
      "Step 311: val_rewards = 0.8746721843794477\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 315: last loss = 43.73087\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 319: last loss = 23.83462\n",
      "eval step --\n",
      "Step 319: val_rewards = 0.8498375650390382\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 323: last loss = 77.87100\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 327: last loss = -46.05122\n",
      "eval step --\n",
      "Step 327: val_rewards = 0.8571298192795606\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 331: last loss = 19.09773\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 335: last loss = 104.26883\n",
      "eval step --\n",
      "Step 335: val_rewards = 0.7817441240831301\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 339: last loss = 20.15751\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 343: last loss = 105.31769\n",
      "eval step --\n",
      "Step 343: val_rewards = 0.781744148943578\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 347: last loss = 90.96317\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 351: last loss = 9.05391\n",
      "eval step --\n",
      "Step 351: val_rewards = 0.7801285170410105\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 355: last loss = 86.97430\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 359: last loss = 116.17328\n",
      "eval step --\n",
      "Step 359: val_rewards = 0.8589319765002104\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 363: last loss = 7.13336\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 367: last loss = 25.78317\n",
      "eval step --\n",
      "Step 367: val_rewards = 0.7607859369241912\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 371: last loss = 165.95212\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 375: last loss = 37.20855\n",
      "eval step --\n",
      "Step 375: val_rewards = 0.7682959947820267\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 379: last loss = 6.18901\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 383: last loss = 98.00729\n",
      "eval step --\n",
      "Step 383: val_rewards = 0.8236373794714809\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 387: last loss = 115.27441\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 391: last loss = 101.64098\n",
      "eval step --\n",
      "Step 391: val_rewards = 0.7661846439791311\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 395: last loss = 191.68596\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 399: last loss = 64.87993\n",
      "eval step --\n",
      "Step 399: val_rewards = 0.812825903132985\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 403: last loss = -37.12099\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 407: last loss = 230.92844\n",
      "eval step --\n",
      "Step 407: val_rewards = 0.7890646653605171\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 411: last loss = 71.12206\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 415: last loss = 119.87276\n",
      "eval step --\n",
      "Step 415: val_rewards = 0.7880861377794709\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 419: last loss = 50.22986\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 423: last loss = 76.49843\n",
      "eval step --\n",
      "Step 423: val_rewards = 0.8031030833737312\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 427: last loss = 101.15975\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 431: last loss = 12.09799\n",
      "eval step --\n",
      "Step 431: val_rewards = 0.7880862042979515\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 435: last loss = 165.98627\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 439: last loss = 16.93750\n",
      "eval step --\n",
      "Step 439: val_rewards = 0.7933627592590619\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 443: last loss = 130.24600\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 447: last loss = 0.55851\n",
      "eval step --\n",
      "Step 447: val_rewards = 0.8475153521027462\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 451: last loss = -35.20176\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 455: last loss = 84.99348\n",
      "eval step --\n",
      "Step 455: val_rewards = 0.7740661321984458\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 459: last loss = 174.32153\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 463: last loss = 119.39318\n",
      "eval step --\n",
      "Step 463: val_rewards = 0.7771908151275247\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 467: last loss = 53.22684\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 471: last loss = -4.87580\n",
      "eval step --\n",
      "Step 471: val_rewards = 0.7880862580876679\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 475: last loss = 197.55878\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 479: last loss = -41.72048\n",
      "eval step --\n",
      "Step 479: val_rewards = 0.7695770738502602\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 483: last loss = 164.13077\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 487: last loss = 129.96442\n",
      "eval step --\n",
      "Step 487: val_rewards = 0.8416187528079143\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 491: last loss = 168.25954\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 495: last loss = 32.14806\n",
      "eval step --\n",
      "Step 495: val_rewards = 0.764598548277011\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 499: last loss = 106.46660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/05 11:05:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "tid = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "with mlflow.start_run(run_name = f\"v2_training_{tid}\") as run:\n",
    "    params = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"train_step\": train_step,\n",
    "        \"eval_step\": eval_step,\n",
    "        \"metric_function\": 'sharpe',\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \n",
    "        \"symbol_universe\" : symbol_universe,\n",
    "        \"feature_set\" : feature_set,\n",
    "        \"d_model\" : d_model,\n",
    "        \"nheads\" : nheads,\n",
    "        \"num_transformer_layers\" : num_transformer_layers,\n",
    "\n",
    "        \"episode_duration\" : 12,    \n",
    "        \"holding_period\" : 1,\n",
    "        \"train_test_split\" : 0.8,\n",
    "        \"symbol_universe\" : symbol_universe,\n",
    "        \"feature_set\" : feature_set,\n",
    "\n",
    "    }\n",
    "    # Log training parameters.\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    portfolio_constructor = PortfolioConstructor(\n",
    "        device = device,\n",
    "        symbol_universe= params['symbol_universe'],\n",
    "        num_features= len(params['feature_set']),\n",
    "        d_model = params['d_model'],\n",
    "        nheads = params['nheads'],\n",
    "        num_transformer_layers = params['num_transformer_layers'],\n",
    "    )\n",
    "\n",
    "    market_env = MarketEnvironment(\n",
    "        device = device,\n",
    "        data_path = data_path,\n",
    "        holding_period = params['holding_period'],\n",
    "        episode_duration = params['episode_duration'],\n",
    "        train_test_split = params['train_test_split'],\n",
    "        symbol_universe = params['symbol_universe'],\n",
    "        feature_set = params['feature_set']\n",
    "        )\n",
    "    \n",
    "    portfolio_constructor.cuda()\n",
    "    portfolio_constructor.train()\n",
    "    market_env.reset(mode = \"train\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(portfolio_constructor.parameters(), lr = learning_rate)\n",
    "    \n",
    "    max_reward = -1\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        is_end = False\n",
    "        rewards = []\n",
    "        tran_costs = []\n",
    "        nlls = []\n",
    "        all_allocations = []\n",
    "\n",
    "        market_env.reset(mode = \"train\", transaction_cost= 1e-7)\n",
    "        state = market_env.get_state()\n",
    "\n",
    "        while not is_end:\n",
    "            symbol_idx, allocations = portfolio_constructor(state)\n",
    "            state, reward, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "            all_allocations.append(allocations)\n",
    "            rewards.append(reward)\n",
    "            tran_costs.append(tran_cost)\n",
    "            mask_tensor = torch.tensor([1 if i in symbol_idx.cpu().numpy() else 0 for i in range(allocations.shape[0])]).type(torch.FloatTensor).cuda()\n",
    "\n",
    "            nlls.append((torch.log(allocations.abs() + 1e-9) * mask_tensor))\n",
    "\n",
    "        sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "\n",
    "        # loss = -sharp_ratio * sum([step_allocations.sum() for step_allocations in all_allocations])\n",
    "        loss = -sharp_ratio * sum([e.sum() for e in nlls])\n",
    "        # loss = - sum([e.sum() for e in nlls])\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        if (episode + 1) % train_step == 0:\n",
    "\n",
    "            print(\"-------------------------------------\")\n",
    "            print(\"training model --\")\n",
    "            print('Step {}: last loss = {:.5f}\\r'.format(episode, loss), end='')\n",
    "            print()\n",
    "            mlflow.log_metric(\"train loss\", f\"{loss:2f}\", step=episode)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            count = 0\n",
    "            \n",
    "        if (episode + 1) % eval_step == 0:\n",
    "            print(\"eval step --\")\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                reward_val, portfolio_constructor = evaluate(portfolio_constructor, market_env)\n",
    "\n",
    "                print('Step {}: val_rewards = {}'.format(episode, reward_val))\n",
    "                mlflow.log_metric(\"eval_sharpe\", f\"{reward_val:2f}\", step=episode)\n",
    "\n",
    "                if max_reward < reward_val:\n",
    "                    max_reward = reward_val\n",
    "\n",
    "                    print(\"*** found better model ***\")\n",
    "                print()\n",
    "                    # torch.save(portfolio_constructor.state_dict(), model_path)\n",
    "    mlflow.pytorch.log_model(portfolio_constructor, f\"portfolio_constructor_{tid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_model = 'runs:/05854e54ff834dadb066a8c3cac986f6/portfolio_constructor_2024_11_05_10_42'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pytorch.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5535767645487388"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_end = False\n",
    "rewards = []\n",
    "tran_costs = []\n",
    "\n",
    "market_env.reset(mode = \"test\")\n",
    "state = market_env.get_state()\n",
    "\n",
    "while not is_end:\n",
    "    _, allocations = loaded_model(state)\n",
    "    state, reward, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "    rewards.append(reward)\n",
    "    tran_costs.append(tran_cost)\n",
    "\n",
    "sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
