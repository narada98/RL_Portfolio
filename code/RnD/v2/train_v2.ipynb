{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PortfolioConstructor import PortfolioConstructor\n",
    "from ExchnageEnv import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda') \n",
    "    torch.get_default_device()\n",
    "    device = 'cuda'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SWKS', 'ALLE', 'BDX', 'CMI', 'APH', 'PNC', 'GWW', 'GLW', 'NRG', 'JKHY']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/naradaw/dev/Charles_Schwab/data/symbol_universe/snp_unique_100_2019\", \"rb\") as fp:\n",
    "    symbol_universe = pickle.load(fp)\n",
    "    \n",
    "symbol_universe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_path = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v1/2024_10_31/feature_set_2024_10_31_11_18.pkl\"\n",
    "\n",
    "with open(feature_set_path, 'rb') as f:\n",
    "    feature_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SBUX',\n",
       " 'CHTR',\n",
       " 'GLW',\n",
       " 'GWW',\n",
       " 'ETN',\n",
       " 'IRM',\n",
       " 'CPB',\n",
       " 'DHI',\n",
       " 'BDX',\n",
       " 'RMD',\n",
       " 'RSG',\n",
       " 'NVDA',\n",
       " 'ALB',\n",
       " 'CHTR',\n",
       " 'DHI',\n",
       " 'HRL',\n",
       " 'AWK',\n",
       " 'CHTR',\n",
       " 'OMC',\n",
       " 'GE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_universe = random.choices(symbol_universe, k = 20)\n",
    "symbol_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "portfolio_constructor = PortfolioConstructor(\n",
    "    device = device,\n",
    "    symbol_universe= symbol_universe,\n",
    "    num_features= len(feature_set),\n",
    "    d_model = 88,\n",
    "    nheads = 2,\n",
    "    num_transformer_layers = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v1/2024_10_31/dataset_sqs_2024_10_31_11_18.pkl\"\n",
    "\n",
    "market_env = MarketEnvironment(\n",
    "    data_path = data_path,\n",
    "    holding_period = 1,\n",
    "    train_test_split= 0.8,\n",
    "    symbol_universe = symbol_universe,\n",
    "    feature_set= feature_set,\n",
    "    device = device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 60, 20, 87)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_env.features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "sharpe ratio measures the excess return of the portfolio over the \n",
    "volatility of it -> risk adjusted performance\n",
    "'''\n",
    "\n",
    "\n",
    "def sharp_ratio_(rewards, tran_costs):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharp_ratio_loss_(rewards, tran_costs, allocations):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env):\n",
    "    model.eval()\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    tran_costs = []\n",
    "    \n",
    "    env.reset(mode = \"test\")\n",
    "    state = env.get_state()\n",
    "\n",
    "    while not is_end:\n",
    "        _, allocations = model(state)\n",
    "        state, reward, is_end, tran_cost = env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return sharp_ratio, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_constructor.cuda()\n",
    "portfolio_constructor.train()\n",
    "market_env.reset(mode = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f9bf05b6350>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_constructor.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "eval_step = 8\n",
    "train_step = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(portfolio_constructor.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(portfolio_constructor.parameters(), lr=0.01, momentum=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_hparams() missing 1 required positional argument: 'metric_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_hparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparam_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepisodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_step\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_step\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: add_hparams() missing 1 required positional argument: 'metric_dict'"
     ]
    }
   ],
   "source": [
    "writer.add_hparams(\n",
    "    hparam_dict = {\n",
    "        'episodes' : episodes,\n",
    "        'train_step' : train_step,\n",
    "        'eval_step' : eval_step,\n",
    "        'learning_rate' : learning_rate\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "training model --\n",
      "Step 7: last loss = 157.77850\n",
      "eval step --\n",
      "Step 7: val_rewards = -0.29190273603945166\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 15: last loss = 80.31731\n",
      "eval step --\n",
      "Step 15: val_rewards = -0.28499105415817527\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 23: last loss = 158.63667\n",
      "eval step --\n",
      "Step 23: val_rewards = 0.4892255342150697\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 31: last loss = 142.88673\n",
      "eval step --\n",
      "Step 31: val_rewards = 0.515483497283407\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 39: last loss = 122.92643\n",
      "eval step --\n",
      "Step 39: val_rewards = 0.5105824975964965\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 47: last loss = 57.19584\n",
      "eval step --\n",
      "Step 47: val_rewards = 0.5120467616809006\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 55: last loss = 124.64124\n",
      "eval step --\n",
      "Step 55: val_rewards = 0.3571396112857136\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 63: last loss = 152.26421\n",
      "eval step --\n",
      "Step 63: val_rewards = 0.5022331330939129\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 71: last loss = 139.77412\n",
      "eval step --\n",
      "Step 71: val_rewards = 0.4836217791015745\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 79: last loss = 132.82619\n",
      "eval step --\n",
      "Step 79: val_rewards = 0.4866364149796721\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 87: last loss = 145.04166\n",
      "eval step --\n",
      "Step 87: val_rewards = 0.4873278964614213\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 95: last loss = 87.00534\n",
      "eval step --\n",
      "Step 95: val_rewards = 0.45421579206538304\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 103: last loss = 132.16750\n",
      "eval step --\n",
      "Step 103: val_rewards = 0.2964336228944736\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 111: last loss = 32.44131\n",
      "eval step --\n",
      "Step 111: val_rewards = 0.3624647945761163\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 119: last loss = 93.21677\n",
      "eval step --\n",
      "Step 119: val_rewards = 0.4740023496016396\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 127: last loss = 60.44733\n",
      "eval step --\n",
      "Step 127: val_rewards = 0.14870341183400876\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 135: last loss = 162.68764\n",
      "eval step --\n",
      "Step 135: val_rewards = 0.244361047419741\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 143: last loss = 120.90995\n",
      "eval step --\n",
      "Step 143: val_rewards = 0.3668251608235054\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 151: last loss = 38.71661\n",
      "eval step --\n",
      "Step 151: val_rewards = 0.1300667483142353\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 159: last loss = 60.13718\n",
      "eval step --\n",
      "Step 159: val_rewards = 0.14739221358113902\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 167: last loss = 142.31422\n",
      "eval step --\n",
      "Step 167: val_rewards = -0.1325553260579965\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 175: last loss = 77.27268\n",
      "eval step --\n",
      "Step 175: val_rewards = -0.04126067387610453\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 183: last loss = 15.72666\n",
      "eval step --\n",
      "Step 183: val_rewards = -0.1921822567907234\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 191: last loss = 116.28623\n",
      "eval step --\n",
      "Step 191: val_rewards = 0.10214317281191079\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 199: last loss = 104.41599\n",
      "eval step --\n",
      "Step 199: val_rewards = -0.12116510993269135\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 207: last loss = 89.70692\n",
      "eval step --\n",
      "Step 207: val_rewards = 0.1912526573180829\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 215: last loss = 103.37760\n",
      "eval step --\n",
      "Step 215: val_rewards = 0.011572277889027907\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 223: last loss = 112.98071\n",
      "eval step --\n",
      "Step 223: val_rewards = -0.09493320098011929\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 231: last loss = 197.97746\n",
      "eval step --\n",
      "Step 231: val_rewards = -0.058910712652314436\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 239: last loss = 88.87050\n",
      "eval step --\n",
      "Step 239: val_rewards = 0.019707746169293723\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 247: last loss = 142.47905\n",
      "eval step --\n",
      "Step 247: val_rewards = -0.11798727494407482\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 255: last loss = 105.48450\n",
      "eval step --\n",
      "Step 255: val_rewards = 0.30780472311388607\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 263: last loss = 114.40069\n",
      "eval step --\n",
      "Step 263: val_rewards = 0.07268888784418534\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 271: last loss = 115.02424\n",
      "eval step --\n",
      "Step 271: val_rewards = 0.16183468676154855\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 279: last loss = 22.97938\n",
      "eval step --\n",
      "Step 279: val_rewards = 0.024296982084459996\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 287: last loss = 109.01607\n",
      "eval step --\n",
      "Step 287: val_rewards = -0.16232234144794463\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 295: last loss = 96.10384\n",
      "eval step --\n",
      "Step 295: val_rewards = 0.005464116587320107\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 303: last loss = 101.48875\n",
      "eval step --\n",
      "Step 303: val_rewards = 0.0024644343645238956\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 311: last loss = 146.97697\n",
      "eval step --\n",
      "Step 311: val_rewards = 0.04857513145375444\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 319: last loss = 97.70464\n",
      "eval step --\n",
      "Step 319: val_rewards = 0.19385142492700855\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 327: last loss = 76.46446\n",
      "eval step --\n",
      "Step 327: val_rewards = -0.06621726915033284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 335: last loss = 175.86867\n",
      "eval step --\n",
      "Step 335: val_rewards = -0.13925051234838803\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 343: last loss = 105.98635\n",
      "eval step --\n",
      "Step 343: val_rewards = -0.0516652301001749\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 351: last loss = 185.13345\n",
      "eval step --\n",
      "Step 351: val_rewards = 0.05752512374736089\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 359: last loss = 168.75597\n",
      "eval step --\n",
      "Step 359: val_rewards = -0.005601808062129161\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 367: last loss = 160.06664\n",
      "eval step --\n",
      "Step 367: val_rewards = -0.03668988208283493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 375: last loss = 16.79784\n",
      "eval step --\n",
      "Step 375: val_rewards = -0.016943327978683698\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 383: last loss = 79.45465\n",
      "eval step --\n",
      "Step 383: val_rewards = 0.2976362083626587\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 391: last loss = 82.08721\n",
      "eval step --\n",
      "Step 391: val_rewards = -0.014900766925553867\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 399: last loss = 17.59202\n",
      "eval step --\n",
      "Step 399: val_rewards = -0.12544709555521283\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 407: last loss = 138.01825\n",
      "eval step --\n",
      "Step 407: val_rewards = -0.005419890544062331\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 415: last loss = 97.85758\n",
      "eval step --\n",
      "Step 415: val_rewards = -0.020054351882028962\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 423: last loss = 178.28856\n",
      "eval step --\n",
      "Step 423: val_rewards = 0.08107006447154846\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 431: last loss = 165.26295\n",
      "eval step --\n",
      "Step 431: val_rewards = 0.008538687202568082\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 439: last loss = 165.21526\n",
      "eval step --\n",
      "Step 439: val_rewards = -0.0909497353791069\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 447: last loss = 4.13880\n",
      "eval step --\n",
      "Step 447: val_rewards = -0.03193524706247672\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 455: last loss = 89.41486\n",
      "eval step --\n",
      "Step 455: val_rewards = -0.007890210938529566\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 463: last loss = 65.42379\n",
      "eval step --\n",
      "Step 463: val_rewards = 0.028015199998704426\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 471: last loss = 5.21793\n",
      "eval step --\n",
      "Step 471: val_rewards = 0.2842862890169942\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 479: last loss = 57.97723\n",
      "eval step --\n",
      "Step 479: val_rewards = -0.10334551187545274\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 487: last loss = 58.35289\n",
      "eval step --\n",
      "Step 487: val_rewards = -0.059638671327012636\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 495: last loss = 126.37994\n",
      "eval step --\n",
      "Step 495: val_rewards = 0.07463427804825505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_reward = -1\n",
    "\n",
    "for episode in range(episodes):\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    tran_costs = []\n",
    "    nlls = []\n",
    "    all_allocations = []\n",
    "\n",
    "    market_env.reset(mode = \"train\", transaction_cost= 1e-7)\n",
    "    state = market_env.get_state()\n",
    "\n",
    "    while not is_end:\n",
    "        symbol_idx, allocations = portfolio_constructor(state)\n",
    "        state, reward, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "        all_allocations.append(allocations)\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "        mask_tensor = torch.tensor([1 if i in symbol_idx.cpu().numpy() else 0 for i in range(allocations.shape[0])]).type(torch.FloatTensor).cuda()\n",
    "\n",
    "        nlls.append((torch.log(allocations.abs() + 1e-9) * mask_tensor))\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "\n",
    "    # loss = -sharp_ratio * sum([step_allocations.sum() for step_allocations in all_allocations])\n",
    "    loss = -sharp_ratio * sum([e.sum() for e in nlls])\n",
    "    # loss = - sum([e.sum() for e in nlls])\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    if (episode + 1) % train_step == 0:\n",
    "\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\"training model --\")\n",
    "        print('Step {}: last loss = {:.5f}\\r'.format(episode, loss), end='')\n",
    "        print()\n",
    "        writer.add_scalar(\"Loss/train\", sharp_ratio, episode)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        count = 0\n",
    "        \n",
    "    if (episode + 1) % eval_step == 0:\n",
    "        print(\"eval step --\")\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            reward_val, portfolio_constructor = evaluate(portfolio_constructor, market_env)\n",
    "\n",
    "            print('Step {}: val_rewards = {}'.format(episode, reward_val))\n",
    "            writer.add_scalar(\"eval_sharpe/train\", reward_val, episode)\n",
    "\n",
    "            if max_reward < reward_val:\n",
    "                max_reward = reward_val\n",
    "\n",
    "                print(\"*** found better model ***\")\n",
    "            print()\n",
    "                # torch.save(portfolio_constructor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = market_env.get_random_state()\n",
    "# random_state.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  2,  3,  5,  6, 11, 12, 14, 17, 19], device='cuda:0'),\n",
       " tensor([0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
       "         0.0000, 0.1000], device='cuda:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_constructor.eval()\n",
    "portfolio_constructor(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/dev/Charles_Schwab/code/RnD/v2/PortfolioConstructor.py:135: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  long_mask = torch.Tensor([0 if i in long_sqs else 1 for i in range(rank.shape[0])]).to(self.device)\n",
      "/home/naradaw/dev/Charles_Schwab/code/RnD/v2/PortfolioConstructor.py:162: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  portfolio_allocations = [allocation.item() for allocation in allocations if allocation != 0]\n"
     ]
    }
   ],
   "source": [
    "writer.add_graph(portfolio_constructor, random_state.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.3242, device='cuda:0'), tensor(-4.8283, device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor([0.4, 0.3, 0.3])).sum(), torch.log(torch.tensor([0.8, 0.1, 0.1])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9163, -1.2040, -1.2040], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor([0.4, 0.3, 0.3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
