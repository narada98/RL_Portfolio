{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PortfolioConstructor import PortfolioConstructor\n",
    "from ExchnageEnv import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda') \n",
    "    torch.get_default_device()\n",
    "    device = 'cuda'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_fpath = '/home/naradaw/dev/Charles_Schwab/data/historical_random_100/2024_10_15/historical_price_seq_2024_10_15_16_22.pkl'\n",
    "pkl_fpath = '/home/naradaw/dev/Charles_Schwab/data/historical/2024_09_11/historical_price_seq_2024_09_11_12_04.pkl'\n",
    "\n",
    "with open(pkl_fpath, 'rb') as f:\n",
    "    price_sqs_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_sqs_dict[list(price_sqs_dict.keys())[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_universe = list(price_sqs_dict.keys())\n",
    "len(symbol_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_universe = random.choices(list(price_sqs_dict.keys()), k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "portfolio_constructor = PortfolioConstructor(\n",
    "    device= device,\n",
    "    symbol_universe = symbol_universe,\n",
    "    seq_length = 60,\n",
    "    multihead_dim = 2,\n",
    "    num_transformer_layers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_env = MarketEnvironment(\n",
    "    data_path = pkl_fpath,\n",
    "    holding_period = 1,\n",
    "    train_test_split= 0.8,\n",
    "    symbol_universe = symbol_universe,\n",
    "    device = device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 20, 61)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_env.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "sharpe ratio measures the excess return of the portfolio over the \n",
    "volatility of it -> risk adjusted performance\n",
    "'''\n",
    "\n",
    "\n",
    "def sharp_ratio_(rewards, tran_costs):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_constructor.cuda()\n",
    "portfolio_constructor.train()\n",
    "market_env.reset(mode = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f2cb774a740>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_constructor.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(portfolio_constructor.parameters())\n",
    "optimizer = torch.optim.RMSprop(portfolio_constructor.parameters(), lr=0.01, momentum=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[168.6900, 169.7200, 167.9400,  ..., 199.6100, 203.8400, 199.4000],\n",
       "        [ 65.1800,  66.9800,  66.8000,  ...,  93.0100,  94.5500,  95.2000],\n",
       "        [ 25.2000,  25.3500,  25.4400,  ...,  31.2900,  32.1800,  33.5400],\n",
       "        ...,\n",
       "        [ 65.1700,  67.7400,  66.9500,  ...,  83.4900,  84.4500,  84.6000],\n",
       "        [ 25.2000,  25.3500,  25.4400,  ...,  31.2900,  32.1800,  33.5400],\n",
       "        [163.1600, 162.9600, 160.8800,  ..., 190.1100, 193.5600, 199.8300]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_env.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharp_ratio_loss_(rewards, tran_costs, allocations):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 100\n",
    "eval_step = 8\n",
    "train_step = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env):\n",
    "    model.eval()\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    tran_costs = []\n",
    "    \n",
    "    env.reset(mode = \"test\")\n",
    "    state = env.get_state()\n",
    "\n",
    "    while not is_end:\n",
    "        _ , allocations = model(state)\n",
    "        state, reward, is_end, tran_cost = env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 5.618464911094634e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.484484972839709e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 7.244319704113877e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.4091923478408717e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.976031001049705e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.996117902081096e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.1143185929540778e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.5794192879402544e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.3027909012162127e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.686071184143657e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.0791816243436188e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.4538510388083523e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.964816184838128e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.808236440112523e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 7.250726525853679e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.374716586928116e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.997695875388672e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.9538394724349928e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.0895204241023748e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.4058413146121893e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.310712145321304e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.4593689431640087e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.864620339300018e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.647977225933573e-06\n",
      "Grad SREM.ff.weight: 7.900515811343212e-06\n",
      "Grad SREM.ff.bias: 6.154780294309603e-06\n",
      "Grad CAAN.W_Q.weight: 4.082762188772904e-06\n",
      "Grad CAAN.W_Q.bias: 1.704453467255007e-07\n",
      "Grad CAAN.W_K.weight: 5.707812306354754e-06\n",
      "Grad CAAN.W_K.bias: 4.062976084046044e-12\n",
      "Grad CAAN.W_V.weight: 3.00777151096554e-06\n",
      "Grad CAAN.W_V.bias: 4.46910100038167e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.9446340704453178e-05\n",
      "Grad CAAN.scorer.bias: 4.716166586149484e-07\n",
      "Grad layer_norm.weight: 3.2271946110995486e-06\n",
      "Grad layer_norm.bias: 3.142400146316504e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.454267804656411e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.692971285498061e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 8.25477286525711e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.692830321393558e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 2.1899609237152617e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.1938122074516286e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.2404526614773204e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.8169142751721665e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.8060177303123055e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.953047214759863e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.4903318919532467e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.7189767024538014e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 5.72052499592246e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.040556629865023e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.225358669733396e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.653645196915022e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.2319203196730086e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.172136248645984e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.2247689937794348e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.732238726821379e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.769100658246316e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.783511263260152e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 4.486737907427596e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.0444807634921744e-06\n",
      "Grad SREM.ff.weight: 9.139985195361078e-06\n",
      "Grad SREM.ff.bias: 7.392204224743182e-06\n",
      "Grad CAAN.W_Q.weight: 4.633648586604977e-06\n",
      "Grad CAAN.W_Q.bias: 3.0606329914917296e-07\n",
      "Grad CAAN.W_K.weight: 6.56922838970786e-06\n",
      "Grad CAAN.W_K.bias: 4.330145617070791e-12\n",
      "Grad CAAN.W_V.weight: 4.132404228585074e-06\n",
      "Grad CAAN.W_V.bias: 5.138573300200733e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.929081867681816e-05\n",
      "Grad CAAN.scorer.bias: 5.422552931122482e-07\n",
      "Grad layer_norm.weight: 3.642891215349664e-06\n",
      "Grad layer_norm.bias: 3.37375399794837e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 8.588248192609171e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.732756444056577e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.1488845075291465e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.953834493586328e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.085741866470926e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.079042585341085e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.771133838701644e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.842792921204818e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 6.087254860176472e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.189306309854146e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.5977307056309655e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.814937372226268e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 7.60729619742051e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 6.745853511347377e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.1255239087404334e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.84158829547232e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.1304068670579e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.101562526808266e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.758752546265896e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.870892669510795e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 5.85780890105525e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 4.045488822157495e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.780691819585627e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.666024324251339e-06\n",
      "Grad SREM.ff.weight: 1.3436547305900604e-05\n",
      "Grad SREM.ff.bias: 1.1700706636474933e-05\n",
      "Grad CAAN.W_Q.weight: 6.961442977626575e-06\n",
      "Grad CAAN.W_Q.bias: 6.171520681164111e-07\n",
      "Grad CAAN.W_K.weight: 9.25166386878118e-06\n",
      "Grad CAAN.W_K.bias: 4.993444893686139e-12\n",
      "Grad CAAN.W_V.weight: 6.902728273416869e-06\n",
      "Grad CAAN.W_V.bias: 1.1087632145745374e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 6.601370114367455e-05\n",
      "Grad CAAN.scorer.bias: 1.171407348010689e-06\n",
      "Grad layer_norm.weight: 5.939558377576759e-06\n",
      "Grad layer_norm.bias: 4.614596491592238e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.6951798897935078e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.8664507024368504e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.595988689790829e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.0117661986441817e-05\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.921713975316379e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 7.022703130132868e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.039404757349985e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 9.764989044924732e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.3602628314401954e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.0414049938844983e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.2454637726477813e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 9.758082342159469e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.6581715271968278e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.5763857845740858e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.56211137639184e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.132006198342424e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 6.950451734155649e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 6.960932523725205e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.994235612481134e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 9.990782928070985e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.2838549082516693e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 9.91230808722321e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.62572723638732e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.1939406249439344e-05\n",
      "Grad SREM.ff.weight: 3.185802779626101e-05\n",
      "Grad SREM.ff.bias: 2.7686608518706635e-05\n",
      "Grad CAAN.W_Q.weight: 1.732291821099352e-05\n",
      "Grad CAAN.W_Q.bias: 1.6228563026743359e-06\n",
      "Grad CAAN.W_K.weight: 2.275816041219514e-05\n",
      "Grad CAAN.W_K.bias: 5.519083454141871e-12\n",
      "Grad CAAN.W_V.weight: 1.8065518816001713e-05\n",
      "Grad CAAN.W_V.bias: 1.7784742567528156e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0001771677052602172\n",
      "Grad CAAN.scorer.bias: 1.878710463643074e-06\n",
      "Grad layer_norm.weight: 1.3311047041497659e-05\n",
      "Grad layer_norm.bias: 1.127446103055263e-05\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.5991263353498653e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.7598745216673706e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.539286924729822e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 9.695278095023241e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.635378895225585e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 6.721542149534798e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.86421606890508e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 9.422019502380863e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.2914848412037827e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.0008751814893913e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.1853438081743661e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 9.36012565944111e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.5680022897868184e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.4903444025549106e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.4861485599103617e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.742945283302106e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 6.650529940088745e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 6.658307256657281e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.815393938566558e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 9.565271284373011e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.2215058632136788e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 9.47460466704797e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.5405390513478778e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.1398428796383087e-05\n",
      "Grad SREM.ff.weight: 3.033832945220638e-05\n",
      "Grad SREM.ff.bias: 2.6520145183894783e-05\n",
      "Grad CAAN.W_Q.weight: 1.6872074411367066e-05\n",
      "Grad CAAN.W_Q.bias: 1.5733870668555028e-06\n",
      "Grad CAAN.W_K.weight: 2.2040223484509625e-05\n",
      "Grad CAAN.W_K.bias: 5.366094721348524e-12\n",
      "Grad CAAN.W_V.weight: 1.6978872736217454e-05\n",
      "Grad CAAN.W_V.bias: 1.8082749875247828e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00016664867871440947\n",
      "Grad CAAN.scorer.bias: 1.9100712052022573e-06\n",
      "Grad layer_norm.weight: 1.2680350664595608e-05\n",
      "Grad layer_norm.bias: 1.0727032531576697e-05\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.77224626440875e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.8986066834258963e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.712474952204502e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.0773390386020765e-05\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 7.268291142281669e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 7.434887834278925e-07\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.266798441676656e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.0500495591259096e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.4389971511263866e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.112022073357366e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.3139897419023328e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0482160178071354e-05\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.7376713685735012e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.6397532363043865e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.687807182155666e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.876353942672722e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 7.295254249584104e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 7.374248980340781e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 4.215249646222219e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.078481727745384e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.3681498785445001e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.0636515980877448e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.7335965821985155e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.2794197573384736e-05\n",
      "Grad SREM.ff.weight: 3.37935853167437e-05\n",
      "Grad SREM.ff.bias: 2.9425700631691143e-05\n",
      "Grad CAAN.W_Q.weight: 1.865235753939487e-05\n",
      "Grad CAAN.W_Q.bias: 1.6805644236228545e-06\n",
      "Grad CAAN.W_K.weight: 2.5227585865650326e-05\n",
      "Grad CAAN.W_K.bias: 3.759449349050037e-12\n",
      "Grad CAAN.W_V.weight: 1.9492446881486103e-05\n",
      "Grad CAAN.W_V.bias: 3.075010397424194e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00019219290697947145\n",
      "Grad CAAN.scorer.bias: 3.2478706089023035e-06\n",
      "Grad layer_norm.weight: 1.4220937373465858e-05\n",
      "Grad layer_norm.bias: 1.199014423036715e-05\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.4279969440831337e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.298069486845634e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 4.005012669949792e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.6033261999837123e-05\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.138351308327401e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.1521924534463324e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 6.647724148933776e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.6040743503253907e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.2399384761229157e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.6310415958287194e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.092719660140574e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.6364279872504994e-05\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.1733848118165042e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.8382767141010845e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.153707777732052e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.5165835066e-05\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.1570111837500008e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.1678179134833044e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 6.689741894660983e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.7609459973755293e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.2123522285255603e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.641240669414401e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.8579248464666307e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.0992796635255218e-05\n",
      "Grad SREM.ff.weight: 5.53235768165905e-05\n",
      "Grad SREM.ff.bias: 4.710782741312869e-05\n",
      "Grad CAAN.W_Q.weight: 3.127373565803282e-05\n",
      "Grad CAAN.W_Q.bias: 2.794290139718214e-06\n",
      "Grad CAAN.W_K.weight: 4.443969373824075e-05\n",
      "Grad CAAN.W_K.bias: 1.039607462616221e-11\n",
      "Grad CAAN.W_V.weight: 3.3686654205666855e-05\n",
      "Grad CAAN.W_V.bias: 5.20322132047113e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00032894196920096874\n",
      "Grad CAAN.scorer.bias: 5.486858754011337e-07\n",
      "Grad layer_norm.weight: 2.1724979887949303e-05\n",
      "Grad layer_norm.bias: 1.9193117623217404e-05\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.6311264466348803e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.518425501169986e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 4.3382824514992535e-06\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.7741618648869917e-05\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.2525933925644495e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.258092879652395e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 7.3167334448953625e-06\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.789921043382492e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.458286689943634e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.8120208551408723e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.2790143702877685e-05\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.8196720702690072e-05\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.3731289477145765e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.9989328140800353e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.50497145720874e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.6832686014822684e-05\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.2710281680483604e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.283774736293708e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.357174126809696e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.946884185599629e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.3967450033524074e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.8209146219305694e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.169312185491435e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.3295828214031644e-05\n",
      "Grad SREM.ff.weight: 6.081445098971017e-05\n",
      "Grad SREM.ff.bias: 5.246549335424788e-05\n",
      "Grad CAAN.W_Q.weight: 3.33812422468327e-05\n",
      "Grad CAAN.W_Q.bias: 2.9311454454727937e-06\n",
      "Grad CAAN.W_K.weight: 4.659117985283956e-05\n",
      "Grad CAAN.W_K.bias: 1.1693017214209345e-11\n",
      "Grad CAAN.W_V.weight: 3.737520819413476e-05\n",
      "Grad CAAN.W_V.bias: 8.841050203045597e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0003664643154479563\n",
      "Grad CAAN.scorer.bias: 9.347409104520921e-07\n",
      "Grad layer_norm.weight: 2.3869342840043828e-05\n",
      "Grad layer_norm.bias: 2.0698227672255598e-05\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 7: last loss = 50.40332\n",
      "eval step --\n",
      "Step 7: val_rewards = 0.41917271016287105\n",
      "*** found better model ***\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.925134325516893e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.3849588142988978e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.280237913574126e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.1204858702272844e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.984687134226064e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 9.375257514765423e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.514297159888045e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.6886924070778093e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 8.100339843331028e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.673549739019904e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.200797594691494e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.4712317586429435e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.7586086748243588e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 3.055049324629522e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.450548787981461e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.1272658362315724e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.388611559072217e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.5659935570511152e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.5075570791722726e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.622954019595227e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.9858060085198304e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.3616301803031092e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.2573131868975906e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.7356119030864647e-07\n",
      "Grad SREM.ff.weight: 6.23849487624284e-08\n",
      "Grad SREM.ff.bias: 7.623756914654223e-08\n",
      "Grad CAAN.W_Q.weight: 2.3434321150261894e-08\n",
      "Grad CAAN.W_Q.bias: 3.190626873816882e-09\n",
      "Grad CAAN.W_K.weight: 3.0339055712147456e-08\n",
      "Grad CAAN.W_K.bias: 1.5476920960100227e-13\n",
      "Grad CAAN.W_V.weight: 3.586682950640352e-08\n",
      "Grad CAAN.W_V.bias: 3.309675378204702e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 5.066269181952521e-07\n",
      "Grad CAAN.scorer.bias: 3.7764039007015526e-07\n",
      "Grad layer_norm.weight: 2.9779723131895253e-10\n",
      "Grad layer_norm.bias: 4.2135792077679923e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.011564827687835e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 9.490195296280035e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.1055057697338952e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.238899760558354e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.221841959888394e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 4.43041048292514e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 7.794601075694629e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.054165948450759e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.078299171122012e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.263992353121694e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.26616127874513e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 7.155473014108793e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.172820063288782e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.3044285651631071e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.290339816748201e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 5.358400656518825e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 6.517083761536924e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 7.267390689236208e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.846916894038714e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.4145030863564898e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.392932298624146e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.1076277672827928e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.629552444792353e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 9.592228025212535e-07\n",
      "Grad SREM.ff.weight: 3.3753673278624774e-07\n",
      "Grad SREM.ff.bias: 4.1922257310034183e-07\n",
      "Grad CAAN.W_Q.weight: 1.9202964551823243e-07\n",
      "Grad CAAN.W_Q.bias: 8.544297003254542e-08\n",
      "Grad CAAN.W_K.weight: 1.7785751538212935e-07\n",
      "Grad CAAN.W_K.bias: 1.597454265221676e-13\n",
      "Grad CAAN.W_V.weight: 2.1422532370252156e-07\n",
      "Grad CAAN.W_V.bias: 3.176619856049001e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.1557638067170046e-06\n",
      "Grad CAAN.scorer.bias: 3.624742248575785e-07\n",
      "Grad layer_norm.weight: 2.5409447879098934e-09\n",
      "Grad layer_norm.bias: 3.355472788513225e-09\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.1119417608895787e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.0279730400242215e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.5732200814587713e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.73079806042864e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.1577891130017974e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 6.119249551517214e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.0430433183827859e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.761080919455594e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.410430687329381e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 8.311593191479005e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 8.472289181327142e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0135489958429389e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.75994365747556e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.9710192589172948e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 6.405868901993017e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.038848875457916e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.013100092706054e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.145316641526506e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.1895447293852612e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.660379732650654e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.357456937967072e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.6223924603764317e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.01501893823297e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.459279474147479e-06\n",
      "Grad SREM.ff.weight: 5.248099910204473e-07\n",
      "Grad SREM.ff.bias: 6.62584966448776e-07\n",
      "Grad CAAN.W_Q.weight: 2.680361603779602e-07\n",
      "Grad CAAN.W_Q.bias: 1.1242435249414484e-07\n",
      "Grad CAAN.W_K.weight: 2.5781332624319475e-07\n",
      "Grad CAAN.W_K.bias: 2.0208409056386711e-13\n",
      "Grad CAAN.W_V.weight: 3.4558698303044366e-07\n",
      "Grad CAAN.W_V.bias: 2.2925104303794797e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 5.074629825685406e-06\n",
      "Grad CAAN.scorer.bias: 2.6157908905588556e-06\n",
      "Grad layer_norm.weight: 2.6797148944268656e-09\n",
      "Grad layer_norm.bias: 3.842155482658427e-09\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.1206349181946962e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.174449204555117e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.497246631705252e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.9757711022805324e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.389463237330915e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 6.818896558513643e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.1391757537282388e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.0741005296874846e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.276345499931722e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.65725988066879e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.263106903745211e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.1256401677428585e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.9370478554492365e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.172511770481833e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 6.875070113210313e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.633712411665329e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.1276010347671672e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.2757458200951532e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.4110098867092802e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.335667469490545e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.4386418456524552e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.73161680550038e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.208527578455687e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.7350266716675833e-06\n",
      "Grad SREM.ff.weight: 6.248798740671191e-07\n",
      "Grad SREM.ff.bias: 7.845958975849499e-07\n",
      "Grad CAAN.W_Q.weight: 3.0723788313480327e-07\n",
      "Grad CAAN.W_Q.bias: 1.2374763969091873e-07\n",
      "Grad CAAN.W_K.weight: 2.947808184217138e-07\n",
      "Grad CAAN.W_K.bias: 1.938750267098388e-13\n",
      "Grad CAAN.W_V.weight: 3.9804851326152857e-07\n",
      "Grad CAAN.W_V.bias: 1.412400791878099e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 6.208831109688617e-06\n",
      "Grad CAAN.scorer.bias: 1.6116018741740845e-06\n",
      "Grad layer_norm.weight: 2.7418780579324675e-09\n",
      "Grad layer_norm.bias: 4.462246572245476e-09\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.5538058617536876e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.0568453889803209e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.086260136342389e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.856188618949318e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.82062675835715e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 8.970545417597009e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.308005148636937e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.417120364446191e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 5.187061891120948e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.3629422923600032e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.479897045139296e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.2570458807203977e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.339387172867191e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.6195982272270157e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.355663538850422e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.0388381355141973e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.4745568321927749e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.677954131196202e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.9036061615906874e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.778611722462301e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.7101190508128639e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.0976764858460228e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.628892164262652e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.352368710489827e-06\n",
      "Grad SREM.ff.weight: 8.42942938561464e-07\n",
      "Grad SREM.ff.bias: 1.0657066695785034e-06\n",
      "Grad CAAN.W_Q.weight: 4.600626368755911e-07\n",
      "Grad CAAN.W_Q.bias: 1.5984100798505096e-07\n",
      "Grad CAAN.W_K.weight: 3.8701381299688364e-07\n",
      "Grad CAAN.W_K.bias: 9.606923682134796e-14\n",
      "Grad CAAN.W_V.weight: 5.267532969810418e-07\n",
      "Grad CAAN.W_V.bias: 1.5969045819019811e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 8.360470019397326e-06\n",
      "Grad CAAN.scorer.bias: 1.8220780475530773e-06\n",
      "Grad layer_norm.weight: 4.023783528595004e-09\n",
      "Grad layer_norm.bias: 4.299863132217752e-09\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.5391784513596463e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.1120752096971387e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.3301061908019847e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.226340306028419e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.463578561266559e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0185133847429029e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.5605101655324916e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 4.1161798414179884e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 5.829117633027181e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.512130332059769e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.171978283309727e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.5372222605947172e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.7976968297593885e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 3.139790649697716e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.0112909620829669e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.2467543797356484e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.7590711109960466e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.0075670903452192e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.289768197272224e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.939258412330673e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.057143717593135e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.5204775511156186e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.9623773823695956e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.8282211133046076e-06\n",
      "Grad SREM.ff.weight: 1.0053125834019738e-06\n",
      "Grad SREM.ff.bias: 1.2742925719066989e-06\n",
      "Grad CAAN.W_Q.weight: 5.537791594178998e-07\n",
      "Grad CAAN.W_Q.bias: 1.8691262937409192e-07\n",
      "Grad CAAN.W_K.weight: 4.63738530243063e-07\n",
      "Grad CAAN.W_K.bias: 2.1859901042087237e-13\n",
      "Grad CAAN.W_V.weight: 6.24883568889345e-07\n",
      "Grad CAAN.W_V.bias: 6.678471464738323e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.0222024684480857e-05\n",
      "Grad CAAN.scorer.bias: 7.620626547577558e-07\n",
      "Grad layer_norm.weight: 4.135571884944511e-09\n",
      "Grad layer_norm.bias: 4.844255663272179e-09\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.5093778449326578e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.1671654753797611e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.365081768791356e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.468303644282855e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.87058132609053e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0894631863322957e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.66282354552294e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 4.381578211365422e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 5.912913714212209e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.6447481598902414e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.2495118539845862e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.6501512334343715e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.9858558292517046e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 3.3573730462421736e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.0605502609450923e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.3062928871931945e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.910046520947617e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.180200908696861e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.485151462394697e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 7.50504867141899e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.1524817839235766e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.63958554569399e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.1370346985349897e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.0638127554993844e-06\n",
      "Grad SREM.ff.weight: 1.0855819709831849e-06\n",
      "Grad SREM.ff.bias: 1.3772730653727194e-06\n",
      "Grad CAAN.W_Q.weight: 5.75130059132789e-07\n",
      "Grad CAAN.W_Q.bias: 1.8970378334870475e-07\n",
      "Grad CAAN.W_K.weight: 4.904754291601421e-07\n",
      "Grad CAAN.W_K.bias: 3.654044840144255e-13\n",
      "Grad CAAN.W_V.weight: 6.647275085924775e-07\n",
      "Grad CAAN.W_V.bias: 1.521898695955315e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.0715949429140892e-05\n",
      "Grad CAAN.scorer.bias: 1.737414550007088e-07\n",
      "Grad layer_norm.weight: 4.17336787350564e-09\n",
      "Grad layer_norm.bias: 5.019339166523196e-09\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.7591944567740825e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.9005339524369447e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 4.70931071916425e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 5.179254358722574e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 9.027228964342271e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.5327559221844922e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.2507828933981955e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 6.068179292384457e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 8.070026424888965e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.5213779508703738e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.749495908143217e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.3057155829064868e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.1698186237226764e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.6987810975451794e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.4710825269048655e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.8192397988059383e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.7167528671157015e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.109844826099106e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.493221711892147e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.0504726333238068e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.0168786224749056e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.644274499947642e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.0029464141989592e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.268722477718256e-06\n",
      "Grad SREM.ff.weight: 1.5005163049863768e-06\n",
      "Grad SREM.ff.bias: 1.9110516404907685e-06\n",
      "Grad CAAN.W_Q.weight: 7.687480660933943e-07\n",
      "Grad CAAN.W_Q.bias: 2.404177621428971e-07\n",
      "Grad CAAN.W_K.weight: 7.100153425199096e-07\n",
      "Grad CAAN.W_K.bias: 4.3476097830348615e-13\n",
      "Grad CAAN.W_V.weight: 9.071488307199616e-07\n",
      "Grad CAAN.W_V.bias: 3.963028234466037e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.5139791685214732e-05\n",
      "Grad CAAN.scorer.bias: 4.521821210801136e-06\n",
      "Grad layer_norm.weight: 4.5767860612500044e-09\n",
      "Grad layer_norm.bias: 7.597199314091085e-09\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 15: last loss = 309.71695\n",
      "eval step --\n",
      "Step 15: val_rewards = 0.26875048500206195\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 8.289036818043272e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.1164706232322814e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.9064994027928606e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 6.664583468429797e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 2.2221821605050707e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.4723502758949856e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.852339974796948e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 8.420816821619326e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.795740305927552e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.83752715768415e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.2642507868653183e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.749814598246303e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 7.142496594170211e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 7.865761375569491e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.1749921863545296e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.3454016506718744e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.0429047342673812e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.2123322618305963e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 4.703741285361218e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 8.399593798280591e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.988012586830791e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 5.0235393622699576e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.169680943912681e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 5.52350094551457e-08\n",
      "Grad SREM.ff.weight: 5.1680974166856686e-08\n",
      "Grad SREM.ff.bias: 5.38390452220483e-08\n",
      "Grad CAAN.W_Q.weight: 3.801910253287133e-08\n",
      "Grad CAAN.W_Q.bias: 7.232700216519561e-09\n",
      "Grad CAAN.W_K.weight: 5.2887280332925e-08\n",
      "Grad CAAN.W_K.bias: 3.3475307893498715e-14\n",
      "Grad CAAN.W_V.weight: 3.199450304691709e-08\n",
      "Grad CAAN.W_V.bias: 6.68157724703633e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 4.174184482508281e-07\n",
      "Grad CAAN.scorer.bias: 7.645876394235529e-07\n",
      "Grad layer_norm.weight: 1.9541393503352822e-10\n",
      "Grad layer_norm.bias: 8.203751566959738e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.1161801344305999e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 4.7921753371094056e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.667269183298515e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.1477782613233956e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 2.9210599217233124e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.524725650239091e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 5.1078117430103376e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.1495768226232883e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.6465699904120186e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.602180941366441e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.009652527197204e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.153206667467657e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.2094371060200615e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.348678724255592e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.499430523135061e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.819293770135346e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.0213315183047484e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.3731875626052954e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.776303490025782e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.4428304107028112e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 6.641597050105474e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 8.468346024415041e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.0637291580906094e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 9.318070937069933e-08\n",
      "Grad SREM.ff.weight: 7.823926040373408e-08\n",
      "Grad SREM.ff.bias: 7.925588363377756e-08\n",
      "Grad CAAN.W_Q.weight: 8.417922003900458e-08\n",
      "Grad CAAN.W_Q.bias: 8.650506089225019e-09\n",
      "Grad CAAN.W_K.weight: 8.183155841834377e-08\n",
      "Grad CAAN.W_K.bias: 4.5662780468695013e-14\n",
      "Grad CAAN.W_V.weight: 5.490799281915315e-08\n",
      "Grad CAAN.W_V.bias: 8.914155102957011e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.1293815305180033e-06\n",
      "Grad CAAN.scorer.bias: 1.0200903943768935e-06\n",
      "Grad layer_norm.weight: 2.194140980016357e-10\n",
      "Grad layer_norm.bias: 1.4392675939944866e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.2898829226948294e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 6.245980815888785e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.457523073928769e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.237286800792603e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.054961145165791e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.6253264279473285e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 5.340706002443518e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.1105051600512894e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.100135426148398e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 6.065313251646387e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.8863540446621982e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.040465295673812e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.2039316210632478e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.335359156584559e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.3132927512724564e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.5583060942201428e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.1832803060183323e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.534239012559226e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 8.286961117320857e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.5326661051418e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 6.110064898479095e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 7.796645107305267e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.184923377195446e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 9.792108812689548e-08\n",
      "Grad SREM.ff.weight: 8.598301803885988e-08\n",
      "Grad SREM.ff.bias: 8.834516762590283e-08\n",
      "Grad CAAN.W_Q.weight: 9.733195582839471e-08\n",
      "Grad CAAN.W_Q.bias: 9.487186147794091e-09\n",
      "Grad CAAN.W_K.weight: 9.430988967551457e-08\n",
      "Grad CAAN.W_K.bias: 4.338242886128309e-14\n",
      "Grad CAAN.W_V.weight: 6.016219344928686e-08\n",
      "Grad CAAN.W_V.bias: 3.926733782577685e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.2294095768083935e-06\n",
      "Grad CAAN.scorer.bias: 4.493774667935213e-07\n",
      "Grad layer_norm.weight: 2.492408779364297e-10\n",
      "Grad layer_norm.bias: 1.7783888250910707e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.335011545755549e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 6.139834474172545e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.102949563960067e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.4406791037924194e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.545077773003946e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 4.013784790646291e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 6.055315276976003e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.1541097244549547e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.6453887131138174e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 7.576166916045679e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.08403569349025e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.9082856950756195e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.1035331537456727e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.212154487895134e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.9220139641239484e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.1115801046865954e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.1119923304956387e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.3425462397929095e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.215270048099853e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.2420220407705074e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 5.4149720263296786e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 6.777368888322144e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 9.280399382305404e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 8.668882145457246e-08\n",
      "Grad SREM.ff.weight: 7.378461219786914e-08\n",
      "Grad SREM.ff.bias: 7.685100911203335e-08\n",
      "Grad CAAN.W_Q.weight: 7.640252874807629e-08\n",
      "Grad CAAN.W_Q.bias: 5.41120703800857e-09\n",
      "Grad CAAN.W_K.weight: 7.0898991566537e-08\n",
      "Grad CAAN.W_K.bias: 3.6633083990312426e-14\n",
      "Grad CAAN.W_V.weight: 5.024351779070457e-08\n",
      "Grad CAAN.W_V.bias: 6.303969257714925e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.1131132851005532e-06\n",
      "Grad CAAN.scorer.bias: 7.214018751255935e-07\n",
      "Grad layer_norm.weight: 2.7697732996045943e-10\n",
      "Grad layer_norm.bias: 2.2017128398221786e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.772227087304401e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.1694456514277363e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.445468632623516e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.8656462485999725e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.9986694511107146e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 4.91839728666843e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 7.272033669680766e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.6272354297353075e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.2228795460452773e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 7.806368884644144e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 4.323579716469794e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.772628597355833e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.7072463442957542e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.8955341740678477e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.054377722439995e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 4.203712489214695e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.5803482401064457e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.929077618813892e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 8.613634250309588e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.5016367038711564e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 7.411220526165607e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 9.766072395223091e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.2050170994371e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.0082040091674571e-07\n",
      "Grad SREM.ff.weight: 9.001973921840545e-08\n",
      "Grad SREM.ff.bias: 9.262566891266033e-08\n",
      "Grad CAAN.W_Q.weight: 1.0211834933215869e-07\n",
      "Grad CAAN.W_Q.bias: 8.346683344484518e-09\n",
      "Grad CAAN.W_K.weight: 1.0010272433191858e-07\n",
      "Grad CAAN.W_K.bias: 3.1732256389584504e-14\n",
      "Grad CAAN.W_V.weight: 6.518367712260442e-08\n",
      "Grad CAAN.W_V.bias: 6.014924736064131e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.398793756379746e-06\n",
      "Grad CAAN.scorer.bias: 6.883236665089498e-07\n",
      "Grad layer_norm.weight: 3.487736488505533e-10\n",
      "Grad layer_norm.bias: 2.8468838397799345e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 2.7171978556062015e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.5483579696162764e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 6.416721598512254e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.878032312996325e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.624025550821088e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 6.484573988885245e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.0905931019378201e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.5441440620888045e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 7.049259509450678e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.075225571031353e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.7156276095658995e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 9.285939128744758e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.9616522567721404e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 3.3067766302963264e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 7.933690149286576e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.60580673389677e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 5.869639774225277e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 6.707567834496331e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.4406372761399666e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.715492719573831e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.589682696589989e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.060786208346599e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.040952011839181e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.795736608301013e-07\n",
      "Grad SREM.ff.weight: 1.4419774174712074e-07\n",
      "Grad SREM.ff.bias: 1.5200473058030184e-07\n",
      "Grad CAAN.W_Q.weight: 1.6574470862451562e-07\n",
      "Grad CAAN.W_Q.bias: 1.5521909091376074e-08\n",
      "Grad CAAN.W_K.weight: 1.5570371658668591e-07\n",
      "Grad CAAN.W_K.bias: 4.2458803806806264e-14\n",
      "Grad CAAN.W_V.weight: 1.3292682865539973e-07\n",
      "Grad CAAN.W_V.bias: 2.322087908623871e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.68847111328796e-06\n",
      "Grad CAAN.scorer.bias: 2.6572065507934894e-06\n",
      "Grad layer_norm.weight: 4.0708919568643864e-10\n",
      "Grad layer_norm.bias: 3.7769914973395657e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.121371772163428e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.9823799546347942e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 6.307148137096874e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.2666208626963567e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 7.745021290972431e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 9.62158963613291e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.5202585024187698e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.631727418795805e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 8.247437177644201e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.5173466927365098e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.417813195966573e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.3828617362321438e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.557372257352199e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.106012057609632e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.1488960005578974e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.2909895374946245e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 7.835641024911411e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 8.906861914681485e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.043651470273744e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.934637948788122e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.2874418803553453e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.040781137997328e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.948869450847269e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.3800554060926515e-07\n",
      "Grad SREM.ff.weight: 1.9507719173361693e-07\n",
      "Grad SREM.ff.bias: 1.9790478233971953e-07\n",
      "Grad CAAN.W_Q.weight: 2.3716765440440213e-07\n",
      "Grad CAAN.W_Q.bias: 2.295975676247508e-08\n",
      "Grad CAAN.W_K.weight: 2.3464508558390662e-07\n",
      "Grad CAAN.W_K.bias: 7.846426687638935e-14\n",
      "Grad CAAN.W_V.weight: 1.6960414939148905e-07\n",
      "Grad CAAN.W_V.bias: 2.2224051576813508e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.5056409615208395e-06\n",
      "Grad CAAN.scorer.bias: 2.543149548728252e-06\n",
      "Grad layer_norm.weight: 4.3958783835229553e-10\n",
      "Grad layer_norm.bias: 5.07827391249549e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.0882121859754363e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.0373396025785695e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 6.328711443792656e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.461213260447238e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 7.904865650942838e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 9.999106403979496e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.571539981481962e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.798823478007307e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 8.35472524496339e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.630556356602142e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.83351444716618e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.4497567590865401e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.746398829524878e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.319720663976568e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.196967858163589e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.3397125186997982e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 8.072207346998539e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 9.170578740835822e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.1524466653488616e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.153683175189826e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.3711418606353618e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.1356087504263996e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.1458765192837745e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.5013619620040117e-07\n",
      "Grad SREM.ff.weight: 2.0976078474177484e-07\n",
      "Grad SREM.ff.bias: 2.1124665749994165e-07\n",
      "Grad CAAN.W_Q.weight: 2.4814011112539447e-07\n",
      "Grad CAAN.W_Q.bias: 2.4637536455429654e-08\n",
      "Grad CAAN.W_K.weight: 2.5455412355768203e-07\n",
      "Grad CAAN.W_K.bias: 5.3103128041772155e-14\n",
      "Grad CAAN.W_V.weight: 1.781142486834142e-07\n",
      "Grad CAAN.W_V.bias: 2.0667107492045034e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.582803174140281e-06\n",
      "Grad CAAN.scorer.bias: 2.3649811282666633e-06\n",
      "Grad layer_norm.weight: 4.443129197895246e-10\n",
      "Grad layer_norm.bias: 5.558244420278413e-10\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 23: last loss = 54.52121\n",
      "eval step --\n",
      "Step 23: val_rewards = 0.42136643459046935\n",
      "*** found better model ***\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.1016146682244505e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.2688274829995034e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 8.575036514191225e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 6.3328864721712286e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.571981457301201e-13\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.10043907580043e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.2911099056198139e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.203887025474139e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 9.353249945387443e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.6411898709777226e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.805651429717344e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.2241813118762934e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.648171786023834e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.9006637369821675e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.857036020743948e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.1138232003382598e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 7.148887384844649e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 8.065563841808121e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.1863641145859471e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.4824661423727967e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.0328394523261295e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.6422686261184936e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.251030168347711e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.6561602089737448e-09\n",
      "Grad SREM.ff.weight: 1.2942352745071162e-09\n",
      "Grad SREM.ff.bias: 1.5108957418519253e-09\n",
      "Grad CAAN.W_Q.weight: 4.279117060690396e-10\n",
      "Grad CAAN.W_Q.bias: 7.134741408787448e-11\n",
      "Grad CAAN.W_K.weight: 5.63526281194271e-10\n",
      "Grad CAAN.W_K.bias: 2.7390251999544028e-14\n",
      "Grad CAAN.W_V.weight: 2.04576622309105e-09\n",
      "Grad CAAN.W_V.bias: 9.615869878132344e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.6656643115074985e-08\n",
      "Grad CAAN.scorer.bias: 1.1119870180209546e-07\n",
      "Grad layer_norm.weight: 6.626806742238145e-12\n",
      "Grad layer_norm.bias: 8.030497274102988e-12\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 5.763457419649676e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.153171726141048e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.1062145083572403e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.2675845616205716e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.7411234129807718e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.7109462678170937e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.689262437212015e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 9.409686571593134e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.1416623682823932e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.6676500298502575e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.0519961558006514e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.741068566043282e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 5.6185035790523585e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 6.103042377025858e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.159052658878835e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.4854587832301434e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.938940757424291e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.45192902040381e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.866534266898757e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.517639206116144e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.801097852613623e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 5.796816360259527e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.286557914705668e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.929122804890994e-09\n",
      "Grad SREM.ff.weight: 4.326693225920053e-09\n",
      "Grad SREM.ff.bias: 4.947328324789169e-09\n",
      "Grad CAAN.W_Q.weight: 3.910348433944222e-10\n",
      "Grad CAAN.W_Q.bias: 9.169767722916333e-11\n",
      "Grad CAAN.W_K.weight: 4.926903884872047e-10\n",
      "Grad CAAN.W_K.bias: 4.9664685883819837e-14\n",
      "Grad CAAN.W_V.weight: 1.0222629853728904e-08\n",
      "Grad CAAN.W_V.bias: 6.768517835098464e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.1771680280835426e-07\n",
      "Grad CAAN.scorer.bias: 7.827187573639094e-07\n",
      "Grad layer_norm.weight: 1.2605061959491959e-11\n",
      "Grad layer_norm.bias: 1.502887050919277e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.3639770736484369e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.1987783122012772e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 7.614434777147849e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 4.1337773482030826e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.249251550266566e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0232641123320008e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.4246013223251364e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.760252179074719e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.1640415020774952e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.1122828486319634e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.228321444557423e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.544259053964936e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.3076311694847362e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.582712377297014e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.921996557693546e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.0956737739320488e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.187859810514169e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.3778453378243682e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.1550484180222753e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.6396049235266617e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.243489172215618e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.6654829454741957e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.2733744725323959e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.6244261047404507e-08\n",
      "Grad SREM.ff.weight: 1.864949261687343e-08\n",
      "Grad SREM.ff.bias: 2.1633724145431188e-08\n",
      "Grad CAAN.W_Q.weight: 1.8375985177954135e-09\n",
      "Grad CAAN.W_Q.bias: 1.5196364999692236e-10\n",
      "Grad CAAN.W_K.weight: 2.218462302749913e-09\n",
      "Grad CAAN.W_K.bias: 8.103256564015449e-14\n",
      "Grad CAAN.W_V.weight: 4.153346466750918e-08\n",
      "Grad CAAN.W_V.bias: 2.7411999781179475e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 8.494944268022664e-07\n",
      "Grad CAAN.scorer.bias: 3.169941464875592e-06\n",
      "Grad layer_norm.weight: 2.94422229651925e-11\n",
      "Grad layer_norm.bias: 5.0508756899159124e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 8.499632421499204e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.2968774491806112e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.9011798172162706e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.1424927844061461e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.257433381532616e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 5.456378404655737e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 6.894069082968857e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.5024228389171057e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.184921159611221e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.676649081354235e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.7826008991714843e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.79068174019659e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.2686837691511244e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.4026878270012588e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.4567413648444187e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 5.055346141702444e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.857550390229392e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 4.3995463522295e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.87890377940137e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.6911523570417586e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.014245461180792e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.1734038185551299e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 9.372927323170188e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.1668434218847779e-08\n",
      "Grad SREM.ff.weight: 5.812481607136988e-09\n",
      "Grad SREM.ff.bias: 6.223295212492985e-09\n",
      "Grad CAAN.W_Q.weight: 2.369106022470646e-09\n",
      "Grad CAAN.W_Q.bias: 3.4154523653739943e-10\n",
      "Grad CAAN.W_K.weight: 3.1126012878246456e-09\n",
      "Grad CAAN.W_K.bias: 6.580396356937676e-14\n",
      "Grad CAAN.W_V.weight: 1.1115018239138408e-08\n",
      "Grad CAAN.W_V.bias: 8.024479747348323e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.529035102976195e-07\n",
      "Grad CAAN.scorer.bias: 9.279187906940933e-08\n",
      "Grad layer_norm.weight: 1.9178625701443686e-11\n",
      "Grad layer_norm.bias: 2.991694739162831e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 8.579793125962354e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.322101195883052e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.0415767127145443e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.2271910918015827e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.545979044590087e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 5.835921156049917e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 7.301815402316336e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.5822565480050343e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.392906177708156e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 6.077203323906488e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 4.0121431177375655e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.020953652845606e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.2752927880388398e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.4113933632931008e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.587642210562848e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 5.185923912520707e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 4.033204673015156e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 4.60302143623359e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 8.226449382808809e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.7720075407012814e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.36467459133894e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2102416846460073e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 9.82404024796324e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.2179962816105672e-08\n",
      "Grad SREM.ff.weight: 6.175409517084063e-09\n",
      "Grad SREM.ff.bias: 6.61599131035473e-09\n",
      "Grad CAAN.W_Q.weight: 2.4904400763858803e-09\n",
      "Grad CAAN.W_Q.bias: 3.4359040612663705e-10\n",
      "Grad CAAN.W_K.weight: 3.292957684308817e-09\n",
      "Grad CAAN.W_K.bias: 6.836074268505923e-14\n",
      "Grad CAAN.W_V.weight: 1.1626463347624849e-08\n",
      "Grad CAAN.W_V.bias: 7.847745564504294e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.614087921097962e-07\n",
      "Grad CAAN.scorer.bias: 9.074813078768784e-08\n",
      "Grad layer_norm.weight: 1.9866497336096245e-11\n",
      "Grad layer_norm.bias: 3.032941606195827e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.700225865605372e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.50010386057442e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.502771502352566e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.709684907009624e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.7072228887289356e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0017381021909522e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.0515694218171667e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.0369082248959955e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 6.457405443693887e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.4298900952169902e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 8.058090306128918e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0238080383473402e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.2445571790097318e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.5333088404799753e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.691879993527607e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.75117542267867e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 6.354793774532297e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 7.327496942544087e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.1751781492375102e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.533887544231561e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.8022389136618244e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.3423176731540707e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.3123049313890078e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.7192618884109834e-08\n",
      "Grad SREM.ff.weight: 1.1744571537519732e-08\n",
      "Grad SREM.ff.bias: 1.3852548441661838e-08\n",
      "Grad CAAN.W_Q.weight: 2.9866711326320683e-09\n",
      "Grad CAAN.W_Q.bias: 4.5661299741261985e-10\n",
      "Grad CAAN.W_K.weight: 3.955770822017257e-09\n",
      "Grad CAAN.W_K.bias: 6.437833258024842e-14\n",
      "Grad CAAN.W_V.weight: 1.898305868053285e-08\n",
      "Grad CAAN.W_V.bias: 1.0044711729051414e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.784987600534805e-07\n",
      "Grad CAAN.scorer.bias: 1.1615745734161464e-06\n",
      "Grad layer_norm.weight: 2.384594431636966e-11\n",
      "Grad layer_norm.bias: 4.2420175011548267e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.770469155956363e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.4855503979727125e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.7212255766270204e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.8903806431612722e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.840685574076687e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0390297996987208e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.0873832216595769e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.153108329989607e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 6.610444830412732e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.505934404955056e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 8.426345732281959e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0708322006891535e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.3200738552553446e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.619392480696092e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.020570956863594e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.0091668611877935e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 6.598938756541273e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 7.610921615164301e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.201610616563542e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.594193748706175e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.8720036631947323e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.435599055772286e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.340758704060363e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.7559937504074696e-08\n",
      "Grad SREM.ff.weight: 1.2231270218876489e-08\n",
      "Grad SREM.ff.bias: 1.4397258496501308e-08\n",
      "Grad CAAN.W_Q.weight: 3.022779582195767e-09\n",
      "Grad CAAN.W_Q.bias: 4.5747108878835263e-10\n",
      "Grad CAAN.W_K.weight: 3.990067387604768e-09\n",
      "Grad CAAN.W_K.bias: 5.745946931147786e-14\n",
      "Grad CAAN.W_V.weight: 1.9959529140578525e-08\n",
      "Grad CAAN.W_V.bias: 1.0810521899884407e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.969472632230463e-07\n",
      "Grad CAAN.scorer.bias: 1.2501332093961537e-06\n",
      "Grad layer_norm.weight: 2.3774931676157074e-11\n",
      "Grad layer_norm.bias: 4.3086669648806364e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.641534966242649e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.4505790665864104e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.58969225072925e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.8960302905778335e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.697485452177409e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0222706361973088e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.0826827281928963e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.1190067889541595e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 6.520144840704845e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.511397118569846e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 8.312052712788898e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0593603771980042e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.3070512167322477e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.605873572480988e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.964254893939483e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.004725969089293e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 6.58153045951515e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 7.595073181487777e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.239995189861176e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.6728391722130596e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.8552068770105734e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.4115442975869428e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.3844529966888786e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.820448147782372e-08\n",
      "Grad SREM.ff.weight: 1.2108220204254394e-08\n",
      "Grad SREM.ff.bias: 1.4322678154599089e-08\n",
      "Grad CAAN.W_Q.weight: 3.2839413410812313e-09\n",
      "Grad CAAN.W_Q.bias: 4.614481574627405e-10\n",
      "Grad CAAN.W_K.weight: 4.254792518310069e-09\n",
      "Grad CAAN.W_K.bias: 6.054336041836847e-14\n",
      "Grad CAAN.W_V.weight: 1.939880434065344e-08\n",
      "Grad CAAN.W_V.bias: 9.823914837170378e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.743367133462016e-07\n",
      "Grad CAAN.scorer.bias: 1.136040737037547e-06\n",
      "Grad layer_norm.weight: 2.3181073380285078e-11\n",
      "Grad layer_norm.bias: 4.39217655301416e-11\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 31: last loss = 37.21892\n",
      "eval step --\n",
      "Step 31: val_rewards = 0.31292515604319404\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 5.674678934218524e-13\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 6.7091297795141e-13\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.8158786026809555e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.3605613991284358e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.1775531641672905e-13\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 5.846586778081309e-13\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 5.335669514723584e-13\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.5585062537390448e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.2834256107349145e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.185669964987035e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 4.42430293789986e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.4649187952726663e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 7.472264994257127e-12\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 8.31758099895108e-12\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.4820119010348094e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.720222740977164e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 4.7952531702366574e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 5.517950679712058e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 4.44462869286788e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.0118172792672464e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 7.941609175832198e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 9.32133675801694e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 4.95129992561516e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.123451190553908e-10\n",
      "Grad SREM.ff.weight: 6.973707167290399e-10\n",
      "Grad SREM.ff.bias: 8.115615957038358e-10\n",
      "Grad CAAN.W_Q.weight: 1.294455376221748e-10\n",
      "Grad CAAN.W_Q.bias: 1.0980844705743564e-11\n",
      "Grad CAAN.W_K.weight: 1.162680715594e-10\n",
      "Grad CAAN.W_K.bias: 5.712925521105666e-15\n",
      "Grad CAAN.W_V.weight: 1.5716935530818432e-09\n",
      "Grad CAAN.W_V.bias: 1.0349574530721384e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.9519156896640197e-08\n",
      "Grad CAAN.scorer.bias: 1.1965666146807052e-07\n",
      "Grad layer_norm.weight: 1.6137512333369575e-12\n",
      "Grad layer_norm.bias: 2.465222687444979e-12\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 2.6167299663898413e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 4.3059293110270236e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 9.71770667407279e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 8.618115769631896e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.4204749572768738e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.9041239898996984e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.6113716184171807e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.0294460596471655e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.3015230386492771e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.2254227894389373e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.048333252486657e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.8643374611346815e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 5.7849652151942266e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 6.551646031249803e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.343449323483071e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.6109717166100666e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.694946696357725e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.1337647754536846e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.7503499477887772e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.162471921644652e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 5.277587256102834e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 6.200835955816331e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.1061302419033154e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.903660061865821e-09\n",
      "Grad SREM.ff.weight: 4.464417724392433e-09\n",
      "Grad SREM.ff.bias: 5.181359341577263e-09\n",
      "Grad CAAN.W_Q.weight: 2.392434306219826e-10\n",
      "Grad CAAN.W_Q.bias: 5.85921242080012e-11\n",
      "Grad CAAN.W_K.weight: 2.749022953718594e-10\n",
      "Grad CAAN.W_K.bias: 1.7038791100157206e-14\n",
      "Grad CAAN.W_V.weight: 9.626532460060844e-09\n",
      "Grad CAAN.W_V.bias: 6.306137834144465e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.8959134706619807e-07\n",
      "Grad CAAN.scorer.bias: 7.290783514690702e-07\n",
      "Grad layer_norm.weight: 7.649308270130106e-12\n",
      "Grad layer_norm.bias: 1.0205241166016954e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 2.9884237445765294e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.581430716911084e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.2757969160281935e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.1523092376120037e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.769385527851386e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.704675888976938e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.495374992391499e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.2859821714450437e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.6313429773706645e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.60916452174709e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.7984917988787004e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.833017874794621e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 7.164264320680402e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 8.14207659627364e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.0145619334120966e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.358502087547066e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.3761143186650244e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.918889171505846e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.39482435218752e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 7.611778568561434e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 6.780517258775376e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 8.030364706534954e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.783291013803591e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.80936312996505e-09\n",
      "Grad SREM.ff.weight: 5.529031454898359e-09\n",
      "Grad SREM.ff.bias: 6.425947773891494e-09\n",
      "Grad CAAN.W_Q.weight: 2.4952712118775366e-10\n",
      "Grad CAAN.W_Q.bias: 6.211885866802547e-11\n",
      "Grad CAAN.W_K.weight: 2.687528810607631e-10\n",
      "Grad CAAN.W_K.bias: 1.6735619373610053e-14\n",
      "Grad CAAN.W_V.weight: 1.1973160241041114e-08\n",
      "Grad CAAN.W_V.bias: 7.868852947012783e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.3629549161796604e-07\n",
      "Grad CAAN.scorer.bias: 9.097506108446396e-07\n",
      "Grad layer_norm.weight: 8.33315100950971e-12\n",
      "Grad layer_norm.bias: 1.3318548520990792e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.03777424222329e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.593249387952914e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.2401913697390743e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.1109287968713577e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.767166599685177e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.6470678907440934e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.369462257292067e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.2468625090744645e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.6036651173667593e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.257254007129397e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.7060612911865576e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.698485489562643e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 7.030891147064011e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 7.981600796957977e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.949208932623293e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.2809557848345605e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.397875730781763e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.948046056745369e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.4486389438592724e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 7.761395692806872e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 6.630053173140027e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 7.837531734722347e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.867641318322512e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.841875345107383e-09\n",
      "Grad SREM.ff.weight: 5.52650947227562e-09\n",
      "Grad SREM.ff.bias: 6.410860731165258e-09\n",
      "Grad CAAN.W_Q.weight: 2.9513180788143245e-10\n",
      "Grad CAAN.W_Q.bias: 7.042572774951239e-11\n",
      "Grad CAAN.W_K.weight: 3.188250774499579e-10\n",
      "Grad CAAN.W_K.bias: 1.7821469872210914e-14\n",
      "Grad CAAN.W_V.weight: 1.2056308840158181e-08\n",
      "Grad CAAN.W_V.bias: 7.929001100137612e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.40146022179033e-07\n",
      "Grad CAAN.scorer.bias: 9.167047210212331e-07\n",
      "Grad layer_norm.weight: 7.889811200201269e-12\n",
      "Grad layer_norm.bias: 1.2809073246522473e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.064863684024144e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.588445938647935e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.2735478470415895e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.1038027263043926e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.772984320122517e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.651643223911982e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.3369014976479825e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.2394659951175946e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.6441343078654747e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.289494189875121e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.665762970950226e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.66257255027358e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 6.939563507168955e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 7.87780327082821e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.9203031659541523e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.2472380340209384e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.3655214032313197e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.908891960113792e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.409185087011046e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 7.672513319123553e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 6.569982891058146e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 7.776535526637929e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.824221828097052e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.80177897443923e-09\n",
      "Grad SREM.ff.weight: 5.484028786639783e-09\n",
      "Grad SREM.ff.bias: 6.3653140536246156e-09\n",
      "Grad CAAN.W_Q.weight: 2.6794930163553943e-10\n",
      "Grad CAAN.W_Q.bias: 6.867709179125825e-11\n",
      "Grad CAAN.W_K.weight: 2.9654972921733247e-10\n",
      "Grad CAAN.W_K.bias: 1.5709132332084562e-14\n",
      "Grad CAAN.W_V.weight: 1.1941583721863935e-08\n",
      "Grad CAAN.W_V.bias: 7.840024807137524e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.3610125765571865e-07\n",
      "Grad CAAN.scorer.bias: 9.064179948836681e-07\n",
      "Grad layer_norm.weight: 8.097352649505396e-12\n",
      "Grad layer_norm.bias: 1.2863285089870224e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 4.278830328247052e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.7746516022275625e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.6259632529269652e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.723323298981951e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3397750790733465e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.3003040484627082e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.1186626671804154e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.3845079556716886e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.1987528117661803e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.5964535249324285e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.072015112934537e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 7.425769027458173e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.3388240996680132e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.2692040954619408e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 7.337817853336759e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 6.328051277426638e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0994490040605598e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.2299125259906951e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.7433573681890913e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.5941114667759066e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.5040459155901686e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.516295144998736e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.225428064051016e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.250470698683671e-09\n",
      "Grad SREM.ff.weight: 1.8643733223910885e-09\n",
      "Grad SREM.ff.bias: 2.137728660756011e-09\n",
      "Grad CAAN.W_Q.weight: 6.295329812999739e-10\n",
      "Grad CAAN.W_Q.bias: 9.921657939671036e-11\n",
      "Grad CAAN.W_K.weight: 6.570121668936224e-10\n",
      "Grad CAAN.W_K.bias: 2.427157663764147e-14\n",
      "Grad CAAN.W_V.weight: 2.999719139751278e-09\n",
      "Grad CAAN.W_V.bias: 1.6169593664017157e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 6.319228873508109e-08\n",
      "Grad CAAN.scorer.bias: 1.869419747890788e-07\n",
      "Grad layer_norm.weight: 1.3258402188631724e-11\n",
      "Grad layer_norm.bias: 5.152268802571802e-12\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 4.388862970966523e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.5067658452476715e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.7510461830516633e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 4.237247705485281e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.33650198113483e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.2889305508329008e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.098094701447062e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.811499730942524e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.3097683488071397e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.9682578483726232e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.75595898669279e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 8.036468712724343e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.6416905121728895e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.6602874417247513e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.22156448424083e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.386667749160637e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.024966309742803e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.1200219571239067e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.820995958190519e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.7654976359213777e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.8311334393228407e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.2018743772722615e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.2055623993821882e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.2846071701110304e-09\n",
      "Grad SREM.ff.weight: 1.5724980206854866e-09\n",
      "Grad SREM.ff.bias: 1.7669714580392792e-09\n",
      "Grad CAAN.W_Q.weight: 6.680593855890038e-10\n",
      "Grad CAAN.W_Q.bias: 1.0651442228226671e-10\n",
      "Grad CAAN.W_K.weight: 7.049458239372086e-10\n",
      "Grad CAAN.W_K.bias: 2.3138592143657696e-14\n",
      "Grad CAAN.W_V.weight: 2.2528852117176257e-09\n",
      "Grad CAAN.W_V.bias: 7.361683707074462e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.966518846709732e-08\n",
      "Grad CAAN.scorer.bias: 8.511024418567104e-08\n",
      "Grad layer_norm.weight: 1.3332204264193681e-11\n",
      "Grad layer_norm.bias: 5.350206822712922e-12\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 4.419386297888073e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.4831402125474744e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.8547644323185786e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 4.7589701276939955e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3667594616836864e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.3444948279706104e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.1527116027264537e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.2479370755400527e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.4256348665763205e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.3435600682297775e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 8.153877573136015e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 9.735173084335358e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.8359857864868054e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.885673471009209e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.0649164883247408e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.0200492533973815e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0266480374165887e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.1128614522959435e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.8466294465779853e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.823195926511147e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.1308389774876701e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.659263087512187e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.2011952260925227e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.3404678195504403e-09\n",
      "Grad SREM.ff.weight: 1.4649877977390702e-09\n",
      "Grad SREM.ff.bias: 1.5554549870344658e-09\n",
      "Grad CAAN.W_Q.weight: 6.549581987869146e-10\n",
      "Grad CAAN.W_Q.bias: 1.0746371154057854e-10\n",
      "Grad CAAN.W_K.weight: 6.931246132602098e-10\n",
      "Grad CAAN.W_K.bias: 2.4210495397749068e-14\n",
      "Grad CAAN.W_V.weight: 2.09237094317416e-09\n",
      "Grad CAAN.W_V.bias: 2.8701254706220425e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.4617425370697674e-08\n",
      "Grad CAAN.scorer.bias: 3.318151797770952e-08\n",
      "Grad layer_norm.weight: 1.3346434200867119e-11\n",
      "Grad layer_norm.bias: 5.90991491855597e-12\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 39: last loss = -3.67208\n",
      "eval step --\n",
      "Step 39: val_rewards = 0.3631773743785312\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.2879896801876178e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.8825475033013017e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 8.18370805150126e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 4.389821405687e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.553376005920264e-13\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.2734193040320196e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.4209874596438077e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.960064047514589e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.0106121668684853e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.2641614277896238e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.2149664607719046e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.5386392160365858e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.206866113463324e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.4765177755736723e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.642715792512746e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.0690872492835624e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 9.95072947651554e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.155703744509795e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.0058949333202616e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.278713329140203e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.1332667576867692e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.565469781057317e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.106756020163857e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.3979239987804704e-09\n",
      "Grad SREM.ff.weight: 1.5489282079172995e-09\n",
      "Grad SREM.ff.bias: 1.7952861419701094e-09\n",
      "Grad CAAN.W_Q.weight: 1.3227870188092794e-10\n",
      "Grad CAAN.W_Q.bias: 1.072167701843263e-11\n",
      "Grad CAAN.W_K.weight: 1.374301922263399e-10\n",
      "Grad CAAN.W_K.bias: 5.986632359549632e-15\n",
      "Grad CAAN.W_V.weight: 3.4926603831308967e-09\n",
      "Grad CAAN.W_V.bias: 2.3558600403816854e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 6.687721310072448e-08\n",
      "Grad CAAN.scorer.bias: 2.7239144628765644e-07\n",
      "Grad layer_norm.weight: 3.829226866147728e-12\n",
      "Grad layer_norm.bias: 4.702546945595243e-12\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.536754198846698e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.4042686284632122e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 6.625983095531751e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.262323744479545e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.4788010066253445e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0205478823133163e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.1380001108118876e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.2138153249761103e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 9.556012925404644e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.775663505121372e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.81047021397785e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.2171692542750634e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.6562985705181887e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.8675307134063956e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 7.625594045101991e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.285385710848914e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 9.618795343557807e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.1122232301818968e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 9.252047045160694e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.1513726911059905e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.7175211253217526e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.0104988784908073e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.0481672418904964e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.2977176666595369e-08\n",
      "Grad SREM.ff.weight: 1.4590595398544792e-08\n",
      "Grad SREM.ff.bias: 1.6991366535989982e-08\n",
      "Grad CAAN.W_Q.weight: 1.3523667741210943e-09\n",
      "Grad CAAN.W_Q.bias: 1.326529996958925e-10\n",
      "Grad CAAN.W_K.weight: 1.231323820682917e-09\n",
      "Grad CAAN.W_K.bias: 6.106213760537563e-14\n",
      "Grad CAAN.W_V.weight: 3.287831873421965e-08\n",
      "Grad CAAN.W_V.bias: 2.191956127717276e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 6.507217449325253e-07\n",
      "Grad CAAN.scorer.bias: 2.534403620302328e-06\n",
      "Grad layer_norm.weight: 2.3400205384493944e-11\n",
      "Grad layer_norm.bias: 4.0190815953078385e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.004247206226605e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.554115690305391e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 7.226428483386726e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.717277180514955e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.041675840173788e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.1373504568701343e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.266869074922905e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.5776576146062666e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.0287441037926115e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.9990060995400682e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.0913477899165969e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.3551364475006267e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.8542786750508355e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.0933645095144726e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.620385516522333e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.339919948558872e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0636051067658414e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.2303333352114976e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.0176187670296599e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.35907210432984e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.9367136783188243e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.2773238850248845e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.1514051934113922e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.4294485595200968e-08\n",
      "Grad SREM.ff.weight: 1.6147346570960508e-08\n",
      "Grad SREM.ff.bias: 1.8798912648776422e-08\n",
      "Grad CAAN.W_Q.weight: 1.43145273412415e-09\n",
      "Grad CAAN.W_Q.bias: 1.5651163698393589e-10\n",
      "Grad CAAN.W_K.weight: 1.3796140896360498e-09\n",
      "Grad CAAN.W_K.bias: 9.442602000872893e-14\n",
      "Grad CAAN.W_V.weight: 3.622011490733712e-08\n",
      "Grad CAAN.W_V.bias: 2.4190430281123554e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 7.181958494584251e-07\n",
      "Grad CAAN.scorer.bias: 2.7969667826255318e-06\n",
      "Grad layer_norm.weight: 2.5870207018274805e-11\n",
      "Grad layer_norm.bias: 4.471563744279372e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.418495497404145e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.3793377898196901e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 6.335917901445498e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.2352277107294825e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.4116034570173e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0090670486684328e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.1188721824040293e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.152797467542712e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 9.024443692329243e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.73885225662751e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.626475172552773e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.1919201181598282e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.6180808082300047e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.824385365001291e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 7.475881025342801e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.082969848999255e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 9.437113590582413e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.091784440632182e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 9.055534100355089e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.1055233945244112e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.6835264293746377e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.9700878706174763e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.0330010624670649e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.2746266264684891e-08\n",
      "Grad SREM.ff.weight: 1.4400025172278674e-08\n",
      "Grad SREM.ff.bias: 1.6756535714534948e-08\n",
      "Grad CAAN.W_Q.weight: 1.4981647034062462e-09\n",
      "Grad CAAN.W_Q.bias: 1.3749722194145164e-10\n",
      "Grad CAAN.W_K.weight: 1.4285019833693013e-09\n",
      "Grad CAAN.W_K.bias: 8.990581174504741e-14\n",
      "Grad CAAN.W_V.weight: 3.195624032059641e-08\n",
      "Grad CAAN.W_V.bias: 2.1400917660230334e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 6.351074830490688e-07\n",
      "Grad CAAN.scorer.bias: 2.474435632393579e-06\n",
      "Grad layer_norm.weight: 2.3215634276096964e-11\n",
      "Grad layer_norm.bias: 3.949809229686352e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.7115482642271473e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.227230288560534e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.4198352216165944e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 7.847349853262742e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.217414363763325e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.383904185165875e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.677051462451896e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 7.627212750271894e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.026216139316972e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.042726708952671e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.3056363485096654e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.8855240419289885e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.936586057129432e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.469833669862311e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.846177655195902e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.02657401970896e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.2315928271954277e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.579943758629355e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.1284787821151951e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.896190120007304e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.161623490261945e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 4.955360566327727e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.368901697025194e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.9738435713966282e-08\n",
      "Grad SREM.ff.weight: 3.39150894035356e-08\n",
      "Grad SREM.ff.bias: 3.9530863915615555e-08\n",
      "Grad CAAN.W_Q.weight: 2.0944102008257914e-09\n",
      "Grad CAAN.W_Q.bias: 2.5554194871268976e-10\n",
      "Grad CAAN.W_K.weight: 2.22375740044356e-09\n",
      "Grad CAAN.W_K.bias: 9.811162976882185e-14\n",
      "Grad CAAN.W_V.weight: 7.55911315764024e-08\n",
      "Grad CAAN.W_V.bias: 5.022032496526663e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.4987763279350474e-06\n",
      "Grad CAAN.scorer.bias: 5.806616627523908e-06\n",
      "Grad layer_norm.weight: 4.93310219062959e-11\n",
      "Grad layer_norm.bias: 9.050341726046796e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.862157303356149e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.482402213816904e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.5451631640850394e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 8.407136098398382e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.321908034424002e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.5985992871402352e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.921082126183805e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 8.317023458825901e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.2040994029914884e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.332404990314842e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.508258711841904e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.1519666876533847e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.3100148450214704e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.897690031313573e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.0267063582934952e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.2267507837625544e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.418326927600134e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.7941371438799933e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.29765720338726e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.284339077427092e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.5716945784590735e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 5.450448536947761e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.5599446118462765e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.221693489763311e-08\n",
      "Grad SREM.ff.weight: 3.68845753939695e-08\n",
      "Grad SREM.ff.bias: 4.302507505826725e-08\n",
      "Grad CAAN.W_Q.weight: 1.882501932115588e-09\n",
      "Grad CAAN.W_Q.bias: 2.936685894461277e-10\n",
      "Grad CAAN.W_K.weight: 1.9995245459369926e-09\n",
      "Grad CAAN.W_K.bias: 1.1611588426885255e-13\n",
      "Grad CAAN.W_V.weight: 8.16220051547134e-08\n",
      "Grad CAAN.W_V.bias: 5.412845212049433e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.6134443967530387e-06\n",
      "Grad CAAN.scorer.bias: 6.2584840634372085e-06\n",
      "Grad layer_norm.weight: 5.1973442910524525e-11\n",
      "Grad layer_norm.bias: 9.697757180626709e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.834975574266373e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.4666838844010783e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.5330554881121117e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 8.346708740836206e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3200502323174046e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.592116972455205e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.912950089473121e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 8.291974745722186e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.180796931927631e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.3146344830269356e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.503218743399316e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.1434104208472036e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.3010253691910805e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.886535065473652e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.018585743002177e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.218393690966991e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.4207302828926913e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.7971178151453557e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.2976284763664978e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.279880976871709e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.557482835565452e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 5.428391514072928e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.5612838072675004e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.225969535947115e-08\n",
      "Grad SREM.ff.weight: 3.6976377515429704e-08\n",
      "Grad SREM.ff.bias: 4.313116619414359e-08\n",
      "Grad CAAN.W_Q.weight: 1.850922526358545e-09\n",
      "Grad CAAN.W_Q.bias: 2.9089358699607715e-10\n",
      "Grad CAAN.W_K.weight: 1.950617445345415e-09\n",
      "Grad CAAN.W_K.bias: 1.3022144940057906e-13\n",
      "Grad CAAN.W_V.weight: 8.182404798162679e-08\n",
      "Grad CAAN.W_V.bias: 5.421333071353729e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.6148073882504832e-06\n",
      "Grad CAAN.scorer.bias: 6.2682975112693384e-06\n",
      "Grad layer_norm.weight: 5.0916878352458284e-11\n",
      "Grad layer_norm.bias: 9.64156809946104e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.8787882707926862e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.5446874602218514e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.5711772161086657e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 8.557406866449568e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3465417149360892e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.6469548775054363e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.9775626408934386e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 8.479497659585888e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.2278387468155358e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.424041688544378e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.558678602326836e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.213491917009037e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.403503950367593e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.001454250752602e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.0654107313333725e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.268875753941302e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.473760085663912e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.858122627458215e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.346110805628854e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.388446355780729e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.6635171280229315e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 5.554348092573491e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.61640415999409e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.296070261171735e-08\n",
      "Grad SREM.ff.weight: 3.78207545281839e-08\n",
      "Grad SREM.ff.bias: 4.4115605390970813e-08\n",
      "Grad CAAN.W_Q.weight: 1.8427944725729617e-09\n",
      "Grad CAAN.W_Q.bias: 2.9529270695327625e-10\n",
      "Grad CAAN.W_K.weight: 1.9395514083697662e-09\n",
      "Grad CAAN.W_K.bias: 1.3189000943897994e-13\n",
      "Grad CAAN.W_V.weight: 8.359722158957084e-08\n",
      "Grad CAAN.W_V.bias: 5.53729648800072e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.6500245010320214e-06\n",
      "Grad CAAN.scorer.bias: 6.40237840343616e-06\n",
      "Grad layer_norm.weight: 5.241760150931363e-11\n",
      "Grad layer_norm.bias: 9.894270819321704e-11\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 47: last loss = -10.12767\n",
      "eval step --\n",
      "Step 47: val_rewards = 0.3415940036428253\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.4104520303143175e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 8.395095382751627e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.497507838901015e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.6352775036865275e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.340996308417223e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 8.377335283804577e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 9.63599339209864e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.6360080784826323e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.774547077146529e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.092099295671119e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 7.463685225417294e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 9.288929070372376e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.2525480652669785e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.384826142647455e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 5.02887120834572e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 5.499597999225614e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 7.246649808001493e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 8.313769117584968e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 8.365092646345218e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.8295288894965012e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.1746955630442812e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.3480274674293469e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.0938052241726837e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.256749548872449e-08\n",
      "Grad SREM.ff.weight: 1.2819580952339038e-08\n",
      "Grad SREM.ff.bias: 1.5076542680958482e-08\n",
      "Grad CAAN.W_Q.weight: 1.3152595679244428e-09\n",
      "Grad CAAN.W_Q.bias: 2.6190202784270866e-10\n",
      "Grad CAAN.W_K.weight: 1.5906872485871304e-09\n",
      "Grad CAAN.W_K.bias: 1.4195929994671383e-14\n",
      "Grad CAAN.W_V.weight: 2.6241604444976474e-08\n",
      "Grad CAAN.W_V.bias: 1.6202677954879618e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 4.303763034840813e-07\n",
      "Grad CAAN.scorer.bias: 1.880482841443154e-06\n",
      "Grad layer_norm.weight: 1.674511744542695e-11\n",
      "Grad layer_norm.bias: 2.2918411959893525e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.74848371629011e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.0533158546766064e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.769953793586467e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.010082031378424e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.936416289098467e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 9.858584434918605e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.1275266310895038e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.16362769314793e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.991227570139145e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.1040019592956085e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 8.992872002622221e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.1277493383587966e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.4970731598840104e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.6673394609423298e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 6.155830289955588e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 6.761040061142864e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 8.705564047417624e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.0000096584139584e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 9.96379020956617e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.189307068967139e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.4374178514131586e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.6534570379533875e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.3037936064108635e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.5033418065968362e-08\n",
      "Grad SREM.ff.weight: 1.542974459312063e-08\n",
      "Grad SREM.ff.bias: 1.812643191101415e-08\n",
      "Grad CAAN.W_Q.weight: 1.4509010659580213e-09\n",
      "Grad CAAN.W_Q.bias: 3.055131425622193e-10\n",
      "Grad CAAN.W_K.weight: 1.720213971267981e-09\n",
      "Grad CAAN.W_K.bias: 1.4515227534468364e-14\n",
      "Grad CAAN.W_V.weight: 3.1394797872508207e-08\n",
      "Grad CAAN.W_V.bias: 1.9473432644190325e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 5.149795470060781e-07\n",
      "Grad CAAN.scorer.bias: 2.260086830574437e-06\n",
      "Grad layer_norm.weight: 1.6180678324984044e-11\n",
      "Grad layer_norm.bias: 2.8503345517183476e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.0126620896000471e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 9.256924220213403e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 4.50887209513251e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.0816518647714943e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.518274635325394e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.065956524437528e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.2230843074445552e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.4224158229623214e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 6.219619957947842e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.1423665485787993e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.874083772842823e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.2172483021544167e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.7041344724244567e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.8982256044797197e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 6.947198372131425e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 7.664482937208561e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0477770040484558e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.2051169784310645e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.2352432965379023e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.736458837304667e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.6198511421094963e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.8579549010411256e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.669006977067511e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.8925573996853018e-08\n",
      "Grad SREM.ff.weight: 1.8747508434557858e-08\n",
      "Grad SREM.ff.bias: 2.2041193759037014e-08\n",
      "Grad CAAN.W_Q.weight: 2.448397262710955e-09\n",
      "Grad CAAN.W_Q.bias: 4.189287250433438e-10\n",
      "Grad CAAN.W_K.weight: 2.4725654856894153e-09\n",
      "Grad CAAN.W_K.bias: 2.0496218654589282e-14\n",
      "Grad CAAN.W_V.weight: 3.776959545120917e-08\n",
      "Grad CAAN.W_V.bias: 2.2919203956917045e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 6.282018603087636e-07\n",
      "Grad CAAN.scorer.bias: 2.6600007458910113e-06\n",
      "Grad layer_norm.weight: 2.714632685307805e-11\n",
      "Grad layer_norm.bias: 2.9584962951689775e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.2979445643268583e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.6964093324522977e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.2010576401251285e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.6734120125992966e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 7.588469783104124e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.2586773637246473e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.4449940376193293e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 4.0290284997190895e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 7.470287582966861e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.4057077724061173e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.1685299394770254e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.455938480887653e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.0162051195260489e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.2579392522370512e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.464761669380039e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.371451392681251e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.2491689627136537e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.4356246746949353e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.4621574784268176e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.2163674501539674e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.9810695306432535e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.27434626687284e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.958679263225349e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.230291684668373e-08\n",
      "Grad SREM.ff.weight: 2.2554125678198034e-08\n",
      "Grad SREM.ff.bias: 2.653092501248011e-08\n",
      "Grad CAAN.W_Q.weight: 3.4056149011973957e-09\n",
      "Grad CAAN.W_Q.bias: 5.912846878786127e-10\n",
      "Grad CAAN.W_K.weight: 3.2036215902309095e-09\n",
      "Grad CAAN.W_K.bias: 1.6828188216283686e-14\n",
      "Grad CAAN.W_V.weight: 4.4916831143382296e-08\n",
      "Grad CAAN.W_V.bias: 2.7320044182488346e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 7.538718023170077e-07\n",
      "Grad CAAN.scorer.bias: 3.1707604648545384e-06\n",
      "Grad layer_norm.weight: 3.2723743853546594e-11\n",
      "Grad layer_norm.bias: 4.406227466224877e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.2980679031660003e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.8097608481260075e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.245126208364148e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.903833076772777e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 7.719655643889656e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.3094896694765268e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.4962683869690352e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 4.194626243569921e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 7.396582651919559e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.4982842744259983e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.2163089424532814e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.5186613078199684e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.0978535575366664e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.3516952274427183e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.815350671653732e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.768668096654665e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.2927736658951972e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.485655071187253e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.5058831959180452e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.318096075677346e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.0611596873720828e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.366696172373395e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.0106549314391486e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.293295509048221e-08\n",
      "Grad SREM.ff.weight: 2.323998238296099e-08\n",
      "Grad SREM.ff.bias: 2.732982373743198e-08\n",
      "Grad CAAN.W_Q.weight: 3.3558182899184885e-09\n",
      "Grad CAAN.W_Q.bias: 5.90265558653158e-10\n",
      "Grad CAAN.W_K.weight: 3.1430726910031126e-09\n",
      "Grad CAAN.W_K.bias: 1.810589337150354e-14\n",
      "Grad CAAN.W_V.weight: 4.6283894050702656e-08\n",
      "Grad CAAN.W_V.bias: 2.827072194122593e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 7.763609346511657e-07\n",
      "Grad CAAN.scorer.bias: 3.2810958146001212e-06\n",
      "Grad layer_norm.weight: 3.181342689284605e-11\n",
      "Grad layer_norm.bias: 4.645704654304694e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.6837444632988863e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.2081698408958914e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 8.655413885616525e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 6.303124688855632e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.1274028585694928e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.2316181888526465e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.492907096030006e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 7.211700681075683e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 9.54508277972721e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 3.2715988251830197e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.126541387426073e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.6676347797405242e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.615012733604317e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.070876691297798e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.5388035290442303e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.7030481469504366e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.08978667703974e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.4147903121551906e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.391561948478227e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.34443045374644e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.52172135897888e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 4.101829986780103e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.0805519912746604e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.590638897321696e-08\n",
      "Grad SREM.ff.weight: 3.7484149117972265e-08\n",
      "Grad SREM.ff.bias: 4.3967506968556336e-08\n",
      "Grad CAAN.W_Q.weight: 3.4481237864980585e-09\n",
      "Grad CAAN.W_Q.bias: 6.984068323667714e-10\n",
      "Grad CAAN.W_K.weight: 3.2387166282177304e-09\n",
      "Grad CAAN.W_K.bias: 1.5374976140326846e-14\n",
      "Grad CAAN.W_V.weight: 7.555914294243848e-08\n",
      "Grad CAAN.W_V.bias: 4.7365026034640323e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.248009880328027e-06\n",
      "Grad CAAN.scorer.bias: 5.497180609381758e-06\n",
      "Grad layer_norm.weight: 3.4579183266769675e-11\n",
      "Grad layer_norm.bias: 8.470093376677923e-11\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 2.1053188706265935e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 4.0715757154696774e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.1637244640150257e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 8.196601036791762e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3159585400546181e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.6580090559114034e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.986767777546362e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 8.640097665102431e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.355871664943109e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.19834805809316e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.5760065192059756e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.2212659206720673e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.354056837296838e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.903009109824552e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.862588971945911e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.0637223041575226e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.5076990484151906e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.8997385048690205e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.825470413192477e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.317634748675971e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.25831414574418e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 4.972019684856832e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.6177894457978255e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.241142903538275e-08\n",
      "Grad SREM.ff.weight: 4.500291339581963e-08\n",
      "Grad SREM.ff.bias: 5.282526771566154e-08\n",
      "Grad CAAN.W_Q.weight: 4.1149506024851235e-09\n",
      "Grad CAAN.W_Q.bias: 7.683676472858281e-10\n",
      "Grad CAAN.W_K.weight: 4.139293352523055e-09\n",
      "Grad CAAN.W_K.bias: 1.4931046172670867e-14\n",
      "Grad CAAN.W_V.weight: 9.02013255199563e-08\n",
      "Grad CAAN.W_V.bias: 5.683643848897191e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.485164375480963e-06\n",
      "Grad CAAN.scorer.bias: 6.596430466743186e-06\n",
      "Grad layer_norm.weight: 4.201807304871075e-11\n",
      "Grad layer_norm.bias: 1.100481095139827e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 2.114167348132856e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 4.087959137866193e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.1684163359060307e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 8.22929224764124e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3194216552658844e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.665801954182534e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.9978148435860774e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 8.66884897199327e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.3623138728213746e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.2116934939606665e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.5860715791026223e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.2336133770627384e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.368958250733357e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.920660545693067e-10\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.869442822766132e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.0719350679598847e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.5176760676259846e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.9113456090357204e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.837024504209751e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.342608660503402e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.274904874534968e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 4.991356661321333e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.632635880990165e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.258496844045112e-08\n",
      "Grad SREM.ff.weight: 4.5191132613808804e-08\n",
      "Grad SREM.ff.bias: 5.304725192445403e-08\n",
      "Grad CAAN.W_Q.weight: 4.123592134419596e-09\n",
      "Grad CAAN.W_Q.bias: 7.714905381206449e-10\n",
      "Grad CAAN.W_K.weight: 4.152799437662225e-09\n",
      "Grad CAAN.W_K.bias: 1.4660497073054266e-14\n",
      "Grad CAAN.W_V.weight: 9.05802508555098e-08\n",
      "Grad CAAN.W_V.bias: 5.707944978894375e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.4914586472514202e-06\n",
      "Grad CAAN.scorer.bias: 6.624633897445165e-06\n",
      "Grad layer_norm.weight: 4.226863650758084e-11\n",
      "Grad layer_norm.bias: 1.103757710230191e-10\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 55: last loss = 2.11414\n",
      "eval step --\n",
      "Step 55: val_rewards = 0.3996844406483186\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.9601282402748055e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.848916636112974e-12\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.1387203679991131e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.1298355834125102e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.192190118327885e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.2032200322892317e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.4674572321182744e-12\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.7296255573617803e-12\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.6068334163232834e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.4646998891532093e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.147473766675681e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.53029635974589e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.5218026491736758e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.5312352080742997e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.94667111491831e-11\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 4.145846860259539e-11\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 5.6144065091467965e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 6.0977932772599e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 8.51486329361828e-12\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.699123133858116e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 8.15708264823023e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 8.862229700090651e-11\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.0644324310860043e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.2765274393089499e-09\n",
      "Grad SREM.ff.weight: 8.144727114967054e-10\n",
      "Grad SREM.ff.bias: 9.027847913678499e-10\n",
      "Grad CAAN.W_Q.weight: 3.622009636661261e-10\n",
      "Grad CAAN.W_Q.bias: 6.178531297695855e-11\n",
      "Grad CAAN.W_K.weight: 3.7505923367042726e-10\n",
      "Grad CAAN.W_K.bias: 2.504617344482895e-15\n",
      "Grad CAAN.W_V.weight: 1.6059941154722424e-09\n",
      "Grad CAAN.W_V.bias: 5.243599332516169e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.714180169505198e-08\n",
      "Grad CAAN.scorer.bias: 6.086906978453044e-08\n",
      "Grad layer_norm.weight: 4.556973288299959e-12\n",
      "Grad layer_norm.bias: 3.462549040891827e-12\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 0.014181716367602348\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 0.01651279255747795\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 0.059388838708400726\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 0.023152777925133705\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 0.0031653093174099922\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 0.004326429218053818\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 0.005116861313581467\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 0.013478012755513191\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 0.09422089159488678\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 0.12749499082565308\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 0.44709694385528564\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 0.5660657286643982\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 0.09778808802366257\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 0.1081184595823288\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 0.4585218131542206\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 0.48194894194602966\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 0.047199878841638565\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 0.0546589195728302\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 0.04878076910972595\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 0.1106589064002037\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0041258335113525\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2134286165237427\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.329014778137207\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.896955490112305\n",
      "Grad SREM.ff.weight: 7.299441337585449\n",
      "Grad SREM.ff.bias: 8.577320098876953\n",
      "Grad CAAN.W_Q.weight: 0.023424066603183746\n",
      "Grad CAAN.W_Q.bias: 0.056546833366155624\n",
      "Grad CAAN.W_K.weight: 0.034793317317962646\n",
      "Grad CAAN.W_K.bias: 2.494454520274303e-06\n",
      "Grad CAAN.W_V.weight: 17.00889015197754\n",
      "Grad CAAN.W_V.bias: 118.5928955078125\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 320.8648986816406\n",
      "Grad CAAN.scorer.bias: 1376.64453125\n",
      "Grad layer_norm.weight: 0.027695951983332634\n",
      "Grad layer_norm.bias: 0.03718150779604912\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 0.014181716367602348\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 0.01651279255747795\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 0.059388838708400726\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 0.023152777925133705\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 0.0031653093174099922\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 0.004326429218053818\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 0.005116861313581467\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 0.013478012755513191\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 0.09422089159488678\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 0.12749499082565308\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 0.44709694385528564\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 0.5660657286643982\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 0.09778808802366257\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 0.1081184595823288\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 0.4585218131542206\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 0.48194894194602966\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 0.047199878841638565\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 0.0546589195728302\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 0.04878076910972595\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 0.1106589064002037\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0041258335113525\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2134286165237427\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.329014778137207\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.896955490112305\n",
      "Grad SREM.ff.weight: 7.299441337585449\n",
      "Grad SREM.ff.bias: 8.577320098876953\n",
      "Grad CAAN.W_Q.weight: 0.023424066603183746\n",
      "Grad CAAN.W_Q.bias: 0.056546833366155624\n",
      "Grad CAAN.W_K.weight: 0.034793317317962646\n",
      "Grad CAAN.W_K.bias: 2.494454520274303e-06\n",
      "Grad CAAN.W_V.weight: 17.00889015197754\n",
      "Grad CAAN.W_V.bias: 118.5928955078125\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 320.8648986816406\n",
      "Grad CAAN.scorer.bias: 1376.64453125\n",
      "Grad layer_norm.weight: 0.027695951983332634\n",
      "Grad layer_norm.bias: 0.03718150779604912\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 0.014181716367602348\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 0.01651279255747795\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 0.059388838708400726\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 0.023152777925133705\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 0.0031653093174099922\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 0.004326429218053818\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 0.005116861313581467\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 0.013478012755513191\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 0.09422089159488678\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 0.12749499082565308\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 0.44709694385528564\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 0.5660657286643982\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 0.09778808802366257\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 0.1081184595823288\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 0.4585218131542206\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 0.48194894194602966\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 0.047199878841638565\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 0.0546589195728302\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 0.04878076910972595\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 0.1106589064002037\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0041258335113525\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2134286165237427\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.329014778137207\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.896955490112305\n",
      "Grad SREM.ff.weight: 7.299441337585449\n",
      "Grad SREM.ff.bias: 8.577320098876953\n",
      "Grad CAAN.W_Q.weight: 0.023424066603183746\n",
      "Grad CAAN.W_Q.bias: 0.056546833366155624\n",
      "Grad CAAN.W_K.weight: 0.034793317317962646\n",
      "Grad CAAN.W_K.bias: 2.494454520274303e-06\n",
      "Grad CAAN.W_V.weight: 17.00889015197754\n",
      "Grad CAAN.W_V.bias: 118.5928955078125\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 320.8648986816406\n",
      "Grad CAAN.scorer.bias: 1376.64453125\n",
      "Grad layer_norm.weight: 0.027695951983332634\n",
      "Grad layer_norm.bias: 0.03718150779604912\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 0.014181716367602348\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 0.01651279255747795\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 0.059388838708400726\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 0.023152777925133705\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 0.0031653093174099922\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 0.004326429218053818\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 0.005116861313581467\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 0.013478012755513191\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 0.09422089159488678\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 0.12749499082565308\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 0.44709694385528564\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 0.5660657286643982\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 0.09778808802366257\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 0.1081184595823288\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 0.4585218131542206\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 0.48194894194602966\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 0.047199878841638565\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 0.0546589195728302\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 0.04878076910972595\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 0.1106589064002037\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0041258335113525\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2134286165237427\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.329014778137207\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.896955490112305\n",
      "Grad SREM.ff.weight: 7.299441337585449\n",
      "Grad SREM.ff.bias: 8.577320098876953\n",
      "Grad CAAN.W_Q.weight: 0.023424066603183746\n",
      "Grad CAAN.W_Q.bias: 0.056546833366155624\n",
      "Grad CAAN.W_K.weight: 0.034793317317962646\n",
      "Grad CAAN.W_K.bias: 2.494454520274303e-06\n",
      "Grad CAAN.W_V.weight: 17.00889015197754\n",
      "Grad CAAN.W_V.bias: 118.5928955078125\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 320.8648986816406\n",
      "Grad CAAN.scorer.bias: 1376.64453125\n",
      "Grad layer_norm.weight: 0.027695951983332634\n",
      "Grad layer_norm.bias: 0.03718150779604912\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 0.014181716367602348\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 0.01651279255747795\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 0.059388838708400726\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 0.023152777925133705\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 0.0031653093174099922\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 0.004326429218053818\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 0.005116861313581467\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 0.013478012755513191\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 0.09422089159488678\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 0.12749499082565308\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 0.44709694385528564\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 0.5660657286643982\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 0.09778808802366257\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 0.1081184595823288\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 0.4585218131542206\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 0.48194894194602966\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 0.047199878841638565\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 0.0546589195728302\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 0.04878076910972595\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 0.1106589064002037\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0041258335113525\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2134286165237427\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.329014778137207\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.896955490112305\n",
      "Grad SREM.ff.weight: 7.299441337585449\n",
      "Grad SREM.ff.bias: 8.577320098876953\n",
      "Grad CAAN.W_Q.weight: 0.023424068465828896\n",
      "Grad CAAN.W_Q.bias: 0.056546833366155624\n",
      "Grad CAAN.W_K.weight: 0.034793317317962646\n",
      "Grad CAAN.W_K.bias: 2.494454520274303e-06\n",
      "Grad CAAN.W_V.weight: 17.00889015197754\n",
      "Grad CAAN.W_V.bias: 118.5928955078125\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 320.8648986816406\n",
      "Grad CAAN.scorer.bias: 1376.64453125\n",
      "Grad layer_norm.weight: 0.027695951983332634\n",
      "Grad layer_norm.bias: 0.03718150779604912\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 0.014181716367602348\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 0.01651279255747795\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 0.059388838708400726\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 0.023152777925133705\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 0.0031653093174099922\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 0.004326429218053818\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 0.005116861313581467\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 0.013478012755513191\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 0.09422089159488678\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 0.12749499082565308\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 0.44709694385528564\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 0.5660657286643982\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 0.09778808802366257\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 0.1081184595823288\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 0.4585218131542206\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 0.48194894194602966\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 0.047199878841638565\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 0.0546589195728302\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 0.04878076910972595\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 0.1106589064002037\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0041258335113525\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2134286165237427\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.329014778137207\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.896955490112305\n",
      "Grad SREM.ff.weight: 7.299441337585449\n",
      "Grad SREM.ff.bias: 8.577320098876953\n",
      "Grad CAAN.W_Q.weight: 0.023424068465828896\n",
      "Grad CAAN.W_Q.bias: 0.056546833366155624\n",
      "Grad CAAN.W_K.weight: 0.034793317317962646\n",
      "Grad CAAN.W_K.bias: 2.494454520274303e-06\n",
      "Grad CAAN.W_V.weight: 17.00889015197754\n",
      "Grad CAAN.W_V.bias: 118.5928955078125\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 320.8648986816406\n",
      "Grad CAAN.scorer.bias: 1376.64453125\n",
      "Grad layer_norm.weight: 0.027695951983332634\n",
      "Grad layer_norm.bias: 0.03718150779604912\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 0.014181716367602348\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 0.01651279255747795\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 0.059388838708400726\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 0.023152777925133705\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 0.0031653093174099922\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 0.004326429218053818\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 0.005116861313581467\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 0.013478012755513191\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 0.09422089159488678\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 0.12749499082565308\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 0.44709694385528564\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 0.5660657286643982\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 0.09778808802366257\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 0.1081184595823288\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 0.4585218131542206\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 0.48194894194602966\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 0.047199878841638565\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 0.0546589195728302\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 0.04878076910972595\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 0.1106589064002037\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0041258335113525\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.2134286165237427\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.329014778137207\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.896955490112305\n",
      "Grad SREM.ff.weight: 7.299441337585449\n",
      "Grad SREM.ff.bias: 8.577320098876953\n",
      "Grad CAAN.W_Q.weight: 0.023424068465828896\n",
      "Grad CAAN.W_Q.bias: 0.056546833366155624\n",
      "Grad CAAN.W_K.weight: 0.034793317317962646\n",
      "Grad CAAN.W_K.bias: 2.494454520274303e-06\n",
      "Grad CAAN.W_V.weight: 17.00889015197754\n",
      "Grad CAAN.W_V.bias: 118.5928955078125\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 320.8648986816406\n",
      "Grad CAAN.scorer.bias: 1376.64453125\n",
      "Grad layer_norm.weight: 0.027695951983332634\n",
      "Grad layer_norm.bias: 0.03718150779604912\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 63: last loss = 58.58237\n",
      "eval step --\n",
      "Step 63: val_rewards = 0.43957525777402257\n",
      "*** found better model ***\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 5.047259499235679e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 4.3802890203892275e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.4226928968241737e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.958294136907625e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 7.044707039938203e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 6.134648344868765e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.4763381628313255e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 8.004654161730684e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.0395033217823766e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.3354130246057139e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.891105085860545e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.853887994229808e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.76133684198976e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.834970125320524e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.2129686410844442e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.4522354874202392e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 4.144053988852647e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.8252743195243966e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.2531295112694352e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.925770179577739e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.7429390431498177e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 4.0849585047908477e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.8685969937214395e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.251450041512726e-06\n",
      "Grad SREM.ff.weight: 2.498615913282265e-06\n",
      "Grad SREM.ff.bias: 1.7769518763088854e-06\n",
      "Grad CAAN.W_Q.weight: 3.6783067116630264e-06\n",
      "Grad CAAN.W_Q.bias: 1.3384365047386382e-06\n",
      "Grad CAAN.W_K.weight: 2.26072438636038e-06\n",
      "Grad CAAN.W_K.bias: 1.1884822292973496e-10\n",
      "Grad CAAN.W_V.weight: 1.6068258901213994e-06\n",
      "Grad CAAN.W_V.bias: 1.6129964919286977e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.681705493363552e-05\n",
      "Grad CAAN.scorer.bias: 1.9041181076318026e-07\n",
      "Grad layer_norm.weight: 2.784609520745107e-08\n",
      "Grad layer_norm.bias: 1.2315574338117585e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 8.767405290655006e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.1056250315277794e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.4619457999742735e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 7.087383124826374e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 5.887710763374798e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 6.5210059574383195e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.3829600575832046e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 9.477616025321822e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.1229296010005783e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.452391925089614e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.5957047025949578e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.1031828509876505e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 4.129096566884982e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 3.4924670444524963e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.420794433215633e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.8029205029488367e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 5.156666560424128e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 5.1884484264519415e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.6120354473514453e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.0301880593033275e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 4.355503733677324e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 5.287727617542259e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 4.0309623727807775e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.5112530895275995e-05\n",
      "Grad SREM.ff.weight: 3.378328256076202e-05\n",
      "Grad SREM.ff.bias: 2.567527917562984e-05\n",
      "Grad CAAN.W_Q.weight: 4.202288982924074e-05\n",
      "Grad CAAN.W_Q.bias: 1.3138064787199255e-05\n",
      "Grad CAAN.W_K.weight: 3.4485965443309397e-05\n",
      "Grad CAAN.W_K.bias: 6.781159522795122e-10\n",
      "Grad CAAN.W_V.weight: 1.9676343072205782e-05\n",
      "Grad CAAN.W_V.bias: 5.0423530240095715e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004966935375705361\n",
      "Grad CAAN.scorer.bias: 5.956899258308113e-07\n",
      "Grad layer_norm.weight: 4.205093944165128e-07\n",
      "Grad layer_norm.bias: 8.486184128742025e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.1409363054326604e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.265574383069179e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.030716927947651e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 7.797081735816391e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3770927509426656e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.440986974188263e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.8402017565886126e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.1287966117379256e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 5.381533583204146e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 3.01792425716485e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.9871232527511893e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.3406327020202298e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 7.638164447598683e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.804845386592206e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.358625124543323e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.63160573519417e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 7.936667856256463e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 7.924275280402071e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.5444961693210644e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.6160598192982434e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 7.509592705901014e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 8.773749868851155e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.956256402190775e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 5.833852992509492e-05\n",
      "Grad SREM.ff.weight: 5.722264540963806e-05\n",
      "Grad SREM.ff.bias: 4.473079752642661e-05\n",
      "Grad CAAN.W_Q.weight: 7.593522605020553e-05\n",
      "Grad CAAN.W_Q.bias: 3.116453444818035e-05\n",
      "Grad CAAN.W_K.weight: 6.425488390959799e-05\n",
      "Grad CAAN.W_K.bias: 5.935872904316852e-10\n",
      "Grad CAAN.W_V.weight: 3.1428415240952745e-05\n",
      "Grad CAAN.W_V.bias: 1.1263467492028667e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.000806396477855742\n",
      "Grad CAAN.scorer.bias: 1.334556145593524e-07\n",
      "Grad layer_norm.weight: 6.044231213309104e-07\n",
      "Grad layer_norm.bias: 8.320188840116316e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.179505346726728e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 8.925984218421945e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.440246677475443e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.9865525991444883e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.1239553288078241e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.1672062427692254e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.373750345758708e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.6368636224228794e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.1166499070423015e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.399472691649862e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.579235908546252e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.556982730719028e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 9.364545690004888e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 7.57989369049028e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.204920176358428e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.2594819054356776e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0694257923660189e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.0721282706072088e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.4181243790953886e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.1887376533413772e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.0384626875747927e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.211396465805592e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 9.3972819740884e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 7.86310265539214e-05\n",
      "Grad SREM.ff.weight: 7.681197166675702e-05\n",
      "Grad SREM.ff.bias: 5.9777717979159206e-05\n",
      "Grad CAAN.W_Q.weight: 9.748144657351077e-05\n",
      "Grad CAAN.W_Q.bias: 3.840748468064703e-05\n",
      "Grad CAAN.W_K.weight: 8.453871123492718e-05\n",
      "Grad CAAN.W_K.bias: 6.96909363551157e-10\n",
      "Grad CAAN.W_V.weight: 4.390939648146741e-05\n",
      "Grad CAAN.W_V.bias: 6.601576529874364e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0011329184053465724\n",
      "Grad CAAN.scorer.bias: 7.79058609623462e-07\n",
      "Grad layer_norm.weight: 7.425467174471123e-07\n",
      "Grad layer_norm.bias: 3.9194640066853026e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.47023011121928e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.777176733725355e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.9558848407541518e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 9.221407992754393e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.4628597888588502e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.5202939351866007e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.7624986621030985e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.7634380117547153e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.3875713256747986e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.725120111790602e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.949560090812156e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 8.411903763771988e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.5184824633251992e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.3102653610985726e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 5.861638328497065e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.074553094644216e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.9297294784337282e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.9661172245832859e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 5.932118938289932e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.765251221921062e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.8794858988258056e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.2461836124421097e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 0.00016753248928580433\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 0.00013515861064661294\n",
      "Grad SREM.ff.weight: 0.00013704966113436967\n",
      "Grad SREM.ff.bias: 0.00010788941290229559\n",
      "Grad CAAN.W_Q.weight: 0.000198208203073591\n",
      "Grad CAAN.W_Q.bias: 7.823618216207251e-05\n",
      "Grad CAAN.W_K.weight: 0.00014082295820116997\n",
      "Grad CAAN.W_K.bias: 8.354651970243765e-10\n",
      "Grad CAAN.W_V.weight: 8.239049202529714e-05\n",
      "Grad CAAN.W_V.bias: 1.4713683071931882e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0021086297929286957\n",
      "Grad CAAN.scorer.bias: 1.7390266293659806e-06\n",
      "Grad layer_norm.weight: 9.382016287418082e-06\n",
      "Grad layer_norm.bias: 1.0192236004513688e-05\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.472501005802769e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.773578545311466e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.9620206614945346e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 9.19526712550578e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.4638936285393811e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.522846559964819e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.771563411054558e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.770781470928796e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.3985992331508896e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.724236480844411e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.9648705246218015e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 8.433155016973615e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.5217917734844377e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.3134905429978971e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 5.8744553825818e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.0777079043909907e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.9336356160692958e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.9701926134985115e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 5.945650514149747e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.773953380914463e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.883538243419025e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.2510537746711634e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 0.0001679254201008007\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 0.00013550135190598667\n",
      "Grad SREM.ff.weight: 0.00013736344408243895\n",
      "Grad SREM.ff.bias: 0.00010813402332132682\n",
      "Grad CAAN.W_Q.weight: 0.00019852629338856786\n",
      "Grad CAAN.W_Q.bias: 7.83305658842437e-05\n",
      "Grad CAAN.W_K.weight: 0.00014111094060353935\n",
      "Grad CAAN.W_K.bias: 8.260120365477519e-10\n",
      "Grad CAAN.W_V.weight: 8.259076275862753e-05\n",
      "Grad CAAN.W_V.bias: 1.4715445217916567e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.002113770227879286\n",
      "Grad CAAN.scorer.bias: 1.7392219433531864e-06\n",
      "Grad layer_norm.weight: 9.380129085911904e-06\n",
      "Grad layer_norm.bias: 1.0193576599704102e-05\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.593170489097247e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.644872539458447e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.1445144682274986e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 9.535683886952029e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.6099637178967896e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.7419797160300732e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 5.642361955437991e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.4087431544094215e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.762306448606978e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 3.1023174074107374e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 7.574169558210997e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0380561434431002e-05\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.8482519408280496e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.6323149338859366e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 7.083225682436023e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.3002551188255893e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.3241503299686883e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.3811091409697838e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 6.916685606483952e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.401197486458841e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.25642306759255e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.7042857254855335e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 0.00020003739336971194\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 0.00015689719293732196\n",
      "Grad SREM.ff.weight: 0.0001600610266905278\n",
      "Grad SREM.ff.bias: 0.00012560549657791853\n",
      "Grad CAAN.W_Q.weight: 0.0002205672935815528\n",
      "Grad CAAN.W_Q.bias: 8.602583693573251e-05\n",
      "Grad CAAN.W_K.weight: 0.00015864160377532244\n",
      "Grad CAAN.W_K.bias: 8.95281626611677e-10\n",
      "Grad CAAN.W_V.weight: 0.00010140244557987899\n",
      "Grad CAAN.W_V.bias: 2.820052884544566e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.002564673312008381\n",
      "Grad CAAN.scorer.bias: 3.332663936816971e-06\n",
      "Grad layer_norm.weight: 9.48078104556771e-06\n",
      "Grad layer_norm.bias: 1.001848158921348e-05\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 9.592381502443459e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.64585138313123e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.1628234492254705e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 9.580810456100153e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.6181864737063734e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.7413443131886197e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 5.708760753009301e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.419732763632055e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.7925582319076057e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 3.109672093160043e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 7.6415526564233e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0465386367286555e-05\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.8718093315328588e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.6512830143255997e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 7.137321063055424e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.312421656824881e-06\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.3419752892550605e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.399871732450265e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.028443178569432e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.4744425053977466e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.272266283398494e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.7233976652496494e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 0.00020343801588751376\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 0.0001603290147613734\n",
      "Grad SREM.ff.weight: 0.0001628521567909047\n",
      "Grad SREM.ff.bias: 0.00012806344602722675\n",
      "Grad CAAN.W_Q.weight: 0.00022427648946177214\n",
      "Grad CAAN.W_Q.bias: 8.717078890185803e-05\n",
      "Grad CAAN.W_K.weight: 0.00016168670845218003\n",
      "Grad CAAN.W_K.bias: 8.702150111616902e-10\n",
      "Grad CAAN.W_V.weight: 0.0001036096946336329\n",
      "Grad CAAN.W_V.bias: 2.9736327178397914e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0026170588098466396\n",
      "Grad CAAN.scorer.bias: 3.5140826639690204e-06\n",
      "Grad layer_norm.weight: 9.478565516474191e-06\n",
      "Grad layer_norm.bias: 1.0019522051152308e-05\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 71: last loss = 8.05827\n",
      "eval step --\n",
      "Step 71: val_rewards = 0.3226093675351585\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 8.95441321091539e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.49452677695217e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 8.346285440552492e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.7984015932579496e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.658653628477396e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 6.762187476638815e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.594698484375101e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.7025945370517945e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.4699637890535087e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.6361102694872898e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.04841925830624e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.619695121415134e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.8249060929065308e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.0032279007532452e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.856979541832061e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.0821378459979769e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.2747803640754682e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.0410015899585687e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 8.730340894658184e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.547438425817063e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.512495324770498e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.1240221908792591e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.0775408959016204e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.2897310120461043e-07\n",
      "Grad SREM.ff.weight: 1.4827780887571862e-06\n",
      "Grad SREM.ff.bias: 3.7900537108725985e-07\n",
      "Grad CAAN.W_Q.weight: 3.123303486063378e-06\n",
      "Grad CAAN.W_Q.bias: 9.539002121528029e-07\n",
      "Grad CAAN.W_K.weight: 2.6512841486692196e-06\n",
      "Grad CAAN.W_K.bias: 2.941738172501651e-11\n",
      "Grad CAAN.W_V.weight: 1.6724226270525833e-06\n",
      "Grad CAAN.W_V.bias: 5.162927418922436e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.184304659953341e-05\n",
      "Grad CAAN.scorer.bias: 6.093432602938265e-08\n",
      "Grad layer_norm.weight: 6.504201621737593e-08\n",
      "Grad layer_norm.bias: 4.1707124864842626e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.581677177066922e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.181139890922168e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.2504919766342937e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.5428312849508075e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 9.005807766193641e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 9.31691390704259e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.0333759120916284e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 5.15826714675427e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.658675140627565e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.1157585905484666e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.3251755603960191e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.0255745053200371e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.9934280831867e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.9107060822420863e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.21388042027138e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 3.06391427784547e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 5.997384810996209e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 5.437324812618272e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.2479344607972962e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.124696158427696e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.588581082818564e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.1971745784176164e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.338851562759373e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 7.127735557332926e-07\n",
      "Grad SREM.ff.weight: 3.85562861993094e-06\n",
      "Grad SREM.ff.bias: 8.835791049932595e-07\n",
      "Grad CAAN.W_Q.weight: 4.913661996397423e-06\n",
      "Grad CAAN.W_Q.bias: 2.3005025013844715e-06\n",
      "Grad CAAN.W_K.weight: 5.242705356067745e-06\n",
      "Grad CAAN.W_K.bias: 3.893113331709941e-10\n",
      "Grad CAAN.W_V.weight: 3.1142940315476153e-06\n",
      "Grad CAAN.W_V.bias: 7.236753418737862e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 5.9633577620843425e-05\n",
      "Grad CAAN.scorer.bias: 8.569077181164175e-08\n",
      "Grad layer_norm.weight: 3.067168563575251e-07\n",
      "Grad layer_norm.bias: 1.9852100763273484e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.676302177766047e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.3661018739603605e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.475163540793801e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.375026930678814e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.7176171596133827e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.6020784698866919e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.36807789938598e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.513311558871578e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.7380189610303205e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.16212779189118e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.38499079993926e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.927544748374203e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.3650172547841066e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.0886193990700122e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.594144229486119e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.4221754440768564e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.6681398423988867e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.4412081245372974e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.048807973802468e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.430455374266785e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.4580411971110152e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.838039338508679e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.636619294411503e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 5.501103260030504e-06\n",
      "Grad SREM.ff.weight: 2.065097760350909e-05\n",
      "Grad SREM.ff.bias: 6.558643690368626e-06\n",
      "Grad CAAN.W_Q.weight: 3.5932273021899164e-05\n",
      "Grad CAAN.W_Q.bias: 1.7321237464784645e-05\n",
      "Grad CAAN.W_K.weight: 3.2053627364803106e-05\n",
      "Grad CAAN.W_K.bias: 2.263099707633387e-09\n",
      "Grad CAAN.W_V.weight: 2.1022098735556938e-05\n",
      "Grad CAAN.W_V.bias: 1.3232501316906564e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00039066278259269893\n",
      "Grad CAAN.scorer.bias: 1.562322722747922e-06\n",
      "Grad layer_norm.weight: 4.183659711998189e-07\n",
      "Grad layer_norm.bias: 4.5270800796970434e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.958072873430865e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.692617932595567e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.6515857953768318e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.5855552365783296e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.8142771729401375e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.6952361736599642e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.648122331474269e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.5142319337589925e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.394783559291682e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.478045750351157e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.497085453498585e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.891587306905421e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.3754495853390836e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.0890244084293954e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.4840868440587656e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.4529230440984975e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.6740611502873435e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.4462275760583907e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.070931858748736e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.5746625093888724e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.4261552223615581e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.794786498976464e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.7313217035261914e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 5.560615136346314e-06\n",
      "Grad SREM.ff.weight: 2.1199552065809257e-05\n",
      "Grad SREM.ff.bias: 6.6296647673880216e-06\n",
      "Grad CAAN.W_Q.weight: 3.744365676539019e-05\n",
      "Grad CAAN.W_Q.bias: 1.7484269847045653e-05\n",
      "Grad CAAN.W_K.weight: 3.32097515638452e-05\n",
      "Grad CAAN.W_K.bias: 2.1276185258045643e-09\n",
      "Grad CAAN.W_V.weight: 2.1891901269555092e-05\n",
      "Grad CAAN.W_V.bias: 1.427759599437195e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00040560640627518296\n",
      "Grad CAAN.scorer.bias: 1.685970346443355e-06\n",
      "Grad layer_norm.weight: 4.2591324245222495e-07\n",
      "Grad layer_norm.bias: 4.77012349620054e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 7.023623282975677e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.5555076983182516e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.5567812755866726e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.432646262207072e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.8371997256849681e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.70298608548336e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.672717768272605e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.8009810026929927e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.079601723105952e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 5.292966420711309e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.539972676160687e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 6.469169306910771e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.4501273426503758e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.1684485201612915e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.952361223331536e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.5582787682433263e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.8439902877153145e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.6435094113708146e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.1325143134399696e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.000271923767286e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.574107386659307e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.9829733446385944e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 4.0467435610480607e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 5.8947694014932495e-06\n",
      "Grad SREM.ff.weight: 2.2488833565148525e-05\n",
      "Grad SREM.ff.bias: 6.9866910052951425e-06\n",
      "Grad CAAN.W_Q.weight: 4.0864659240469337e-05\n",
      "Grad CAAN.W_Q.bias: 1.876182432170026e-05\n",
      "Grad CAAN.W_K.weight: 3.6333411117084324e-05\n",
      "Grad CAAN.W_K.bias: 2.1071904221514615e-09\n",
      "Grad CAAN.W_V.weight: 2.391029011050705e-05\n",
      "Grad CAAN.W_V.bias: 1.2968912699307111e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004389510431792587\n",
      "Grad CAAN.scorer.bias: 1.5313980838982388e-06\n",
      "Grad layer_norm.weight: 4.327039562213031e-07\n",
      "Grad layer_norm.bias: 4.905345463157573e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.960813436762692e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.2672984196533434e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.6259575847120686e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.2377649955274137e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.9038344234445503e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.8083937680657414e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.981879797583133e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.1451232729295953e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.4028254819750146e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.9411767122364836e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.047501415196166e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 7.587578920720262e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.6088331733499217e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.3386473085574835e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 5.814496830680582e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.8237977883472922e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.134836745743996e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.9677888591095325e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.2158356810232362e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.541183950048435e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.840975187406002e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.32066986427526e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 4.41030460933689e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.5989120230369736e-06\n",
      "Grad SREM.ff.weight: 2.4030521672102623e-05\n",
      "Grad SREM.ff.bias: 7.772044227749575e-06\n",
      "Grad CAAN.W_Q.weight: 4.547412754618563e-05\n",
      "Grad CAAN.W_Q.bias: 2.0923362171743065e-05\n",
      "Grad CAAN.W_K.weight: 3.9700946217635646e-05\n",
      "Grad CAAN.W_K.bias: 2.1230803781691066e-09\n",
      "Grad CAAN.W_V.weight: 2.64727532339748e-05\n",
      "Grad CAAN.W_V.bias: 1.0713173281828858e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004798008012585342\n",
      "Grad CAAN.scorer.bias: 1.264975253434386e-06\n",
      "Grad layer_norm.weight: 4.347848232555407e-07\n",
      "Grad layer_norm.bias: 5.023896392231109e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.987579581618775e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.3453600656894196e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.7101240362981116e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.1862677002104647e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.981956598839929e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.7806728314084808e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 4.805147835185153e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.684879651937422e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 4.7002732372902756e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 6.034044019997964e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 5.469671577884583e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.232866442383965e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.368168796034297e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.0614506606998475e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.515496812018682e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.5185048596322304e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.8485272807188267e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.5868356345881693e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.131836526724328e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.958087712087945e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.4011700386618031e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.7965695633392897e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.919072332791984e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.175451744638849e-06\n",
      "Grad SREM.ff.weight: 2.181635682063643e-05\n",
      "Grad SREM.ff.bias: 7.283664217538899e-06\n",
      "Grad CAAN.W_Q.weight: 4.115486808586866e-05\n",
      "Grad CAAN.W_Q.bias: 1.9165969206369482e-05\n",
      "Grad CAAN.W_K.weight: 3.5213502997066826e-05\n",
      "Grad CAAN.W_K.bias: 2.051681269321648e-09\n",
      "Grad CAAN.W_V.weight: 2.2901309421285987e-05\n",
      "Grad CAAN.W_V.bias: 1.6419605231021706e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004200030816718936\n",
      "Grad CAAN.scorer.bias: 1.9388889995752834e-06\n",
      "Grad layer_norm.weight: 4.323487416968419e-07\n",
      "Grad layer_norm.bias: 5.216185172685073e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.690480347515404e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 5.683878612217086e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.7408290748098807e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.6492703142366736e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 2.1284909390573148e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.019723499557813e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 5.749877907135215e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.3778218000435345e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 5.484593756932554e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 6.908314986731057e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.979654472161201e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 7.668546686545596e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.6929905655160837e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.3828883993483032e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 5.989674605189066e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.9237687354234367e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.2319753984211275e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.016516103571121e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.2342565014478168e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.566922650108609e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.8735572666628286e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.375535814280738e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 4.478955452213995e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 6.789122380723711e-06\n",
      "Grad SREM.ff.weight: 2.413087713648565e-05\n",
      "Grad SREM.ff.bias: 7.973245374159887e-06\n",
      "Grad CAAN.W_Q.weight: 4.5167071220930666e-05\n",
      "Grad CAAN.W_Q.bias: 2.132849840563722e-05\n",
      "Grad CAAN.W_K.weight: 4.044817978865467e-05\n",
      "Grad CAAN.W_K.bias: 1.4294886385712857e-09\n",
      "Grad CAAN.W_V.weight: 2.633308213262353e-05\n",
      "Grad CAAN.W_V.bias: 2.6233132643938006e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004769698134623468\n",
      "Grad CAAN.scorer.bias: 3.0986766432761215e-06\n",
      "Grad layer_norm.weight: 4.231749812788621e-07\n",
      "Grad layer_norm.bias: 5.497788038155704e-07\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 79: last loss = 54.08009\n",
      "eval step --\n",
      "Step 79: val_rewards = 0.4077109402687631\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 4.271384135279277e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.6556348231565607e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.2603020283517026e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 1.2894028067478303e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 2.2450097336701447e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.3451834918475356e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 6.107949701572579e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.6064123642411943e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 7.676522528754504e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.740043912931924e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 4.7204228081909605e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.500451128957138e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.117466386091337e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 7.192129114486079e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.7772872002174154e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.334157446971858e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 9.125759037331704e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 9.007020129736532e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.8535178947232112e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.0364715752331222e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 8.86211353190447e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.027508460538229e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.365214062498126e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 4.3687553130666856e-08\n",
      "Grad SREM.ff.weight: 2.839420858435915e-07\n",
      "Grad SREM.ff.bias: 4.0709494442126015e-08\n",
      "Grad CAAN.W_Q.weight: 5.252212531559053e-07\n",
      "Grad CAAN.W_Q.bias: 1.2123106785111304e-07\n",
      "Grad CAAN.W_K.weight: 5.255748760646384e-07\n",
      "Grad CAAN.W_K.bias: 2.967309731261025e-11\n",
      "Grad CAAN.W_V.weight: 3.03021465697384e-07\n",
      "Grad CAAN.W_V.bias: 5.334236163889727e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 9.19426565815229e-06\n",
      "Grad CAAN.scorer.bias: 6.309119271463715e-08\n",
      "Grad layer_norm.weight: 2.7062982965730953e-08\n",
      "Grad layer_norm.bias: 2.4786872998561194e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.507092548536093e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.1331182747653656e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 3.4706808094142616e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.056645425658644e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 2.800022880222741e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.5474369280686915e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 8.152151842466537e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.145133264936817e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 7.222237741189019e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 6.233140936728887e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.801064955652691e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 7.347966857196297e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.785969914180896e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.3188549985443387e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 4.392620667204028e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 7.673413904285553e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.9444525989342765e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.8492912090550817e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 4.979640166880017e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.623246153632408e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.392435592606489e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.6181678574866964e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 1.7375732568325475e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.8902052235935116e-06\n",
      "Grad SREM.ff.weight: 8.544298907509074e-06\n",
      "Grad SREM.ff.bias: 2.846988081728341e-06\n",
      "Grad CAAN.W_Q.weight: 1.9860288375639357e-05\n",
      "Grad CAAN.W_Q.bias: 9.324248821940273e-06\n",
      "Grad CAAN.W_K.weight: 1.5655230527045205e-05\n",
      "Grad CAAN.W_K.bias: 8.85517048576645e-10\n",
      "Grad CAAN.W_V.weight: 7.325741989916423e-06\n",
      "Grad CAAN.W_V.bias: 8.776035009816496e-09\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00022503534273710102\n",
      "Grad CAAN.scorer.bias: 1.0250505511066876e-07\n",
      "Grad layer_norm.weight: 1.028502424560429e-06\n",
      "Grad layer_norm.bias: 1.1900347089977004e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.4589663521746843e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.3025312739500805e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 4.636415340542044e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.205204279765894e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.9002858720493805e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.516653634960676e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.011157468866486e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 3.5135661047291933e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.2821202233226359e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 7.927926048978406e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 9.723127050165203e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 8.442394801022601e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.546616713061667e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.7907284188822814e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 5.405202045949409e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 7.936507273598181e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 2.1241488568080058e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 2.0414770318666342e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 5.708399442028167e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 3.14639443388387e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.637607851989742e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.960277359103202e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.104785198753234e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.9770892524538795e-06\n",
      "Grad SREM.ff.weight: 9.767708434083033e-06\n",
      "Grad SREM.ff.bias: 3.032488393728272e-06\n",
      "Grad CAAN.W_Q.weight: 2.2459978936240077e-05\n",
      "Grad CAAN.W_Q.bias: 1.0348799150960986e-05\n",
      "Grad CAAN.W_K.weight: 1.767635330907069e-05\n",
      "Grad CAAN.W_K.bias: 8.585673283434403e-10\n",
      "Grad CAAN.W_V.weight: 9.078431503439788e-06\n",
      "Grad CAAN.W_V.bias: 3.4119884162464587e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00026784310466609895\n",
      "Grad CAAN.scorer.bias: 4.032285232824506e-06\n",
      "Grad layer_norm.weight: 9.710630592962843e-07\n",
      "Grad layer_norm.bias: 1.4521992852678522e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.4835518413747195e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.3774490525975125e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 4.7740709163690553e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.498191958328789e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 3.9674317164895e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.788674263205394e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.0849627862796751e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 6.2032370351516875e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.159475289114198e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.373192710881995e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.3398641840467462e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.484612198510149e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.284253864421771e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.7082771225650504e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.0240351002721582e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.198208903791965e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.3827529222207886e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.3782420416628156e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.62922240937769e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.477579906847495e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.2482162168889772e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.867057785100769e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.278228541603312e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.876863956975285e-06\n",
      "Grad SREM.ff.weight: 1.5062522834341507e-05\n",
      "Grad SREM.ff.bias: 3.814765022980282e-06\n",
      "Grad CAAN.W_Q.weight: 3.190631832694635e-05\n",
      "Grad CAAN.W_Q.bias: 1.2959545529156458e-05\n",
      "Grad CAAN.W_K.weight: 2.7012050850316882e-05\n",
      "Grad CAAN.W_K.bias: 3.110440793818725e-10\n",
      "Grad CAAN.W_V.weight: 1.4486058717011474e-05\n",
      "Grad CAAN.W_V.bias: 6.579026035069546e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004316266276873648\n",
      "Grad CAAN.scorer.bias: 7.773567631375045e-06\n",
      "Grad layer_norm.weight: 8.95586140359228e-07\n",
      "Grad layer_norm.bias: 1.4801462384639308e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.6001665414933086e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.523755344123856e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.02280954606249e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.785746471862694e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.018916310855047e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.899736533696796e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.101469315756276e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 6.4682241784908e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.1866384852510237e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.0067052613749183e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.3736996606894536e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.5433120097441133e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.3330238125017786e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.7739628194467514e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.0657606708264211e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.3322833442307456e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.5182416979750997e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.537850901125239e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.842532312452022e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.643526096970163e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.386923481230042e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 4.0456416172673926e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.40041151503101e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.870676209771773e-06\n",
      "Grad SREM.ff.weight: 1.561680983286351e-05\n",
      "Grad SREM.ff.bias: 3.991082849097438e-06\n",
      "Grad CAAN.W_Q.weight: 3.2634383387630805e-05\n",
      "Grad CAAN.W_Q.bias: 1.3138225767761469e-05\n",
      "Grad CAAN.W_K.weight: 2.794179272314068e-05\n",
      "Grad CAAN.W_K.bias: 1.5773007899788638e-10\n",
      "Grad CAAN.W_V.weight: 1.4993248441896867e-05\n",
      "Grad CAAN.W_V.bias: 6.576752866749302e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004495352040976286\n",
      "Grad CAAN.scorer.bias: 7.770911906845868e-06\n",
      "Grad layer_norm.weight: 1.0385801942902617e-06\n",
      "Grad layer_norm.bias: 1.712284870336589e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.610382440730973e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.541923211334506e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.062056018800831e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.816323257410659e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.036711853672159e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.845017637615911e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.0838924424660945e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 6.2088258978576505e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.2047034658735356e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.977337356303906e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.3297938039613655e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.4647837360826088e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.2155352869267517e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.641963874339126e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.0081037089548772e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.1897062651987653e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.3808042587679665e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.378198698555934e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.633306609022839e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.4803659449144106e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.201837671440444e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.842499609163497e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.323777127661742e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.83538599635358e-06\n",
      "Grad SREM.ff.weight: 1.5391400665976107e-05\n",
      "Grad SREM.ff.bias: 3.953446139348671e-06\n",
      "Grad CAAN.W_Q.weight: 3.193367228959687e-05\n",
      "Grad CAAN.W_Q.bias: 1.2978120139450766e-05\n",
      "Grad CAAN.W_K.weight: 2.7286785552860238e-05\n",
      "Grad CAAN.W_K.bias: 9.822508084678105e-11\n",
      "Grad CAAN.W_V.weight: 1.4531313354382291e-05\n",
      "Grad CAAN.W_V.bias: 6.663270255558018e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.0004362617037259042\n",
      "Grad CAAN.scorer.bias: 7.873088179621845e-06\n",
      "Grad layer_norm.weight: 1.047583282343112e-06\n",
      "Grad layer_norm.bias: 1.7355804402541253e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.6103783195831056e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.5418743259942858e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.06208301942479e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.816260374378544e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.03659106140708e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.84487197635508e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.083769518572808e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 6.208138003671593e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.204604842541812e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.97679379111105e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.3295713188199443e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.4644233488070313e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.2150057904800633e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.6415023057779763e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.0078808827529429e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.1894223323215556e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.3802539434191203e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.377569157692051e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.632584697603306e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.4797946685548595e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.2011289476940874e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.84164468414383e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.32338604493998e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.835070856439415e-06\n",
      "Grad SREM.ff.weight: 1.539007280371152e-05\n",
      "Grad SREM.ff.bias: 3.953216946683824e-06\n",
      "Grad CAAN.W_Q.weight: 3.1931140256347135e-05\n",
      "Grad CAAN.W_Q.bias: 1.2977184269402642e-05\n",
      "Grad CAAN.W_K.weight: 2.7284208044875413e-05\n",
      "Grad CAAN.W_K.bias: 9.821954360944574e-11\n",
      "Grad CAAN.W_V.weight: 1.4529168765875511e-05\n",
      "Grad CAAN.W_V.bias: 6.663688054686645e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00043619860662147403\n",
      "Grad CAAN.scorer.bias: 7.873582035244908e-06\n",
      "Grad layer_norm.weight: 1.0475647513885633e-06\n",
      "Grad layer_norm.bias: 1.7355509953631554e-06\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.639341746795253e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.552768651436054e-07\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 5.085771448420928e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.952107263671678e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 4.062473912824771e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 3.748306998119233e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.060255616636141e-08\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 5.773382216744949e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 1.2991053210953396e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 9.988577431840895e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.275187287319568e-06\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.3585282658823417e-06\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.1205490813590586e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.511070249511249e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.367563507112209e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.0368926811897836e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 3.185890662393831e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 3.161997597089794e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 7.298973514480167e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 4.220746063765546e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.976416908495594e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.578442829166306e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.1252402550308034e-05\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 3.6123876725469017e-06\n",
      "Grad SREM.ff.weight: 1.4694075616716873e-05\n",
      "Grad SREM.ff.bias: 3.7467170841409825e-06\n",
      "Grad CAAN.W_Q.weight: 3.0334775146911852e-05\n",
      "Grad CAAN.W_Q.bias: 1.2295689884922467e-05\n",
      "Grad CAAN.W_K.weight: 2.5954657758120447e-05\n",
      "Grad CAAN.W_K.bias: 1.0925242655002165e-10\n",
      "Grad CAAN.W_V.weight: 1.3590522030426655e-05\n",
      "Grad CAAN.W_V.bias: 6.611461458305712e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00040689672459848225\n",
      "Grad CAAN.scorer.bias: 7.811871000740211e-06\n",
      "Grad layer_norm.weight: 1.0438321851324872e-06\n",
      "Grad layer_norm.bias: 1.7490548316345667e-06\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 87: last loss = -41.19295\n",
      "eval step --\n",
      "Step 87: val_rewards = 0.7452304552614307\n",
      "*** found better model ***\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.3642839746808022e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 6.725943357821507e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.2430720452982769e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.874513960193781e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 6.19214401975654e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 5.612232345519885e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 1.1632620422474815e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 5.684721582355223e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.183773339936579e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.0625393898067159e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.3478421578838606e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.5013036147593084e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 2.980997493295945e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.3183101660606553e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 9.85603207936947e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 5.1234035680636225e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 5.02228703069818e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 5.047918971712306e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.0090463575807007e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 5.547855952414693e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.1225101793097565e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 3.7427818710966676e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 2.7508185667102225e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 7.005760380707216e-07\n",
      "Grad SREM.ff.weight: 1.5670723314542556e-06\n",
      "Grad SREM.ff.bias: 5.160612204235804e-07\n",
      "Grad CAAN.W_Q.weight: 3.6156156966171693e-06\n",
      "Grad CAAN.W_Q.bias: 9.510789027444844e-07\n",
      "Grad CAAN.W_K.weight: 3.489880555207492e-06\n",
      "Grad CAAN.W_K.bias: 1.2744721944457638e-10\n",
      "Grad CAAN.W_V.weight: 1.1963302313233726e-06\n",
      "Grad CAAN.W_V.bias: 3.291231109869841e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 3.2482323149451986e-05\n",
      "Grad CAAN.scorer.bias: 3.888071660185233e-06\n",
      "Grad layer_norm.weight: 8.240543536430778e-08\n",
      "Grad layer_norm.bias: 5.737242148029509e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.5915292195245456e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.947905444893877e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.2418681194503733e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 4.038151590890493e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 9.129573763644316e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 7.718974903703213e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.1536707972558133e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.035053376341466e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.4531210840450512e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.6261868296396642e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.768430533706123e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.2707313835089735e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 6.152126275082992e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.028365279713398e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.198169681832951e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 8.959158748211848e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0418020224278735e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.0404368921967944e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.151450573251168e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.2476308874909137e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 6.930324616405414e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 8.156513899848505e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.478851901192684e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.8232879028801108e-06\n",
      "Grad SREM.ff.weight: 3.654894271676312e-06\n",
      "Grad SREM.ff.bias: 1.1516265203681542e-06\n",
      "Grad CAAN.W_Q.weight: 8.418380275543313e-06\n",
      "Grad CAAN.W_Q.bias: 2.1464827568706824e-06\n",
      "Grad CAAN.W_K.weight: 8.163398888427764e-06\n",
      "Grad CAAN.W_K.bias: 3.880300247782742e-10\n",
      "Grad CAAN.W_V.weight: 3.072600065934239e-06\n",
      "Grad CAAN.W_V.bias: 5.132947080710437e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 8.719422476133332e-05\n",
      "Grad CAAN.scorer.bias: 6.063608452677727e-06\n",
      "Grad layer_norm.weight: 8.753232094704799e-08\n",
      "Grad layer_norm.bias: 3.7318994117185866e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.624691492452257e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.88374254767632e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.2577648256240082e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 4.039708567660227e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 9.209793483400119e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 7.777924415641735e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.2099604368719383e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.0734896305208963e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.4686377386728964e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.7079862857372063e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.8702635290756007e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.446885727953486e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 6.391645968051307e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.278896253457788e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.314945959369652e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.287511204547627e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0773448799739072e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.0768410163564113e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.193858250620906e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.2773137214594499e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 7.341387231463159e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 8.633049333184317e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.61325839246274e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.8932450984721072e-06\n",
      "Grad SREM.ff.weight: 3.7422601053549442e-06\n",
      "Grad SREM.ff.bias: 1.1867795137732173e-06\n",
      "Grad CAAN.W_Q.weight: 8.608167263446376e-06\n",
      "Grad CAAN.W_Q.bias: 2.1787134301121114e-06\n",
      "Grad CAAN.W_K.weight: 8.350724783667829e-06\n",
      "Grad CAAN.W_K.bias: 3.3523034348448277e-10\n",
      "Grad CAAN.W_V.weight: 3.15602051159658e-06\n",
      "Grad CAAN.W_V.bias: 5.277314016893797e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 8.907826122594997e-05\n",
      "Grad CAAN.scorer.bias: 6.234163265617099e-06\n",
      "Grad layer_norm.weight: 8.540239093690616e-08\n",
      "Grad layer_norm.bias: 4.302686917867504e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.6209012798640288e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 7.917590139072672e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.267062543774955e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 4.046774026988942e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 9.218724117410204e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 7.767105847378275e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.2331865245917015e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.0889509294287336e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.4557051503393268e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.697694962388141e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.9114855237821757e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 3.5138853604621545e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 6.483347192443034e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 5.373614087034184e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.3580456343097467e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.396416800200313e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.0889857016138649e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.0891803015056212e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.212923533306821e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.2892241940676286e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 7.477866006411205e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 8.791984100753325e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 6.695451247651363e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.9063752461079275e-06\n",
      "Grad SREM.ff.weight: 3.776346602535341e-06\n",
      "Grad SREM.ff.bias: 1.1945141977776075e-06\n",
      "Grad CAAN.W_Q.weight: 8.722163329366595e-06\n",
      "Grad CAAN.W_Q.bias: 2.203549684054451e-06\n",
      "Grad CAAN.W_K.weight: 8.445362254860811e-06\n",
      "Grad CAAN.W_K.bias: 3.3979843938602983e-10\n",
      "Grad CAAN.W_V.weight: 3.206681412848411e-06\n",
      "Grad CAAN.W_V.bias: 5.252599635241495e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 9.051401139004156e-05\n",
      "Grad CAAN.scorer.bias: 6.204981673363363e-06\n",
      "Grad layer_norm.weight: 8.529003991952777e-08\n",
      "Grad layer_norm.bias: 4.287081623033373e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.651742032038328e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 8.924534000698259e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.5329291613852547e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.4788028013821304e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.0721756815712524e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 9.704332892823686e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.618373740048696e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.3359994222028604e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.2511977110516455e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 2.0541653711347863e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.439539284499915e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.3369584545871476e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 7.651029676480903e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 6.583977807395058e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.871671824777877e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.1116782872022668e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.2772574997654829e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.276255900961587e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.5534875547350566e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.490819556693168e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.125826068157039e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.0716470342231332e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 7.997092325240374e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.3110869733500294e-06\n",
      "Grad SREM.ff.weight: 4.454563622857677e-06\n",
      "Grad SREM.ff.bias: 1.384975689688872e-06\n",
      "Grad CAAN.W_Q.weight: 1.0431045666337013e-05\n",
      "Grad CAAN.W_Q.bias: 2.372029484831728e-06\n",
      "Grad CAAN.W_K.weight: 1.0125282642547973e-05\n",
      "Grad CAAN.W_K.bias: 1.4599518816105217e-10\n",
      "Grad CAAN.W_V.weight: 3.903759534296114e-06\n",
      "Grad CAAN.W_V.bias: 4.771736143993621e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00011175333202118054\n",
      "Grad CAAN.scorer.bias: 5.636969490296906e-06\n",
      "Grad layer_norm.weight: 9.18021285656323e-08\n",
      "Grad layer_norm.bias: 3.2008763639623794e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.6525664392474937e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 9.306225123850709e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.555260276120407e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.815558535791297e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.1606370309280578e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0171706810169212e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.785472075217399e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.3578521640411623e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.244433699478577e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.9463275435782634e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.6119152468927496e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.5016963667876553e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 8.408274254634307e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 7.061078832748535e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.9466733053595817e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.0658093430038207e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.3246988395110293e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.324773890587494e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.503144003185298e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.446487107870098e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.378725849273906e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.1132639201605343e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 7.27585711501888e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.0578088449619827e-06\n",
      "Grad SREM.ff.weight: 3.843359991151374e-06\n",
      "Grad SREM.ff.bias: 9.188787544189836e-07\n",
      "Grad CAAN.W_Q.weight: 9.679341019364074e-06\n",
      "Grad CAAN.W_Q.bias: 2.170456582462066e-06\n",
      "Grad CAAN.W_K.weight: 9.302497346652672e-06\n",
      "Grad CAAN.W_K.bias: 4.2259898358487646e-10\n",
      "Grad CAAN.W_V.weight: 3.406365749469842e-06\n",
      "Grad CAAN.W_V.bias: 5.920595640418469e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 9.830384806264192e-05\n",
      "Grad CAAN.scorer.bias: 6.994255727477139e-06\n",
      "Grad layer_norm.weight: 8.997049150138992e-08\n",
      "Grad layer_norm.bias: 3.482470134485993e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.4119629788165184e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.073258364987396e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.6089121146478647e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 5.5541429233585404e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.2092273848907098e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 9.948528667536038e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.8947670926982028e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.3654943842311695e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.3583148706384236e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.4917775459366567e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.836842097371118e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 4.784260454471223e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 8.760223835224679e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 7.39621910383903e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.1470477779294015e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.110633363055058e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.4080301369290282e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.412192229821585e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.644272001361969e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.5466227409888234e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.99364829112892e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.187072030006675e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 7.707981239946093e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.2044040431646863e-06\n",
      "Grad SREM.ff.weight: 4.087512934347615e-06\n",
      "Grad SREM.ff.bias: 1.0399553502793424e-06\n",
      "Grad CAAN.W_Q.weight: 1.0205244507233147e-05\n",
      "Grad CAAN.W_Q.bias: 2.225757953056018e-06\n",
      "Grad CAAN.W_K.weight: 9.903578757075593e-06\n",
      "Grad CAAN.W_K.bias: 4.750896342997635e-10\n",
      "Grad CAAN.W_V.weight: 3.5631414903036784e-06\n",
      "Grad CAAN.W_V.bias: 7.123509817574813e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00010238268441753462\n",
      "Grad CAAN.scorer.bias: 8.415385309490375e-06\n",
      "Grad layer_norm.weight: 2.3814050109649543e-07\n",
      "Grad layer_norm.bias: 1.4783337576318445e-07\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 4.2861003635152883e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 2.8345770175519647e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 1.6930725266206537e-08\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 9.024718750083593e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.3573117074727747e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.0580334386389723e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.4291527395424737e-09\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.6304441130543523e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.3565378032562876e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.7853164280268174e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 4.6423843969023437e-07\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.955078563602001e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.0301771879994703e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 9.075572648953312e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.8752804698560794e-07\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.3438427970413613e-07\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.6673622482699102e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.6709721606389394e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 3.1029639302460055e-08\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 1.8440442062228612e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 1.2502906656663981e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 1.473477254876343e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 9.394057087774854e-06\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.7127105138570187e-06\n",
      "Grad SREM.ff.weight: 4.842647740588291e-06\n",
      "Grad SREM.ff.bias: 1.2651694305532146e-06\n",
      "Grad CAAN.W_Q.weight: 1.209397851198446e-05\n",
      "Grad CAAN.W_Q.bias: 2.5784820536500774e-06\n",
      "Grad CAAN.W_K.weight: 1.1755865671148058e-05\n",
      "Grad CAAN.W_K.bias: 1.4692481953293424e-10\n",
      "Grad CAAN.W_V.weight: 4.511992756306427e-06\n",
      "Grad CAAN.W_V.bias: 6.471989308920456e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 0.00013318618584889919\n",
      "Grad CAAN.scorer.bias: 7.64583091950044e-06\n",
      "Grad layer_norm.weight: 3.0688553920299455e-07\n",
      "Grad layer_norm.bias: 2.3615783106833987e-07\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 95: last loss = 162.46994\n",
      "eval step --\n",
      "Step 95: val_rewards = 0.7160572483683484\n",
      "\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 6.257384888019857e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.51857050440163e-11\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.0546087053929796e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 3.511022722557655e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 2.1796481286529e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 2.325901624078419e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 6.917258432714846e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.1441341757633126e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 6.546667097317993e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 3.496582745565746e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 6.517175688003363e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 5.1438138193304894e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 1.4912130419375558e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.023730211713314e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 3.394975189863203e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 9.035223680342597e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 1.8091138309639376e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 1.624055473437025e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 5.575288009040946e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 2.789259934132815e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 9.588860372389263e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 9.291044378301194e-09\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 8.711418075790789e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 7.186579864537634e-08\n",
      "Grad SREM.ff.weight: 1.0127446614660585e-07\n",
      "Grad SREM.ff.bias: 3.929057967866356e-08\n",
      "Grad CAAN.W_Q.weight: 1.399406244217971e-07\n",
      "Grad CAAN.W_Q.bias: 2.2088732620773044e-08\n",
      "Grad CAAN.W_K.weight: 1.5130525810036488e-07\n",
      "Grad CAAN.W_K.bias: 8.832550142667817e-12\n",
      "Grad CAAN.W_V.weight: 6.476315661529952e-08\n",
      "Grad CAAN.W_V.bias: 3.925525859926893e-08\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.3094975201966008e-06\n",
      "Grad CAAN.scorer.bias: 4.637549864128232e-07\n",
      "Grad layer_norm.weight: 2.797631848405757e-10\n",
      "Grad layer_norm.bias: 2.915337138365004e-10\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 1.809219440929155e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 1.369667823603038e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 8.739343693164869e-10\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 2.3415838712459447e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 7.084181158356628e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 7.590929274048364e-11\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.2136237287639915e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 6.532258761682286e-11\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 2.270368559820213e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 1.681724981494881e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 1.9654818217418324e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 9.128894085108641e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.502042433822794e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 1.997960019650691e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 8.42581826532296e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 1.950620109880674e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 5.00746444309641e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 4.2849199188843556e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.498888124729092e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 6.501866822716806e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 2.5965555039419996e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.4774937656957263e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 3.3792264275689377e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.3360048001231917e-07\n",
      "Grad SREM.ff.weight: 2.398107881163014e-07\n",
      "Grad SREM.ff.bias: 7.994785988785225e-08\n",
      "Grad CAAN.W_Q.weight: 4.572640079913981e-07\n",
      "Grad CAAN.W_Q.bias: 6.034808563981642e-08\n",
      "Grad CAAN.W_K.weight: 3.9065852774911036e-07\n",
      "Grad CAAN.W_K.bias: 1.1600803651035108e-10\n",
      "Grad CAAN.W_V.weight: 2.862377073142852e-07\n",
      "Grad CAAN.W_V.bias: 1.2302074026138143e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 8.898861779016443e-06\n",
      "Grad CAAN.scorer.bias: 1.453197910450399e-06\n",
      "Grad layer_norm.weight: 9.995991590017184e-09\n",
      "Grad layer_norm.bias: 1.7165550758591053e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.89208221207582e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.585048258258894e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.395049714110087e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 9.475495943433998e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.0621991480830317e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.132731686226407e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 2.149530414774503e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 1.1800893595648176e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.5125908848243625e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 4.432983313762406e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 2.092234474559973e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 1.1608624284065172e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 3.807469450833878e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 2.261355103172491e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 1.053416820440134e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 2.4573980628161962e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 6.316175915621614e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 5.335225039537761e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 1.9107968540765796e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 8.752240043818915e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 3.101522239035148e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 2.796321041387273e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 5.032380272496084e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 1.460278440390539e-07\n",
      "Grad SREM.ff.weight: 3.233218137665972e-07\n",
      "Grad SREM.ff.bias: 8.89637448153735e-08\n",
      "Grad CAAN.W_Q.weight: 4.610210453392938e-07\n",
      "Grad CAAN.W_Q.bias: 9.437957970703792e-08\n",
      "Grad CAAN.W_K.weight: 4.4461825154940016e-07\n",
      "Grad CAAN.W_K.bias: 1.4280562843360656e-11\n",
      "Grad CAAN.W_V.weight: 3.7286829979166214e-07\n",
      "Grad CAAN.W_V.bias: 1.339534065891712e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 1.19873993753572e-05\n",
      "Grad CAAN.scorer.bias: 1.5824007277842611e-06\n",
      "Grad layer_norm.weight: 2.0254709909295343e-08\n",
      "Grad layer_norm.bias: 2.296094514520064e-08\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.in_proj_bias\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.weight\n",
      "No grad for SREM.transformer_encoder_layer.self_attn.out_proj.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear1.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear1.bias\n",
      "No grad for SREM.transformer_encoder_layer.linear2.weight\n",
      "No grad for SREM.transformer_encoder_layer.linear2.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm1.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm1.bias\n",
      "No grad for SREM.transformer_encoder_layer.norm2.weight\n",
      "No grad for SREM.transformer_encoder_layer.norm2.bias\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_weight: 3.7089642468401962e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.in_proj_bias: 3.579503138340101e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.weight: 2.842436508387891e-09\n",
      "Grad SREM.transformer_encoder.layers.0.self_attn.out_proj.bias: 8.023065545259556e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.weight: 1.6069442998478678e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear1.bias: 1.770001922807296e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.weight: 3.084411059894876e-10\n",
      "Grad SREM.transformer_encoder.layers.0.linear2.bias: 2.008856414548177e-10\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.weight: 3.6505078959692128e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm1.bias: 3.860348929407564e-09\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.weight: 3.0346551938009725e-08\n",
      "Grad SREM.transformer_encoder.layers.0.norm2.bias: 2.1102080083323926e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_weight: 6.51742171342562e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.in_proj_bias: 4.581264700931342e-09\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.weight: 2.0608291961821124e-08\n",
      "Grad SREM.transformer_encoder.layers.1.self_attn.out_proj.bias: 4.581321100260993e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.weight: 8.309447574461615e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear1.bias: 7.22787385498691e-10\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.weight: 2.3267339166466172e-09\n",
      "Grad SREM.transformer_encoder.layers.1.linear2.bias: 9.892812125045225e-10\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.weight: 6.324248857936254e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm1.bias: 6.815070463517259e-08\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.weight: 8.413663294959406e-07\n",
      "Grad SREM.transformer_encoder.layers.1.norm2.bias: 2.6716330125964305e-07\n",
      "Grad SREM.ff.weight: 4.660650461119076e-07\n",
      "Grad SREM.ff.bias: 1.4946421345030103e-07\n",
      "Grad CAAN.W_Q.weight: 6.819828399784456e-07\n",
      "Grad CAAN.W_Q.bias: 1.6082898923741595e-07\n",
      "Grad CAAN.W_K.weight: 6.222765023267129e-07\n",
      "Grad CAAN.W_K.bias: 7.293788489848296e-11\n",
      "Grad CAAN.W_V.weight: 6.798569529564702e-07\n",
      "Grad CAAN.W_V.bias: 2.921538566624804e-07\n",
      "No grad for CAAN.W_O.weight\n",
      "No grad for CAAN.W_O.bias\n",
      "No grad for CAAN.normalizer.weight\n",
      "No grad for CAAN.normalizer.bias\n",
      "Grad CAAN.scorer.weight: 2.20179463212844e-05\n",
      "Grad CAAN.scorer.bias: 3.4514050639700145e-06\n",
      "Grad layer_norm.weight: 1.8972178494891523e-08\n",
      "Grad layer_norm.bias: 3.0825216157381874e-08\n"
     ]
    }
   ],
   "source": [
    "max_reward = -1\n",
    "\n",
    "for training_step in range(training_steps):\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    tran_costs = []\n",
    "    nlls = []\n",
    "\n",
    "    market_env.reset(mode = \"train\", transaction_cost= 1e-7)\n",
    "    state = market_env.get_state()\n",
    "\n",
    "    while not is_end:\n",
    "        symbol_idx, allocations = portfolio_constructor(state)\n",
    "        state, reward, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "        mask_tensor = torch.tensor([1 if i in symbol_idx.cpu().numpy() else 0 for i in range(allocations.shape[0])]).type(torch.FloatTensor).cuda()\n",
    "        # nlls.append(torch.log(allocations.abs() / 2 + 1e-9) * mask_tensor)\n",
    "        nlls.append((torch.log(allocations.abs() + 1e-9) * mask_tensor))\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "    loss = -sharp_ratio * sum([e.sum() for e in nlls])\n",
    "    # loss = - sum([e.sum() for e in nlls])\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    for name, param in portfolio_constructor.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Grad {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No grad for {name}\")\n",
    "\n",
    "    if (training_step + 1) % train_step == 0:\n",
    "\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\"training model --\")\n",
    "        print('Step {}: last loss = {:.5f}\\r'.format(training_step, loss), end='')\n",
    "        print()\n",
    "        writer.add_scalar(\"Loss/train\", sharp_ratio, training_step)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        count = 0\n",
    "        \n",
    "    if (training_step + 1) % eval_step == 0:\n",
    "        print(\"eval step --\")\n",
    "        with torch.no_grad():\n",
    "            reward_val = evaluate(portfolio_constructor, market_env)\n",
    "            print('Step {}: val_rewards = {}'.format(training_step, reward_val))\n",
    "            writer.add_scalar(\"eval_sharpe/train\", reward_val, training_step)\n",
    "\n",
    "            if max_reward < reward_val:\n",
    "                max_reward = reward_val\n",
    "\n",
    "                print(\"*** found better model ***\")\n",
    "            print()\n",
    "                # torch.save(portfolio_constructor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = market_env.get_random_state()\n",
    "# random_state.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  4,  7, 11, 12, 13, 16, 17, 19], device='cuda:0'),\n",
       " tensor([0.1000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
       "         0.0000, 0.1000], device='cuda:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_constructor.eval()\n",
    "portfolio_constructor(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/dev/Charles_Schwab/code/RnD/enviroment/PortfolioConstructor.py:142: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  long_mask = torch.Tensor([0 if i in long_sqs else 1 for i in range(rank.shape[0])]).to(self.device)\n",
      "/home/naradaw/dev/Charles_Schwab/code/RnD/enviroment/PortfolioConstructor.py:170: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  portfolio_allocations = [allocation.item() for allocation in allocations if allocation != 0]\n"
     ]
    }
   ],
   "source": [
    "writer.add_graph(portfolio_constructor, random_state.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.3242, device='cuda:0'), tensor(-4.8283, device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor([0.4, 0.3, 0.3])).sum(), torch.log(torch.tensor([0.8, 0.1, 0.1])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9163, -1.2040, -1.2040], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor([0.4, 0.3, 0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-23.0259, device='cuda:0', grad_fn=<SumBackward0>)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.sum() for e in nlls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0000, -2.3028, -2.3031, -2.3027, -2.3026, -0.0000, -2.3020, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -2.3025, -0.0000, -2.3024, -2.3022, -0.0000,\n",
       "         -0.0000, -2.3027, -0.0000, -2.3029], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-0.0000, -2.3026, -2.3027, -2.3028, -0.0000, -2.3027, -0.0000, -2.3029,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000, -2.3023, -0.0000, -2.3028, -0.0000,\n",
       "         -2.3029, -0.0000, -2.3027, -2.3016], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-2.3018, -0.0000, -0.0000, -0.0000, -2.3030, -0.0000, -2.3031, -0.0000,\n",
       "         -2.3023, -2.3025, -2.3025, -0.0000, -0.0000, -2.3024, -2.3032, -0.0000,\n",
       "         -2.3026, -2.3025, -0.0000, -0.0000], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-2.3028, -0.0000, -0.0000, -2.3028, -0.0000, -2.3028, -2.3021, -2.3032,\n",
       "         -2.3020, -0.0000, -0.0000, -2.3030, -0.0000, -0.0000, -2.3019, -2.3031,\n",
       "         -0.0000, -0.0000, -2.3023, -0.0000], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-2.3024, -0.0000, -2.3025, -2.3028, -2.3026, -2.3029, -0.0000, -2.3022,\n",
       "         -2.3025, -2.3027, -0.0000, -2.3025, -0.0000, -2.3028, -0.0000, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-2.3028, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -2.3027,\n",
       "         -0.0000, -2.3026, -2.3022, -2.3028, -0.0000, -0.0000, -2.3024, -2.3027,\n",
       "         -0.0000, -2.3026, -2.3026, -2.3026], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-0.0000, -0.0000, -0.0000, -2.3021, -2.3028, -0.0000, -2.3022, -2.3029,\n",
       "         -2.3028, -2.3019, -0.0000, -2.3025, -0.0000, -0.0000, -2.3031, -0.0000,\n",
       "         -0.0000, -0.0000, -2.3028, -2.3027], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-0.0000, -0.0000, -2.3031, -2.3027, -0.0000, -0.0000, -0.0000, -2.3026,\n",
       "         -2.3021, -2.3025, -0.0000, -2.3023, -2.3020, -0.0000, -2.3027, -0.0000,\n",
       "         -2.3032, -0.0000, -2.3027, -0.0000], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-2.3027, -0.0000, -0.0000, -2.3029, -0.0000, -2.3016, -2.3027, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000, -2.3024, -2.3026, -2.3031, -2.3030,\n",
       "         -2.3030, -2.3020, -0.0000, -0.0000], device='cuda:0',\n",
       "        grad_fn=<MulBackward0>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlls)\n",
    "nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(7.420267669782051),\n",
       " np.float64(-5.709579933807262),\n",
       " np.float64(0.826904764696958),\n",
       " np.float64(8.979349654316891),\n",
       " np.float64(0.26943840406834996),\n",
       " np.float64(1.1629813939333005),\n",
       " np.float64(4.28277961239219),\n",
       " np.float64(-2.4243068919330772),\n",
       " np.float64(0.3083210603892752)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
