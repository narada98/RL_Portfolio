{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PortfolioConstructor import PortfolioConstructor\n",
    "from ExchnageEnv import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda') \n",
    "    torch.get_default_device()\n",
    "    device = 'cuda'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_loc = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v3/2024_11_21/2024_11_21_09_36\"\n",
    "data_path = f\"{data_base_loc}/dataset_sqs.pkl\"\n",
    "feature_set_path = f\"{data_base_loc}/feature_set.pkl\"\n",
    "symbol_universe_path = f\"{data_base_loc}/symbol_universe.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AJG', 'MA', 'NVDA', 'NRG', 'NOC', 'NCLH', 'MU', 'MRO', 'MMM', 'MLM',\n",
       "       'MDLZ', 'MCK', 'LYB', 'ALB', 'LRCX', 'KR', 'KO', 'KMX', 'KMB', 'KHC',\n",
       "       'JPM', 'JNJ', 'JKHY', 'JCI', 'NWS', 'OMC', 'ORLY', 'PNC', 'WEC', 'VZ',\n",
       "       'VTR', 'VRSN', 'UAL', 'TTWO', 'TSN', 'TAP', 'SWKS', 'SHW', 'SBUX',\n",
       "       'RSG', 'ROST', 'ROP', 'RMD', 'RF', 'REG', 'REGN', 'QRVO', 'PYPL', 'PNR',\n",
       "       'IRM', 'IPG', 'INTC', 'CSCO', 'CMI', 'CINF', 'CHTR', 'CHD', 'CF', 'CE',\n",
       "       'CAH', 'CAG', 'BIIB', 'BEN', 'BDX', 'BBY', 'A', 'AXP', 'AWK', 'APH',\n",
       "       'APD', 'APA', 'ANET', 'ALL', 'ALLE', 'CPB', 'CTAS', 'IFF', 'DAL', 'HUM',\n",
       "       'HRL', 'HCA', 'GWW', 'GL', 'GLW', 'GE', 'FRT', 'FMC', 'EXPE', 'WM',\n",
       "       'ETN', 'ES', 'EMR', 'EL', 'EIX', 'ED', 'EBAY', 'DHI', 'DG', 'DFS'],\n",
       "      dtype='object', name='symbol')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(symbol_universe_path, \"rb\") as fp:\n",
    "    symbol_universe = pickle.load(fp)\n",
    "    \n",
    "symbol_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bb_bbm', 'bb_bbh', 'bb_bbl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(feature_set_path, 'rb') as f:\n",
    "    feature_set = pickle.load(f)\n",
    "\n",
    "feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/naradaw/dev/Charles_Schwab/code/RnD/v5/mlflow_experiments/622240344876609488', creation_time=1732163408797, experiment_id='622240344876609488', last_update_time=1732163408797, lifecycle_stage='active', name='/portfolio-contructor-v5', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_tracking_uri = 'file:/home/naradaw/dev/Charles_Schwab/code/RnD/v5/mlflow_experiments'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "experiment_name = \"/portfolio-contructor-v5\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "eval_step = 1\n",
    "train_step = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "symbol_universe= symbol_universe\n",
    "num_features= len(feature_set)\n",
    "d_model = 88\n",
    "nheads = 4\n",
    "num_transformer_layers = 5\n",
    "\n",
    "episode_duration= 12   \n",
    "holding_period = 1\n",
    "train_test_split= 0.7\n",
    "symbol_universe = symbol_universe\n",
    "feature_set= feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_universe = random.choices(symbol_universe, k = 20)\n",
    "# symbol_universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "sharpe ratio measures the excess return of the portfolio over the \n",
    "volatility of it -> risk adjusted performance\n",
    "'''\n",
    "\n",
    "def sharp_ratio_(rewards, tran_costs):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_ratio(rewards, baseline_rewards):\n",
    "    excess_return = np.array([reward - baseline_reward for reward, baseline_reward in zip(rewards, baseline_rewards)])\n",
    "    \n",
    "    information_ratio = excess_return.mean()/excess_return.std()\n",
    "\n",
    "    return information_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env):\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    baseline_returns = []\n",
    "    tran_costs = []\n",
    "    \n",
    "    env.reset(mode = \"val\")\n",
    "    state = env.get_state()\n",
    "\n",
    "    print(\"\")\n",
    "    while not is_end:\n",
    "        _, allocations = model(state)\n",
    "        state, reward, baseline_return, is_end, tran_cost = env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "        baseline_returns.append(baseline_return)\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "    baseline_sharp_ratio = sharp_ratio_(baseline_returns, tran_costs)\n",
    "    return sharp_ratio, baseline_sharp_ratio, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "training model --\n",
      "Step 0: last loss = 0.03332\n",
      "eval step --\n",
      "\n",
      "Step 0: val_rewards = -0.0371325730378639 | baseline_reward = -0.207119952210963\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 1: last loss = 0.77915\n",
      "eval step --\n",
      "\n",
      "Step 1: val_rewards = 0.07035889486264765 | baseline_reward = 0.8223658920534244\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 2: last loss = 0.55694\n",
      "eval step --\n",
      "\n",
      "Step 2: val_rewards = -0.036806689635888123 | baseline_reward = -0.15164827140945486\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 3: last loss = 0.37144\n",
      "eval step --\n",
      "\n",
      "Step 3: val_rewards = 0.3550143869392668 | baseline_reward = 0.48672117965628475\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 4: last loss = 0.87468\n",
      "eval step --\n",
      "\n",
      "Step 4: val_rewards = -0.010468421044833252 | baseline_reward = -0.10044885213795635\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 5: last loss = 0.16328\n",
      "eval step --\n",
      "\n",
      "Step 5: val_rewards = 0.5889650257390899 | baseline_reward = 0.7881221759723845\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 6: last loss = 0.83065\n",
      "eval step --\n",
      "\n",
      "Step 6: val_rewards = 0.046513552558480095 | baseline_reward = 0.7659950764657154\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 7: last loss = 0.27066\n",
      "eval step --\n",
      "\n",
      "Step 7: val_rewards = 0.5228032510108188 | baseline_reward = 0.5464799477140734\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 8: last loss = 0.12803\n",
      "eval step --\n",
      "\n",
      "Step 8: val_rewards = 0.42585954284766137 | baseline_reward = 0.5535139907221109\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 9: last loss = 0.06390\n",
      "eval step --\n",
      "\n",
      "Step 9: val_rewards = -0.06776847601127027 | baseline_reward = -0.12004309054607247\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 10: last loss = 0.60067\n",
      "eval step --\n",
      "\n",
      "Step 10: val_rewards = 0.029765951695552587 | baseline_reward = -0.031455106902401365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 11: last loss = 0.46418\n",
      "eval step --\n",
      "\n",
      "Step 11: val_rewards = -0.09262062218638828 | baseline_reward = -0.10561581099855756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 12: last loss = 0.62117\n",
      "eval step --\n",
      "\n",
      "Step 12: val_rewards = -0.01456272448061464 | baseline_reward = 0.4312131923869825\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 13: last loss = -0.00900\n",
      "eval step --\n",
      "\n",
      "Step 13: val_rewards = 0.4863311853268773 | baseline_reward = 1.146428138865238\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 14: last loss = 0.01754\n",
      "eval step --\n",
      "\n",
      "Step 14: val_rewards = 0.03000191417954653 | baseline_reward = 0.17614576473102567\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 15: last loss = 1.01119\n",
      "eval step --\n",
      "\n",
      "Step 15: val_rewards = 0.4453117537974009 | baseline_reward = 1.1910070511535245\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 16: last loss = 0.76170\n",
      "eval step --\n",
      "\n",
      "Step 16: val_rewards = 0.4080222564499885 | baseline_reward = 0.7181084330239024\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 17: last loss = 0.13911\n",
      "eval step --\n",
      "\n",
      "Step 17: val_rewards = 0.14734260133963636 | baseline_reward = 0.36685312996470437\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 18: last loss = 0.55424\n",
      "eval step --\n",
      "\n",
      "Step 18: val_rewards = 0.15952641934593115 | baseline_reward = 0.6818715039957512\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 19: last loss = 0.22705\n",
      "eval step --\n",
      "\n",
      "Step 19: val_rewards = 0.050365498656057626 | baseline_reward = 0.6867815855147756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 20: last loss = 0.74862\n",
      "eval step --\n",
      "\n",
      "Step 20: val_rewards = -0.01021654132242527 | baseline_reward = 0.5517491248297536\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 21: last loss = 0.03297\n",
      "eval step --\n",
      "\n",
      "Step 21: val_rewards = 0.16486430001686317 | baseline_reward = 0.3599848102006836\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 22: last loss = 1.10571\n",
      "eval step --\n",
      "\n",
      "Step 22: val_rewards = 0.2955352681679915 | baseline_reward = 0.08159079228309284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 23: last loss = 0.18318\n",
      "eval step --\n",
      "\n",
      "Step 23: val_rewards = 0.6416350582691086 | baseline_reward = 0.7787926443553859\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 24: last loss = 0.95384\n",
      "eval step --\n",
      "\n",
      "Step 24: val_rewards = -0.020664524731255574 | baseline_reward = -0.016511857148819385\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 25: last loss = 0.34208\n",
      "eval step --\n",
      "\n",
      "Step 25: val_rewards = -0.07437606170990482 | baseline_reward = 0.4595972833246864\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 26: last loss = 0.73398\n",
      "eval step --\n",
      "\n",
      "Step 26: val_rewards = 0.046837302917509135 | baseline_reward = -0.02399293385146675\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 27: last loss = 0.18451\n",
      "eval step --\n",
      "\n",
      "Step 27: val_rewards = 0.2984460438111357 | baseline_reward = 0.1888250569240263\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 28: last loss = 0.11962\n",
      "eval step --\n",
      "\n",
      "Step 28: val_rewards = 0.4324578483148306 | baseline_reward = 0.5178446205859352\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 29: last loss = 0.27844\n",
      "eval step --\n",
      "\n",
      "Step 29: val_rewards = 0.335107951297251 | baseline_reward = 0.6742653842942563\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 30: last loss = -0.09662\n",
      "eval step --\n",
      "\n",
      "Step 30: val_rewards = 0.2068765333758588 | baseline_reward = 0.5933428545608107\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 31: last loss = 0.93846\n",
      "eval step --\n",
      "\n",
      "Step 31: val_rewards = 0.17826438950281584 | baseline_reward = -0.06875045936192646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 32: last loss = 0.23635\n",
      "eval step --\n",
      "\n",
      "Step 32: val_rewards = 0.2404855564598282 | baseline_reward = 0.15907879776329997\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 33: last loss = 0.68269\n",
      "eval step --\n",
      "\n",
      "Step 33: val_rewards = 0.15446097743728385 | baseline_reward = -0.1779046340768385\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 34: last loss = 0.10156\n",
      "eval step --\n",
      "\n",
      "Step 34: val_rewards = -0.10672800559323477 | baseline_reward = -0.12004309054607247\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 35: last loss = -0.04468\n",
      "eval step --\n",
      "\n",
      "Step 35: val_rewards = -0.04033628932754646 | baseline_reward = 0.37125188513479357\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 36: last loss = 0.12094\n",
      "eval step --\n",
      "\n",
      "Step 36: val_rewards = -0.028857059978099807 | baseline_reward = 0.4271139492873292\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 37: last loss = 0.19010\n",
      "eval step --\n",
      "\n",
      "Step 37: val_rewards = 0.0740021189366838 | baseline_reward = 0.3538580199375594\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 38: last loss = 0.27818\n",
      "eval step --\n",
      "\n",
      "Step 38: val_rewards = -0.03903274760404183 | baseline_reward = -0.19449346654345587\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 39: last loss = 0.59459\n",
      "eval step --\n",
      "\n",
      "Step 39: val_rewards = -0.09962279170220707 | baseline_reward = -0.2255425204676616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 40: last loss = 0.56098\n",
      "eval step --\n",
      "\n",
      "Step 40: val_rewards = 0.15580258590534865 | baseline_reward = -0.12430381157482846\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 41: last loss = 0.28790\n",
      "eval step --\n",
      "\n",
      "Step 41: val_rewards = 0.23359876455763637 | baseline_reward = 0.44493492314737587\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 42: last loss = 0.74220\n",
      "eval step --\n",
      "\n",
      "Step 42: val_rewards = 0.4213664639976206 | baseline_reward = 0.8254197826811209\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 43: last loss = 0.12522\n",
      "eval step --\n",
      "\n",
      "Step 43: val_rewards = 0.16385695440618026 | baseline_reward = 0.37631735284729056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 44: last loss = 0.77951\n",
      "eval step --\n",
      "\n",
      "Step 44: val_rewards = -0.07874269855332108 | baseline_reward = 0.11731911556164225\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 45: last loss = 0.06067\n",
      "eval step --\n",
      "\n",
      "Step 45: val_rewards = 0.266207034140161 | baseline_reward = 0.45055211890790137\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 46: last loss = 0.45581\n",
      "eval step --\n",
      "\n",
      "Step 46: val_rewards = 0.350514600619233 | baseline_reward = 0.5770104337391783\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 47: last loss = 0.56547\n",
      "eval step --\n",
      "\n",
      "Step 47: val_rewards = 0.15337520278363337 | baseline_reward = -0.09608712917273023\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 48: last loss = 0.10363\n",
      "eval step --\n",
      "\n",
      "Step 48: val_rewards = 0.02584525544622311 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 49: last loss = 0.25432\n",
      "eval step --\n",
      "\n",
      "Step 49: val_rewards = 0.02775339472692828 | baseline_reward = -0.05442783963941461\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 50: last loss = 0.13774\n",
      "eval step --\n",
      "\n",
      "Step 50: val_rewards = 0.16391828432636724 | baseline_reward = 0.7277122264121534\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 51: last loss = 0.20688\n",
      "eval step --\n",
      "\n",
      "Step 51: val_rewards = 0.587835873324429 | baseline_reward = 0.7881221759723845\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 52: last loss = 0.81781\n",
      "eval step --\n",
      "\n",
      "Step 52: val_rewards = -0.028940283662001083 | baseline_reward = 0.3118687685849124\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 53: last loss = 0.23788\n",
      "eval step --\n",
      "\n",
      "Step 53: val_rewards = 0.06698906262129935 | baseline_reward = -0.16904979893747296\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 54: last loss = 0.15822\n",
      "eval step --\n",
      "\n",
      "Step 54: val_rewards = -0.039061352963855614 | baseline_reward = 0.4117356102694612\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 55: last loss = 0.66484\n",
      "eval step --\n",
      "\n",
      "Step 55: val_rewards = 0.49867666660095566 | baseline_reward = 0.9626438844346673\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 56: last loss = 0.15948\n",
      "eval step --\n",
      "\n",
      "Step 56: val_rewards = 0.0089913448762935 | baseline_reward = 0.4917295336353529\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 57: last loss = 1.50095\n",
      "eval step --\n",
      "\n",
      "Step 57: val_rewards = 0.448126596135381 | baseline_reward = 0.4874748681648409\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 58: last loss = 0.17775\n",
      "eval step --\n",
      "\n",
      "Step 58: val_rewards = 0.0024828502730747494 | baseline_reward = -0.13239103663513663\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 59: last loss = 0.85123\n",
      "eval step --\n",
      "\n",
      "Step 59: val_rewards = 0.18855820408908397 | baseline_reward = -0.09917551120496682\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 60: last loss = 0.42899\n",
      "eval step --\n",
      "\n",
      "Step 60: val_rewards = 0.3196946093222467 | baseline_reward = 0.7669808496689923\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 61: last loss = 0.24868\n",
      "eval step --\n",
      "\n",
      "Step 61: val_rewards = 0.09884203005777964 | baseline_reward = 0.28706217188086874\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 62: last loss = -0.02869\n",
      "eval step --\n",
      "\n",
      "Step 62: val_rewards = 0.06496316869850609 | baseline_reward = 0.20034468310807002\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 63: last loss = 1.70482\n",
      "eval step --\n",
      "\n",
      "Step 63: val_rewards = 0.02435777675474008 | baseline_reward = 0.3060921456196715\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 64: last loss = 0.32102\n",
      "eval step --\n",
      "\n",
      "Step 64: val_rewards = 0.19429083616079926 | baseline_reward = 0.31717491102312284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 65: last loss = 0.19585\n",
      "eval step --\n",
      "\n",
      "Step 65: val_rewards = 0.48846871132513925 | baseline_reward = 1.146428138865238\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 66: last loss = 0.81568\n",
      "eval step --\n",
      "\n",
      "Step 66: val_rewards = 0.23309013810902365 | baseline_reward = 0.7998046578887152\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 67: last loss = 0.97059\n",
      "eval step --\n",
      "\n",
      "Step 67: val_rewards = 0.11071548954202988 | baseline_reward = -0.06703971028350392\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 68: last loss = 0.38870\n",
      "eval step --\n",
      "\n",
      "Step 68: val_rewards = 0.09365080975160651 | baseline_reward = 0.5903940656341876\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 69: last loss = 0.27134\n",
      "eval step --\n",
      "\n",
      "Step 69: val_rewards = 0.8577775602381548 | baseline_reward = 0.9626438844346673\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 70: last loss = 0.06106\n",
      "eval step --\n",
      "\n",
      "Step 70: val_rewards = 0.0170845434099203 | baseline_reward = -0.207119952210963\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 71: last loss = 0.20428\n",
      "eval step --\n",
      "\n",
      "Step 71: val_rewards = 0.16216689452055574 | baseline_reward = 0.861823392070796\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 72: last loss = 0.80180\n",
      "eval step --\n",
      "\n",
      "Step 72: val_rewards = 0.6303415950996504 | baseline_reward = 0.8244034374954186\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 73: last loss = 0.18076\n",
      "eval step --\n",
      "\n",
      "Step 73: val_rewards = -0.02307634135855698 | baseline_reward = 0.4312131923869825\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 74: last loss = 0.19873\n",
      "eval step --\n",
      "\n",
      "Step 74: val_rewards = 0.16622572473942007 | baseline_reward = -0.2316945935310149\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 75: last loss = 0.53322\n",
      "eval step --\n",
      "\n",
      "Step 75: val_rewards = 0.2924128796696434 | baseline_reward = 0.7009177250988704\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 76: last loss = 0.13924\n",
      "eval step --\n",
      "\n",
      "Step 76: val_rewards = 0.1253426382454286 | baseline_reward = 0.3090145715378005\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 77: last loss = 0.61875\n",
      "eval step --\n",
      "\n",
      "Step 77: val_rewards = -0.11767978582080121 | baseline_reward = 0.468248880948304\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 78: last loss = 0.11388\n",
      "eval step --\n",
      "\n",
      "Step 78: val_rewards = 0.1887935894253174 | baseline_reward = 0.12380159765707535\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 79: last loss = 0.20577\n",
      "eval step --\n",
      "\n",
      "Step 79: val_rewards = -0.02552181685854773 | baseline_reward = -0.031455106902401365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 80: last loss = 0.30673\n",
      "eval step --\n",
      "\n",
      "Step 80: val_rewards = 0.07862006518332995 | baseline_reward = -0.16088979886726948\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 81: last loss = 0.11131\n",
      "eval step --\n",
      "\n",
      "Step 81: val_rewards = 0.2809893533976516 | baseline_reward = 0.5661528162848493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 82: last loss = 0.33489\n",
      "eval step --\n",
      "\n",
      "Step 82: val_rewards = 0.247608183212855 | baseline_reward = 0.3599848102006836\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 83: last loss = 0.27129\n",
      "eval step --\n",
      "\n",
      "Step 83: val_rewards = 0.3181995413400968 | baseline_reward = 0.24766792956249126\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 84: last loss = 0.38226\n",
      "eval step --\n",
      "\n",
      "Step 84: val_rewards = 0.22631904186757726 | baseline_reward = 0.28614782436479835\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 85: last loss = 0.26946\n",
      "eval step --\n",
      "\n",
      "Step 85: val_rewards = -0.040723613813496026 | baseline_reward = 0.4043101570866993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 86: last loss = -0.09063\n",
      "eval step --\n",
      "\n",
      "Step 86: val_rewards = 0.08531452319358433 | baseline_reward = 0.4312131923869825\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 87: last loss = -0.01496\n",
      "eval step --\n",
      "\n",
      "Step 87: val_rewards = 0.023004560599163344 | baseline_reward = 0.7220333265973591\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 88: last loss = 0.51572\n",
      "eval step --\n",
      "\n",
      "Step 88: val_rewards = 0.45411480090294987 | baseline_reward = 0.5661528162848493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 89: last loss = 0.90245\n",
      "eval step --\n",
      "\n",
      "Step 89: val_rewards = 0.08180821461278467 | baseline_reward = 0.2432610208392248\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 90: last loss = 0.43375\n",
      "eval step --\n",
      "\n",
      "Step 90: val_rewards = 0.23298096462195383 | baseline_reward = 0.35272161195344426\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 91: last loss = 0.87215\n",
      "eval step --\n",
      "\n",
      "Step 91: val_rewards = 0.00917232060571185 | baseline_reward = 0.28708945736553004\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 92: last loss = -0.00434\n",
      "eval step --\n",
      "\n",
      "Step 92: val_rewards = 0.019218983625736777 | baseline_reward = -0.05472005817189169\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 93: last loss = 0.52084\n",
      "eval step --\n",
      "\n",
      "Step 93: val_rewards = 0.23418208008289243 | baseline_reward = 0.2702599925690532\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 94: last loss = 0.23995\n",
      "eval step --\n",
      "\n",
      "Step 94: val_rewards = 0.13034414737917258 | baseline_reward = 0.3021740309935395\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 95: last loss = 0.15826\n",
      "eval step --\n",
      "\n",
      "Step 95: val_rewards = -0.12990366994521757 | baseline_reward = -0.2863321481641413\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 96: last loss = 0.54646\n",
      "eval step --\n",
      "\n",
      "Step 96: val_rewards = 0.5313200221529448 | baseline_reward = 0.9440461755678171\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 97: last loss = 0.16729\n",
      "eval step --\n",
      "\n",
      "Step 97: val_rewards = 0.5470469882941361 | baseline_reward = 1.0688939544432159\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 98: last loss = 0.54189\n",
      "eval step --\n",
      "\n",
      "Step 98: val_rewards = 0.17191096266520772 | baseline_reward = 0.11667247592164368\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 99: last loss = 0.14847\n",
      "eval step --\n",
      "\n",
      "Step 99: val_rewards = 0.14045333262905696 | baseline_reward = 0.03231802212210823\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 100: last loss = 0.42604\n",
      "eval step --\n",
      "\n",
      "Step 100: val_rewards = 0.12147992855858 | baseline_reward = 0.5925582127279282\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 101: last loss = 0.39964\n",
      "eval step --\n",
      "\n",
      "Step 101: val_rewards = 0.27050627477230565 | baseline_reward = 0.17499492880561787\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 102: last loss = 0.77179\n",
      "eval step --\n",
      "\n",
      "Step 102: val_rewards = 0.3663124144559623 | baseline_reward = 0.6043944165896826\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 103: last loss = 0.27584\n",
      "eval step --\n",
      "\n",
      "Step 103: val_rewards = 0.20431957674217563 | baseline_reward = 0.11015714289866095\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 104: last loss = 0.40820\n",
      "eval step --\n",
      "\n",
      "Step 104: val_rewards = -0.022087692917519934 | baseline_reward = 0.2995999336319901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 105: last loss = 0.12607\n",
      "eval step --\n",
      "\n",
      "Step 105: val_rewards = 0.1617641382490691 | baseline_reward = 0.5738405502523503\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 106: last loss = 0.14207\n",
      "eval step --\n",
      "\n",
      "Step 106: val_rewards = 0.05191032090108012 | baseline_reward = 0.17436261906408138\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 107: last loss = 0.34472\n",
      "eval step --\n",
      "\n",
      "Step 107: val_rewards = 0.17016385518095356 | baseline_reward = 0.6220125350710067\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 108: last loss = 0.10164\n",
      "eval step --\n",
      "\n",
      "Step 108: val_rewards = 0.185591698875983 | baseline_reward = 0.3599848102006836\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 109: last loss = 0.52789\n",
      "eval step --\n",
      "\n",
      "Step 109: val_rewards = -0.0008307486370158603 | baseline_reward = 0.6867815855147756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 110: last loss = -0.01281\n",
      "eval step --\n",
      "\n",
      "Step 110: val_rewards = 0.007766623147021838 | baseline_reward = 0.30276011091662647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 111: last loss = 0.58746\n",
      "eval step --\n",
      "\n",
      "Step 111: val_rewards = 0.23064520207548705 | baseline_reward = 0.23888400621369194\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 112: last loss = 0.36023\n",
      "eval step --\n",
      "\n",
      "Step 112: val_rewards = 0.5680066355877346 | baseline_reward = 0.4538587421274766\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 113: last loss = 0.13020\n",
      "eval step --\n",
      "\n",
      "Step 113: val_rewards = -0.056655267411611775 | baseline_reward = 0.3702117888496327\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 114: last loss = 0.21648\n",
      "eval step --\n",
      "\n",
      "Step 114: val_rewards = 0.1830480336799783 | baseline_reward = 0.5567011827009365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 115: last loss = 0.19009\n",
      "eval step --\n",
      "\n",
      "Step 115: val_rewards = 0.0438704641283752 | baseline_reward = -0.09309637816407894\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 116: last loss = 0.55967\n",
      "eval step --\n",
      "\n",
      "Step 116: val_rewards = 0.2757896452212447 | baseline_reward = 0.46218582008508496\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 117: last loss = 0.43331\n",
      "eval step --\n",
      "\n",
      "Step 117: val_rewards = -0.09476400120862691 | baseline_reward = -0.05421979556190041\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 118: last loss = 0.33497\n",
      "eval step --\n",
      "\n",
      "Step 118: val_rewards = 0.0472944481759362 | baseline_reward = -0.02119186696164961\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 119: last loss = 0.76625\n",
      "eval step --\n",
      "\n",
      "Step 119: val_rewards = -0.021334152829174925 | baseline_reward = -0.24620996402409803\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 120: last loss = 0.23254\n",
      "eval step --\n",
      "\n",
      "Step 120: val_rewards = 0.15079499298556426 | baseline_reward = 0.4921055499273047\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 121: last loss = 0.76655\n",
      "eval step --\n",
      "\n",
      "Step 121: val_rewards = 0.5689093660672062 | baseline_reward = 1.0499871651077277\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 122: last loss = 0.91368\n",
      "eval step --\n",
      "\n",
      "Step 122: val_rewards = 0.07836605167867179 | baseline_reward = 0.19092567740642546\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 123: last loss = 0.61155\n",
      "eval step --\n",
      "\n",
      "Step 123: val_rewards = 0.19553216098167608 | baseline_reward = 0.20296428271805433\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 124: last loss = 0.06011\n",
      "eval step --\n",
      "\n",
      "Step 124: val_rewards = -0.007116837320684461 | baseline_reward = -0.07780050973361903\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 125: last loss = 0.21643\n",
      "eval step --\n",
      "\n",
      "Step 125: val_rewards = 0.1796895442567598 | baseline_reward = 0.6757802668183599\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 126: last loss = 0.02532\n",
      "eval step --\n",
      "\n",
      "Step 126: val_rewards = 0.14916907450048952 | baseline_reward = 0.3872986398128717\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 127: last loss = 0.29748\n",
      "eval step --\n",
      "\n",
      "Step 127: val_rewards = 0.0202144541612693 | baseline_reward = -0.009088879188255738\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 128: last loss = 0.17014\n",
      "eval step --\n",
      "\n",
      "Step 128: val_rewards = 0.3422653979755847 | baseline_reward = 0.8259468310451428\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 129: last loss = 0.03134\n",
      "eval step --\n",
      "\n",
      "Step 129: val_rewards = 0.1596097245916474 | baseline_reward = 0.5313125594188126\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 130: last loss = 0.26677\n",
      "eval step --\n",
      "\n",
      "Step 130: val_rewards = 0.16129933777688987 | baseline_reward = 0.4381560332401185\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 131: last loss = 0.58062\n",
      "eval step --\n",
      "\n",
      "Step 131: val_rewards = 0.044066253552799375 | baseline_reward = 0.0014208169237701382\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 132: last loss = 0.65434\n",
      "eval step --\n",
      "\n",
      "Step 132: val_rewards = 0.2848402574804805 | baseline_reward = 0.58349669716649\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 133: last loss = 0.30227\n",
      "eval step --\n",
      "\n",
      "Step 133: val_rewards = 0.006388159071469503 | baseline_reward = -0.21418759877625257\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 134: last loss = 0.26005\n",
      "eval step --\n",
      "\n",
      "Step 134: val_rewards = -0.05056917418284867 | baseline_reward = -0.07733072873150211\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 135: last loss = 0.08296\n",
      "eval step --\n",
      "\n",
      "Step 135: val_rewards = 0.06625105292872849 | baseline_reward = -0.0484675220347934\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 136: last loss = 0.32650\n",
      "eval step --\n",
      "\n",
      "Step 136: val_rewards = 0.20369365073654624 | baseline_reward = 0.14477155423780128\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 137: last loss = 0.07033\n",
      "eval step --\n",
      "\n",
      "Step 137: val_rewards = -0.10513624959825157 | baseline_reward = -0.09309637816407894\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 138: last loss = 0.45051\n",
      "eval step --\n",
      "\n",
      "Step 138: val_rewards = 0.091079568562705 | baseline_reward = 0.1477246880925813\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 139: last loss = 0.00830\n",
      "eval step --\n",
      "\n",
      "Step 139: val_rewards = -0.05576180835225597 | baseline_reward = -0.2255425204676616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 140: last loss = 0.70851\n",
      "eval step --\n",
      "\n",
      "Step 140: val_rewards = 0.4763434311020889 | baseline_reward = 0.6846530134363902\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 141: last loss = 0.63659\n",
      "eval step --\n",
      "\n",
      "Step 141: val_rewards = 0.09150219028934226 | baseline_reward = 0.5324035047867532\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 142: last loss = 0.00857\n",
      "eval step --\n",
      "\n",
      "Step 142: val_rewards = 0.12359356590721915 | baseline_reward = 0.007091603586160839\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 143: last loss = -0.25847\n",
      "eval step --\n",
      "\n",
      "Step 143: val_rewards = 0.12491381741597855 | baseline_reward = 0.31717606811587057\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 144: last loss = 0.20812\n",
      "eval step --\n",
      "\n",
      "Step 144: val_rewards = 0.2808775031635331 | baseline_reward = 0.3231605434124334\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 145: last loss = 0.10803\n",
      "eval step --\n",
      "\n",
      "Step 145: val_rewards = 0.06526330198070848 | baseline_reward = -0.07213108081473303\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 146: last loss = 0.85451\n",
      "eval step --\n",
      "\n",
      "Step 146: val_rewards = 0.31878172010155525 | baseline_reward = 0.6355862422471259\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 147: last loss = 0.40889\n",
      "eval step --\n",
      "\n",
      "Step 147: val_rewards = -0.012652376037798277 | baseline_reward = -0.1717815554990048\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 148: last loss = 0.10184\n",
      "eval step --\n",
      "\n",
      "Step 148: val_rewards = 0.1223870676576661 | baseline_reward = -0.09608712917273023\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 149: last loss = 0.18249\n",
      "eval step --\n",
      "\n",
      "Step 149: val_rewards = 0.005848532655598321 | baseline_reward = -0.2255425204676616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 150: last loss = 0.29171\n",
      "eval step --\n",
      "\n",
      "Step 150: val_rewards = 0.06843824203666381 | baseline_reward = -0.19844914691930596\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 151: last loss = 0.25981\n",
      "eval step --\n",
      "\n",
      "Step 151: val_rewards = -0.07408746984211745 | baseline_reward = 0.7277122264121534\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 152: last loss = 0.84162\n",
      "eval step --\n",
      "\n",
      "Step 152: val_rewards = 0.13834306858558226 | baseline_reward = -0.06794982988207018\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 153: last loss = -0.00250\n",
      "eval step --\n",
      "\n",
      "Step 153: val_rewards = 0.043886071301820026 | baseline_reward = 0.1904116984870983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 154: last loss = 0.70311\n",
      "eval step --\n",
      "\n",
      "Step 154: val_rewards = -0.028461531329378897 | baseline_reward = 0.2738009290954773\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 155: last loss = 0.70905\n",
      "eval step --\n",
      "\n",
      "Step 155: val_rewards = -0.03596146119168986 | baseline_reward = -0.06360012958771603\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 156: last loss = 0.69193\n",
      "eval step --\n",
      "\n",
      "Step 156: val_rewards = 0.11287564874893874 | baseline_reward = 0.21159166371931457\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 157: last loss = 0.91600\n",
      "eval step --\n",
      "\n",
      "Step 157: val_rewards = 0.07138228017288077 | baseline_reward = 0.45225390939114807\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 158: last loss = 0.33084\n",
      "eval step --\n",
      "\n",
      "Step 158: val_rewards = 0.15652623972544583 | baseline_reward = -0.02119186696164961\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 159: last loss = 0.07473\n",
      "eval step --\n",
      "\n",
      "Step 159: val_rewards = -0.007321083216115849 | baseline_reward = 0.16603698525017901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 160: last loss = 0.65276\n",
      "eval step --\n",
      "\n",
      "Step 160: val_rewards = 0.04513343767506927 | baseline_reward = 0.1904116984870983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 161: last loss = 0.75316\n",
      "eval step --\n",
      "\n",
      "Step 161: val_rewards = 0.0589028706355024 | baseline_reward = 0.0002751260614751749\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 162: last loss = 0.94181\n",
      "eval step --\n",
      "\n",
      "Step 162: val_rewards = 0.23127441140870977 | baseline_reward = 0.8051362424683901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 163: last loss = 0.21837\n",
      "eval step --\n",
      "\n",
      "Step 163: val_rewards = 0.7616663199051926 | baseline_reward = 0.808899641240367\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 164: last loss = 0.61896\n",
      "eval step --\n",
      "\n",
      "Step 164: val_rewards = 0.10559080314309469 | baseline_reward = 0.7609543791645151\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 165: last loss = 0.16748\n",
      "eval step --\n",
      "\n",
      "Step 165: val_rewards = 0.018965554061791994 | baseline_reward = 0.2124053309131323\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 166: last loss = 0.22915\n",
      "eval step --\n",
      "\n",
      "Step 166: val_rewards = 0.024506715430342208 | baseline_reward = 0.13329417664528073\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 167: last loss = 0.54639\n",
      "eval step --\n",
      "\n",
      "Step 167: val_rewards = 0.25647325737851123 | baseline_reward = 0.14477155423780128\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 168: last loss = 1.07841\n",
      "eval step --\n",
      "\n",
      "Step 168: val_rewards = 0.10325077431966599 | baseline_reward = -0.05038019433944224\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 169: last loss = 0.17303\n",
      "eval step --\n",
      "\n",
      "Step 169: val_rewards = 0.10021233165773585 | baseline_reward = 0.7009177250988704\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 170: last loss = 0.06935\n",
      "eval step --\n",
      "\n",
      "Step 170: val_rewards = 0.014620844724414784 | baseline_reward = 0.23159805988313442\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 171: last loss = 0.54962\n",
      "eval step --\n",
      "\n",
      "Step 171: val_rewards = 0.07480764187256943 | baseline_reward = -0.020216636464273522\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 172: last loss = 0.06944\n",
      "eval step --\n",
      "\n",
      "Step 172: val_rewards = 0.26616814438775377 | baseline_reward = 0.22777368070532983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 173: last loss = 0.35760\n",
      "eval step --\n",
      "\n",
      "Step 173: val_rewards = 0.059985319839369534 | baseline_reward = 0.5495212447107057\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 174: last loss = 0.10700\n",
      "eval step --\n",
      "\n",
      "Step 174: val_rewards = 0.07358809232907124 | baseline_reward = -0.07423859050900064\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 175: last loss = 1.06678\n",
      "eval step --\n",
      "\n",
      "Step 175: val_rewards = 0.021382866995121427 | baseline_reward = 0.29129226906421013\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 176: last loss = 0.27138\n",
      "eval step --\n",
      "\n",
      "Step 176: val_rewards = 0.1878460304673687 | baseline_reward = 0.6662998548169972\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 177: last loss = 0.29672\n",
      "eval step --\n",
      "\n",
      "Step 177: val_rewards = 0.022164373280302036 | baseline_reward = 0.40204321052583647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 178: last loss = 0.45118\n",
      "eval step --\n",
      "\n",
      "Step 178: val_rewards = 0.16219223643857444 | baseline_reward = 0.5483738482333121\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 179: last loss = 0.51962\n",
      "eval step --\n",
      "\n",
      "Step 179: val_rewards = 0.2459463729110313 | baseline_reward = 0.5005914990654504\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 180: last loss = 0.40935\n",
      "eval step --\n",
      "\n",
      "Step 180: val_rewards = 0.004705356106089027 | baseline_reward = 0.387986503372861\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 181: last loss = 0.15693\n",
      "eval step --\n",
      "\n",
      "Step 181: val_rewards = 0.3140002731076912 | baseline_reward = 0.58349669716649\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 182: last loss = 0.09111\n",
      "eval step --\n",
      "\n",
      "Step 182: val_rewards = 0.04165153821615763 | baseline_reward = 0.046967505744021296\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 183: last loss = 0.47654\n",
      "eval step --\n",
      "\n",
      "Step 183: val_rewards = -0.12759962715811737 | baseline_reward = -0.14745649769542646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 184: last loss = 0.22135\n",
      "eval step --\n",
      "\n",
      "Step 184: val_rewards = -0.018287226977052824 | baseline_reward = -0.06485393844787087\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 185: last loss = 1.74947\n",
      "eval step --\n",
      "\n",
      "Step 185: val_rewards = 0.2510652634546564 | baseline_reward = 0.28614782436479835\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 186: last loss = 0.58787\n",
      "eval step --\n",
      "\n",
      "Step 186: val_rewards = 0.10611485403982103 | baseline_reward = -0.07213108081473303\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 187: last loss = 0.58021\n",
      "eval step --\n",
      "\n",
      "Step 187: val_rewards = 0.3460116542665637 | baseline_reward = 0.14934381229045454\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 188: last loss = -0.00953\n",
      "eval step --\n",
      "\n",
      "Step 188: val_rewards = 0.1452705516015216 | baseline_reward = 0.3904089531078603\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 189: last loss = -0.00174\n",
      "eval step --\n",
      "\n",
      "Step 189: val_rewards = 0.4406568120080004 | baseline_reward = 0.527794423711986\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 190: last loss = 0.46671\n",
      "eval step --\n",
      "\n",
      "Step 190: val_rewards = 0.2222027278522056 | baseline_reward = 0.31571909219847616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 191: last loss = 0.08874\n",
      "eval step --\n",
      "\n",
      "Step 191: val_rewards = -0.20045368758841794 | baseline_reward = -0.06909818671718444\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 192: last loss = 0.32338\n",
      "eval step --\n",
      "\n",
      "Step 192: val_rewards = 0.17734736140233687 | baseline_reward = -0.011238943939511286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 193: last loss = 0.05217\n",
      "eval step --\n",
      "\n",
      "Step 193: val_rewards = -0.022723679770015468 | baseline_reward = -0.04377323144370932\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 194: last loss = 0.27506\n",
      "eval step --\n",
      "\n",
      "Step 194: val_rewards = 0.6167049454635344 | baseline_reward = 0.9100759627035324\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 195: last loss = 1.05851\n",
      "eval step --\n",
      "\n",
      "Step 195: val_rewards = -0.008953711851211767 | baseline_reward = 0.3670258653332694\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 196: last loss = 0.23681\n",
      "eval step --\n",
      "\n",
      "Step 196: val_rewards = -0.0604291085091371 | baseline_reward = -0.15014093262043282\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 197: last loss = 0.65080\n",
      "eval step --\n",
      "\n",
      "Step 197: val_rewards = 0.19471444630324128 | baseline_reward = 0.3475072327521015\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 198: last loss = 0.05692\n",
      "eval step --\n",
      "\n",
      "Step 198: val_rewards = 0.012485572374101873 | baseline_reward = -0.01684594218315476\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 199: last loss = 0.14175\n",
      "eval step --\n",
      "\n",
      "Step 199: val_rewards = 0.08758341571959417 | baseline_reward = 0.255747197151394\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 200: last loss = 0.34873\n",
      "eval step --\n",
      "\n",
      "Step 200: val_rewards = 0.07432798243431901 | baseline_reward = 0.1233275802676543\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 201: last loss = 0.31499\n",
      "eval step --\n",
      "\n",
      "Step 201: val_rewards = -0.004964037398614345 | baseline_reward = -0.12465234709331634\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 202: last loss = 0.35094\n",
      "eval step --\n",
      "\n",
      "Step 202: val_rewards = 0.6241872591990357 | baseline_reward = 0.6346321514508708\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 203: last loss = -0.09173\n",
      "eval step --\n",
      "\n",
      "Step 203: val_rewards = 0.2174801889483636 | baseline_reward = 0.43422096382035263\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 204: last loss = 0.05454\n",
      "eval step --\n",
      "\n",
      "Step 204: val_rewards = 0.08974686254902463 | baseline_reward = -0.17184255023297654\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 205: last loss = 0.83882\n",
      "eval step --\n",
      "\n",
      "Step 205: val_rewards = 0.13418785449516074 | baseline_reward = -0.14586522337179472\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 206: last loss = 0.12547\n",
      "eval step --\n",
      "\n",
      "Step 206: val_rewards = 0.06946082347004019 | baseline_reward = 0.38995650003014914\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 207: last loss = 0.13463\n",
      "eval step --\n",
      "\n",
      "Step 207: val_rewards = 0.06337705851913944 | baseline_reward = 0.17486070796494071\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 208: last loss = -0.12368\n",
      "eval step --\n",
      "\n",
      "Step 208: val_rewards = 0.4746428321700026 | baseline_reward = 0.8543664175585357\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 209: last loss = 0.21546\n",
      "eval step --\n",
      "\n",
      "Step 209: val_rewards = 0.5780812504785241 | baseline_reward = 0.7754039079169707\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 210: last loss = 0.06954\n",
      "eval step --\n",
      "\n",
      "Step 210: val_rewards = 0.21751520011936112 | baseline_reward = -0.1779046340768385\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 211: last loss = 0.72952\n",
      "eval step --\n",
      "\n",
      "Step 211: val_rewards = 0.21951627458343517 | baseline_reward = 0.4921055499273047\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 212: last loss = 0.13921\n",
      "eval step --\n",
      "\n",
      "Step 212: val_rewards = 0.13747636164261895 | baseline_reward = 0.2124053309131323\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 213: last loss = 1.42508\n",
      "eval step --\n",
      "\n",
      "Step 213: val_rewards = 0.036318233187225255 | baseline_reward = -0.05038019433944224\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 214: last loss = 0.53886\n",
      "eval step --\n",
      "\n",
      "Step 214: val_rewards = -0.03420147391695124 | baseline_reward = 0.585014475106025\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 215: last loss = -0.00272\n",
      "eval step --\n",
      "\n",
      "Step 215: val_rewards = 0.10268083248887765 | baseline_reward = -0.27622771429108856\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 216: last loss = 0.03878\n",
      "eval step --\n",
      "\n",
      "Step 216: val_rewards = 0.14171683585703476 | baseline_reward = 0.4680217543380443\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 217: last loss = 0.30965\n",
      "eval step --\n",
      "\n",
      "Step 217: val_rewards = -0.09452498809585237 | baseline_reward = 0.28708945736553004\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 218: last loss = 0.22188\n",
      "eval step --\n",
      "\n",
      "Step 218: val_rewards = -0.11031189788651113 | baseline_reward = 0.44352376289444034\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 219: last loss = 0.21079\n",
      "eval step --\n",
      "\n",
      "Step 219: val_rewards = 0.13869193916682093 | baseline_reward = 0.03231802212210823\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 220: last loss = 0.38005\n",
      "eval step --\n",
      "\n",
      "Step 220: val_rewards = 0.3294228774511572 | baseline_reward = 0.7346818070851453\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 221: last loss = 0.84199\n",
      "eval step --\n",
      "\n",
      "Step 221: val_rewards = 0.16622674383345593 | baseline_reward = 0.5813186457512338\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 222: last loss = 0.28324\n",
      "eval step --\n",
      "\n",
      "Step 222: val_rewards = -0.17829980284813024 | baseline_reward = 0.5313125594188126\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 223: last loss = -0.10822\n",
      "eval step --\n",
      "\n",
      "Step 223: val_rewards = -0.14596332058319164 | baseline_reward = -0.05892795948965544\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 224: last loss = 0.04049\n",
      "eval step --\n",
      "\n",
      "Step 224: val_rewards = 0.03344833332165548 | baseline_reward = 0.602770560327721\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 225: last loss = 0.79169\n",
      "eval step --\n",
      "\n",
      "Step 225: val_rewards = 0.2557285805537804 | baseline_reward = -0.2024200961693741\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 226: last loss = 0.89444\n",
      "eval step --\n",
      "\n",
      "Step 226: val_rewards = 0.23145358143758382 | baseline_reward = 0.3231605434124334\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 227: last loss = 0.44167\n",
      "eval step --\n",
      "\n",
      "Step 227: val_rewards = -0.0003208070634373184 | baseline_reward = 0.3672249022147251\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 228: last loss = 0.10220\n",
      "eval step --\n",
      "\n",
      "Step 228: val_rewards = 0.12789343234704795 | baseline_reward = 0.6585371418915216\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 229: last loss = 0.20815\n",
      "eval step --\n",
      "\n",
      "Step 229: val_rewards = -0.060520665845363836 | baseline_reward = -0.1717815554990048\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 230: last loss = 0.72746\n",
      "eval step --\n",
      "\n",
      "Step 230: val_rewards = 0.25675277507546684 | baseline_reward = 0.585014475106025\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 231: last loss = 0.21724\n",
      "eval step --\n",
      "\n",
      "Step 231: val_rewards = 0.2633361040707331 | baseline_reward = 0.14934381229045454\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 232: last loss = 0.90402\n",
      "eval step --\n",
      "\n",
      "Step 232: val_rewards = 0.10305370786700632 | baseline_reward = 0.14505887930825864\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 233: last loss = 0.72759\n",
      "eval step --\n",
      "\n",
      "Step 233: val_rewards = 0.2708862914377722 | baseline_reward = 0.7346818070851453\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 234: last loss = 0.77995\n",
      "eval step --\n",
      "\n",
      "Step 234: val_rewards = 0.02453203063161373 | baseline_reward = 0.11731911556164225\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 235: last loss = -0.01035\n",
      "eval step --\n",
      "\n",
      "Step 235: val_rewards = 0.11887991067747362 | baseline_reward = 0.22777368070532983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 236: last loss = 0.05185\n",
      "eval step --\n",
      "\n",
      "Step 236: val_rewards = -0.028094810661320385 | baseline_reward = -0.11179834278066905\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 237: last loss = 0.02671\n",
      "eval step --\n",
      "\n",
      "Step 237: val_rewards = 0.1903407822188889 | baseline_reward = 0.40042586228915794\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 238: last loss = 0.02881\n",
      "eval step --\n",
      "\n",
      "Step 238: val_rewards = 0.3699823845551578 | baseline_reward = 0.4284104212671813\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 239: last loss = -0.02143\n",
      "eval step --\n",
      "\n",
      "Step 239: val_rewards = 0.8585124081493899 | baseline_reward = 0.6846530134363902\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 240: last loss = 0.26263\n",
      "eval step --\n",
      "\n",
      "Step 240: val_rewards = 0.2076050901904702 | baseline_reward = 0.40468641733178123\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 241: last loss = 0.18185\n",
      "eval step --\n",
      "\n",
      "Step 241: val_rewards = 0.3472195924705933 | baseline_reward = 0.682422985440802\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 242: last loss = 0.86351\n",
      "eval step --\n",
      "\n",
      "Step 242: val_rewards = 0.11221996468625164 | baseline_reward = 0.6867815855147756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 243: last loss = 0.18185\n",
      "eval step --\n",
      "\n",
      "Step 243: val_rewards = 0.33872722431437874 | baseline_reward = 0.13954921367937764\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 244: last loss = 0.20192\n",
      "eval step --\n",
      "\n",
      "Step 244: val_rewards = 0.5172143905963342 | baseline_reward = 0.5178446205859352\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 245: last loss = 0.92496\n",
      "eval step --\n",
      "\n",
      "Step 245: val_rewards = 0.6944503232308818 | baseline_reward = 1.1910070511535245\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 246: last loss = 1.63371\n",
      "eval step --\n",
      "\n",
      "Step 246: val_rewards = 0.233930826448222 | baseline_reward = 0.11015714289866095\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 247: last loss = 0.26564\n",
      "eval step --\n",
      "\n",
      "Step 247: val_rewards = -0.041433147530604106 | baseline_reward = 0.47472277483115344\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 248: last loss = 0.55710\n",
      "eval step --\n",
      "\n",
      "Step 248: val_rewards = 0.19005838934732816 | baseline_reward = 0.5153802792007658\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 249: last loss = 0.13595\n",
      "eval step --\n",
      "\n",
      "Step 249: val_rewards = 0.06770559922776279 | baseline_reward = 0.3390738911655097\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 250: last loss = 0.21479\n",
      "eval step --\n",
      "\n",
      "Step 250: val_rewards = -0.017063456168154648 | baseline_reward = -0.10561581099855756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 251: last loss = 0.11202\n",
      "eval step --\n",
      "\n",
      "Step 251: val_rewards = 0.04308562165703299 | baseline_reward = 0.7047432984001528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 252: last loss = 0.21046\n",
      "eval step --\n",
      "\n",
      "Step 252: val_rewards = 0.2870550575215678 | baseline_reward = 0.6164913848726767\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 253: last loss = 0.12037\n",
      "eval step --\n",
      "\n",
      "Step 253: val_rewards = 0.12845528489636437 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 254: last loss = 0.06088\n",
      "eval step --\n",
      "\n",
      "Step 254: val_rewards = 0.044062961454799265 | baseline_reward = -0.12465234709331634\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 255: last loss = -0.03235\n",
      "eval step --\n",
      "\n",
      "Step 255: val_rewards = 0.1536407888037957 | baseline_reward = -0.011238943939511286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 256: last loss = 0.33327\n",
      "eval step --\n",
      "\n",
      "Step 256: val_rewards = 0.821283830731509 | baseline_reward = 0.808899641240367\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 257: last loss = 0.43429\n",
      "eval step --\n",
      "\n",
      "Step 257: val_rewards = 0.37578289412025295 | baseline_reward = 0.5328081386881862\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 258: last loss = 0.15115\n",
      "eval step --\n",
      "\n",
      "Step 258: val_rewards = 0.3482182140300769 | baseline_reward = 0.46218582008508496\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 259: last loss = -0.00604\n",
      "eval step --\n",
      "\n",
      "Step 259: val_rewards = 0.6610869379882799 | baseline_reward = 0.8244034374954186\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 260: last loss = 0.15107\n",
      "eval step --\n",
      "\n",
      "Step 260: val_rewards = 0.081527060064389 | baseline_reward = 0.11261059360014673\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 261: last loss = 0.69428\n",
      "eval step --\n",
      "\n",
      "Step 261: val_rewards = 0.291782998503682 | baseline_reward = 0.5882721255930647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 262: last loss = 0.13505\n",
      "eval step --\n",
      "\n",
      "Step 262: val_rewards = 0.1492493401792344 | baseline_reward = 0.9026240930820785\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 263: last loss = 0.58587\n",
      "eval step --\n",
      "\n",
      "Step 263: val_rewards = 0.7722096596849696 | baseline_reward = 0.8543664175585357\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 264: last loss = 0.11481\n",
      "eval step --\n",
      "\n",
      "Step 264: val_rewards = 0.20571597875422715 | baseline_reward = 0.7775086599707709\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 265: last loss = 0.24635\n",
      "eval step --\n",
      "\n",
      "Step 265: val_rewards = 0.10598170123732954 | baseline_reward = 0.11261059360014673\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 266: last loss = 0.75708\n",
      "eval step --\n",
      "\n",
      "Step 266: val_rewards = 0.06510717391984418 | baseline_reward = 0.6024305476419072\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 267: last loss = 0.11053\n",
      "eval step --\n",
      "\n",
      "Step 267: val_rewards = 0.05292276666011272 | baseline_reward = 0.7277122264121534\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 268: last loss = 0.27538\n",
      "eval step --\n",
      "\n",
      "Step 268: val_rewards = 0.2339908933932076 | baseline_reward = -0.2114658332810284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 269: last loss = 0.32687\n",
      "eval step --\n",
      "\n",
      "Step 269: val_rewards = 0.20985083717494768 | baseline_reward = 0.24461665486811268\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 270: last loss = 0.40419\n",
      "eval step --\n",
      "\n",
      "Step 270: val_rewards = 0.3053647815579379 | baseline_reward = 0.685461084543975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 271: last loss = 0.23732\n",
      "eval step --\n",
      "\n",
      "Step 271: val_rewards = 0.05178139752932619 | baseline_reward = 0.6757802668183599\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 272: last loss = 0.87251\n",
      "eval step --\n",
      "\n",
      "Step 272: val_rewards = 0.15098697669376712 | baseline_reward = 0.5286367137926585\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 273: last loss = -0.00262\n",
      "eval step --\n",
      "\n",
      "Step 273: val_rewards = 0.35788919240643363 | baseline_reward = 0.8402781731484693\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 274: last loss = 0.19143\n",
      "eval step --\n",
      "\n",
      "Step 274: val_rewards = 0.31382469827638787 | baseline_reward = 0.37125188513479357\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 275: last loss = 0.22677\n",
      "eval step --\n",
      "\n",
      "Step 275: val_rewards = -0.066095396035975 | baseline_reward = -0.12430381157482846\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 276: last loss = 0.94754\n",
      "eval step --\n",
      "\n",
      "Step 276: val_rewards = -0.3697383303691505 | baseline_reward = -0.2269895269211927\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 277: last loss = -0.01811\n",
      "eval step --\n",
      "\n",
      "Step 277: val_rewards = 0.056105128844076944 | baseline_reward = 0.5671239306855195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 278: last loss = 0.03498\n",
      "eval step --\n",
      "\n",
      "Step 278: val_rewards = 0.4599327731185507 | baseline_reward = 0.755995986508099\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 279: last loss = 0.41663\n",
      "eval step --\n",
      "\n",
      "Step 279: val_rewards = 0.0811338631326651 | baseline_reward = -0.09351335608180286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 280: last loss = 0.24839\n",
      "eval step --\n",
      "\n",
      "Step 280: val_rewards = 0.19471639427262752 | baseline_reward = 0.508275459388004\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 281: last loss = 1.21050\n",
      "eval step --\n",
      "\n",
      "Step 281: val_rewards = 0.2879286112073321 | baseline_reward = 0.895889172658081\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 282: last loss = 0.85991\n",
      "eval step --\n",
      "\n",
      "Step 282: val_rewards = 0.15759435439773398 | baseline_reward = 0.602770560327721\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 283: last loss = 0.87870\n",
      "eval step --\n",
      "\n",
      "Step 283: val_rewards = 0.06592721395344803 | baseline_reward = -0.1580967988333066\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 284: last loss = 1.26794\n",
      "eval step --\n",
      "\n",
      "Step 284: val_rewards = 0.18411593138193105 | baseline_reward = 0.8873379280713072\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 285: last loss = 1.25569\n",
      "eval step --\n",
      "\n",
      "Step 285: val_rewards = 0.007410642619227054 | baseline_reward = 0.11261059360014673\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 286: last loss = 0.32657\n",
      "eval step --\n",
      "\n",
      "Step 286: val_rewards = 0.3585323198213063 | baseline_reward = 0.7682499609317\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 287: last loss = 0.91701\n",
      "eval step --\n",
      "\n",
      "Step 287: val_rewards = 0.004530470937799594 | baseline_reward = 0.43771144453979627\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 288: last loss = 0.31241\n",
      "eval step --\n",
      "\n",
      "Step 288: val_rewards = 0.3467159494388693 | baseline_reward = 0.58349669716649\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 289: last loss = 0.03581\n",
      "eval step --\n",
      "\n",
      "Step 289: val_rewards = -0.06614248809062649 | baseline_reward = -0.18220330705785379\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 290: last loss = 0.23531\n",
      "eval step --\n",
      "\n",
      "Step 290: val_rewards = -0.08676159835077311 | baseline_reward = -0.08023269704425942\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 291: last loss = 0.11711\n",
      "eval step --\n",
      "\n",
      "Step 291: val_rewards = 0.038027777330933155 | baseline_reward = 0.046967505744021296\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 292: last loss = 0.63649\n",
      "eval step --\n",
      "\n",
      "Step 292: val_rewards = 0.32253393957322657 | baseline_reward = -0.06794982988207018\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 293: last loss = 0.21945\n",
      "eval step --\n",
      "\n",
      "Step 293: val_rewards = 0.08231602679496719 | baseline_reward = 0.8873379280713072\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 294: last loss = 0.39848\n",
      "eval step --\n",
      "\n",
      "Step 294: val_rewards = 0.18925660305309444 | baseline_reward = 0.22777368070532983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 295: last loss = 1.65962\n",
      "eval step --\n",
      "\n",
      "Step 295: val_rewards = 0.002453730466093425 | baseline_reward = -0.16088979886726948\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 296: last loss = 0.59403\n",
      "eval step --\n",
      "\n",
      "Step 296: val_rewards = 0.1887079340035667 | baseline_reward = 0.685461084543975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 297: last loss = 0.35652\n",
      "eval step --\n",
      "\n",
      "Step 297: val_rewards = 0.24587690083820182 | baseline_reward = 0.30585187174118195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 298: last loss = 0.15167\n",
      "eval step --\n",
      "\n",
      "Step 298: val_rewards = 0.6032135616746183 | baseline_reward = 0.709124352651483\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 299: last loss = 0.46188\n",
      "eval step --\n",
      "\n",
      "Step 299: val_rewards = 0.1926374512813329 | baseline_reward = 0.2738009290954773\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 300: last loss = 0.24117\n",
      "eval step --\n",
      "\n",
      "Step 300: val_rewards = 0.062280165619408856 | baseline_reward = 0.7393541260102793\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 301: last loss = 0.62002\n",
      "eval step --\n",
      "\n",
      "Step 301: val_rewards = 0.04320353994895528 | baseline_reward = -0.07636668264342028\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 302: last loss = 0.25356\n",
      "eval step --\n",
      "\n",
      "Step 302: val_rewards = 0.08273648547373054 | baseline_reward = 0.6757802668183599\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 303: last loss = 0.20819\n",
      "eval step --\n",
      "\n",
      "Step 303: val_rewards = 0.5001206130782802 | baseline_reward = 0.614931111202216\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 304: last loss = 0.11946\n",
      "eval step --\n",
      "\n",
      "Step 304: val_rewards = 0.49298301605663414 | baseline_reward = 0.198247059029331\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 305: last loss = 0.63276\n",
      "eval step --\n",
      "\n",
      "Step 305: val_rewards = 0.26722721737034183 | baseline_reward = 0.6413037700199038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 306: last loss = 0.31472\n",
      "eval step --\n",
      "\n",
      "Step 306: val_rewards = 0.04886351675704243 | baseline_reward = 0.15907879776329997\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 307: last loss = 0.60297\n",
      "eval step --\n",
      "\n",
      "Step 307: val_rewards = 0.03552180455532694 | baseline_reward = -0.040765858608859395\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 308: last loss = 0.26275\n",
      "eval step --\n",
      "\n",
      "Step 308: val_rewards = 0.13484150411189771 | baseline_reward = 0.6392639173998975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 309: last loss = 0.26808\n",
      "eval step --\n",
      "\n",
      "Step 309: val_rewards = 0.1309649199801465 | baseline_reward = 0.3670258653332694\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 310: last loss = 0.14567\n",
      "eval step --\n",
      "\n",
      "Step 310: val_rewards = 0.3623452393884735 | baseline_reward = 0.363108076373057\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 311: last loss = -0.01129\n",
      "eval step --\n",
      "\n",
      "Step 311: val_rewards = -0.05849548857148924 | baseline_reward = -0.2102732677682993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 312: last loss = 0.91809\n",
      "eval step --\n",
      "\n",
      "Step 312: val_rewards = 0.49411489832445826 | baseline_reward = 0.9100759627035324\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 313: last loss = 0.35640\n",
      "eval step --\n",
      "\n",
      "Step 313: val_rewards = 0.3040786127678636 | baseline_reward = 0.6934756493586767\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 314: last loss = 0.23605\n",
      "eval step --\n",
      "\n",
      "Step 314: val_rewards = 0.6031650930616019 | baseline_reward = 0.5899313153949148\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 315: last loss = 0.33400\n",
      "eval step --\n",
      "\n",
      "Step 315: val_rewards = 0.5053239527509852 | baseline_reward = 0.9308423841541791\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 316: last loss = 0.27122\n",
      "eval step --\n",
      "\n",
      "Step 316: val_rewards = 0.4209383484493451 | baseline_reward = 0.198247059029331\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 317: last loss = 0.90090\n",
      "eval step --\n",
      "\n",
      "Step 317: val_rewards = -0.059430057922923565 | baseline_reward = -0.13918292868994334\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 318: last loss = 0.16325\n",
      "eval step --\n",
      "\n",
      "Step 318: val_rewards = 0.4474568545809874 | baseline_reward = 0.755995986508099\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 319: last loss = 0.12817\n",
      "eval step --\n",
      "\n",
      "Step 319: val_rewards = -0.003773040259141942 | baseline_reward = 0.4119648023467082\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 320: last loss = -0.10244\n",
      "eval step --\n",
      "\n",
      "Step 320: val_rewards = -0.1344447102249568 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 321: last loss = 0.11611\n",
      "eval step --\n",
      "\n",
      "Step 321: val_rewards = 0.15527663934994382 | baseline_reward = 0.5353544840088765\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 322: last loss = 0.09488\n",
      "eval step --\n",
      "\n",
      "Step 322: val_rewards = 0.09702998976969936 | baseline_reward = 0.30585187174118195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 323: last loss = 0.23143\n",
      "eval step --\n",
      "\n",
      "Step 323: val_rewards = 0.48086686181095767 | baseline_reward = 0.2702599925690532\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 324: last loss = 1.55050\n",
      "eval step --\n",
      "\n",
      "Step 324: val_rewards = -0.07093429723801942 | baseline_reward = -0.13266265820572704\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 325: last loss = 0.15402\n",
      "eval step --\n",
      "\n",
      "Step 325: val_rewards = 0.22044171152642977 | baseline_reward = 0.5208847074263286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 326: last loss = 0.47320\n",
      "eval step --\n",
      "\n",
      "Step 326: val_rewards = 0.10050545679616923 | baseline_reward = -0.2019959841622891\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 327: last loss = 0.07153\n",
      "eval step --\n",
      "\n",
      "Step 327: val_rewards = 0.0394579013685944 | baseline_reward = -0.07733072873150211\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 328: last loss = 0.76705\n",
      "eval step --\n",
      "\n",
      "Step 328: val_rewards = -0.08130717011058655 | baseline_reward = -0.2102732677682993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 329: last loss = 0.57506\n",
      "eval step --\n",
      "\n",
      "Step 329: val_rewards = 0.21299717169509982 | baseline_reward = 0.5250379478864617\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 330: last loss = -0.04573\n",
      "eval step --\n",
      "\n",
      "Step 330: val_rewards = 0.0025718879559038866 | baseline_reward = 0.5661528162848493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 331: last loss = -0.07348\n",
      "eval step --\n",
      "\n",
      "Step 331: val_rewards = 0.7140581347264401 | baseline_reward = 0.6256085574161052\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 332: last loss = 0.49952\n",
      "eval step --\n",
      "\n",
      "Step 332: val_rewards = 0.18505744708373045 | baseline_reward = -0.04283047071071109\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 333: last loss = 0.07455\n",
      "eval step --\n",
      "\n",
      "Step 333: val_rewards = -0.062038394775990095 | baseline_reward = 0.38055901598314096\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 334: last loss = 0.32625\n",
      "eval step --\n",
      "\n",
      "Step 334: val_rewards = 0.08415360770446717 | baseline_reward = 0.0014208169237701382\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 335: last loss = 0.41769\n",
      "eval step --\n",
      "\n",
      "Step 335: val_rewards = 0.15881567360152096 | baseline_reward = -0.0484675220347934\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 336: last loss = 0.21968\n",
      "eval step --\n",
      "\n",
      "Step 336: val_rewards = 0.28025576939634955 | baseline_reward = 0.4060583580745704\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 337: last loss = 0.88606\n",
      "eval step --\n",
      "\n",
      "Step 337: val_rewards = 0.5187199060612057 | baseline_reward = 0.8051362424683901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 338: last loss = 0.15636\n",
      "eval step --\n",
      "\n",
      "Step 338: val_rewards = 0.17471742194075496 | baseline_reward = 0.2881678125668414\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 339: last loss = 0.60175\n",
      "eval step --\n",
      "\n",
      "Step 339: val_rewards = 0.25798375451159067 | baseline_reward = -0.1989330505264255\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 340: last loss = 0.65757\n",
      "eval step --\n",
      "\n",
      "Step 340: val_rewards = 0.7084712462260729 | baseline_reward = 0.5671239306855195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 341: last loss = 0.38913\n",
      "eval step --\n",
      "\n",
      "Step 341: val_rewards = 0.026296187111731692 | baseline_reward = 0.6584406229119629\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 342: last loss = 0.53967\n",
      "eval step --\n",
      "\n",
      "Step 342: val_rewards = 0.2876226971968809 | baseline_reward = 0.03231802212210823\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 343: last loss = 0.29920\n",
      "eval step --\n",
      "\n",
      "Step 343: val_rewards = 0.06090607603722162 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 344: last loss = 0.61026\n",
      "eval step --\n",
      "\n",
      "Step 344: val_rewards = 0.10217240270236475 | baseline_reward = 0.43771144453979627\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 345: last loss = 0.69885\n",
      "eval step --\n",
      "\n",
      "Step 345: val_rewards = 0.057640378962426846 | baseline_reward = 0.6588783630444365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 346: last loss = 0.49757\n",
      "eval step --\n",
      "\n",
      "Step 346: val_rewards = -0.06959518663413769 | baseline_reward = 0.28708945736553004\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 347: last loss = 0.82890\n",
      "eval step --\n",
      "\n",
      "Step 347: val_rewards = 0.7770298247065178 | baseline_reward = 0.7221405546661565\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 348: last loss = 0.05947\n",
      "eval step --\n",
      "\n",
      "Step 348: val_rewards = 0.2929189681797228 | baseline_reward = 0.56619154414774\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 349: last loss = 0.63557\n",
      "eval step --\n",
      "\n",
      "Step 349: val_rewards = 0.1218372450474222 | baseline_reward = -0.03618619235511578\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 350: last loss = 0.72908\n",
      "eval step --\n",
      "\n",
      "Step 350: val_rewards = 0.06424491286251846 | baseline_reward = 0.04985099102552516\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 351: last loss = 0.71529\n",
      "eval step --\n",
      "\n",
      "Step 351: val_rewards = 0.18241419076979773 | baseline_reward = 0.11731911556164225\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 352: last loss = 0.25006\n",
      "eval step --\n",
      "\n",
      "Step 352: val_rewards = -0.07593675786287798 | baseline_reward = 0.3904089531078603\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 353: last loss = 0.32219\n",
      "eval step --\n",
      "\n",
      "Step 353: val_rewards = 0.29837818658826737 | baseline_reward = 0.608802754917565\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 354: last loss = 0.12947\n",
      "eval step --\n",
      "\n",
      "Step 354: val_rewards = 0.1285690816765535 | baseline_reward = 0.6392639173998975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 355: last loss = 0.14902\n",
      "eval step --\n",
      "\n",
      "Step 355: val_rewards = 0.2667399652326413 | baseline_reward = 0.48672117965628475\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 356: last loss = 0.28931\n",
      "eval step --\n",
      "\n",
      "Step 356: val_rewards = 0.23794564888197464 | baseline_reward = 0.13325769088418363\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 357: last loss = 0.11087\n",
      "eval step --\n",
      "\n",
      "Step 357: val_rewards = 0.22987274896672796 | baseline_reward = -0.16229108079929988\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 358: last loss = 0.13594\n",
      "eval step --\n",
      "\n",
      "Step 358: val_rewards = 0.33403020645951365 | baseline_reward = 0.6090966240642443\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 359: last loss = 0.60535\n",
      "eval step --\n",
      "\n",
      "Step 359: val_rewards = 0.0805892663928516 | baseline_reward = 0.2992559300978452\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 360: last loss = -0.13322\n",
      "eval step --\n",
      "\n",
      "Step 360: val_rewards = 0.1078147631900287 | baseline_reward = -0.19844914691930596\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 361: last loss = -0.01850\n",
      "eval step --\n",
      "\n",
      "Step 361: val_rewards = 0.32723218951015853 | baseline_reward = 0.2759578983022423\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 362: last loss = 0.32358\n",
      "eval step --\n",
      "\n",
      "Step 362: val_rewards = 0.46715276901641234 | baseline_reward = 0.3122829893956993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 363: last loss = 0.20797\n",
      "eval step --\n",
      "\n",
      "Step 363: val_rewards = 0.29136530641671965 | baseline_reward = 0.8244034374954186\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 364: last loss = 0.15273\n",
      "eval step --\n",
      "\n",
      "Step 364: val_rewards = 0.8052727339335265 | baseline_reward = 1.1910070511535245\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 365: last loss = 0.44439\n",
      "eval step --\n",
      "\n",
      "Step 365: val_rewards = 0.2601764618591872 | baseline_reward = 0.6901048969803064\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 366: last loss = 0.24090\n",
      "eval step --\n",
      "\n",
      "Step 366: val_rewards = -0.15466891818662665 | baseline_reward = -0.1794920735327776\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 367: last loss = 0.71887\n",
      "eval step --\n",
      "\n",
      "Step 367: val_rewards = 0.04012478168207974 | baseline_reward = 0.3118687685849124\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 368: last loss = 0.86766\n",
      "eval step --\n",
      "\n",
      "Step 368: val_rewards = 0.5236785996942978 | baseline_reward = 0.40042586228915794\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 369: last loss = 0.84414\n",
      "eval step --\n",
      "\n",
      "Step 369: val_rewards = 0.007134324155892143 | baseline_reward = -0.19178819050120557\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 370: last loss = 0.21693\n",
      "eval step --\n",
      "\n",
      "Step 370: val_rewards = 0.8012636325326813 | baseline_reward = 0.7239937256719368\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 371: last loss = 0.00887\n",
      "eval step --\n",
      "\n",
      "Step 371: val_rewards = 0.7715418231909158 | baseline_reward = 0.4444138742060351\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 372: last loss = 0.75759\n",
      "eval step --\n",
      "\n",
      "Step 372: val_rewards = 0.4060118680289419 | baseline_reward = 0.5633453183952102\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 373: last loss = 0.15554\n",
      "eval step --\n",
      "\n",
      "Step 373: val_rewards = 0.004788121457286001 | baseline_reward = 0.15907879776329997\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 374: last loss = 0.12562\n",
      "eval step --\n",
      "\n",
      "Step 374: val_rewards = 0.2325838967357665 | baseline_reward = 0.5483738482333121\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 375: last loss = 0.91670\n",
      "eval step --\n",
      "\n",
      "Step 375: val_rewards = 0.10138542629765128 | baseline_reward = 0.6392639173998975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 376: last loss = 0.09738\n",
      "eval step --\n",
      "\n",
      "Step 376: val_rewards = -0.048743593854032104 | baseline_reward = -0.09608712917273023\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 377: last loss = 0.52775\n",
      "eval step --\n",
      "\n",
      "Step 377: val_rewards = 0.1906224195754073 | baseline_reward = 0.45225390939114807\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 378: last loss = -0.03120\n",
      "eval step --\n",
      "\n",
      "Step 378: val_rewards = -0.001679333603373658 | baseline_reward = 0.6867815855147756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 379: last loss = 0.71311\n",
      "eval step --\n",
      "\n",
      "Step 379: val_rewards = 0.15720715391864631 | baseline_reward = 0.3546388738531832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 380: last loss = 0.06632\n",
      "eval step --\n",
      "\n",
      "Step 380: val_rewards = 0.23882878970080118 | baseline_reward = 0.228423187242766\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 381: last loss = 0.02810\n",
      "eval step --\n",
      "\n",
      "Step 381: val_rewards = 0.12217388016951578 | baseline_reward = 0.3564550550721981\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 382: last loss = 0.60150\n",
      "eval step --\n",
      "\n",
      "Step 382: val_rewards = 0.28155725595955644 | baseline_reward = 0.620925690715889\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 383: last loss = 0.35172\n",
      "eval step --\n",
      "\n",
      "Step 383: val_rewards = 0.08947087237383702 | baseline_reward = -0.27622771429108856\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 384: last loss = 0.06768\n",
      "eval step --\n",
      "\n",
      "Step 384: val_rewards = 0.16576719437154638 | baseline_reward = 0.3390507335185886\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 385: last loss = 0.08469\n",
      "eval step --\n",
      "\n",
      "Step 385: val_rewards = 0.27649322066010484 | baseline_reward = 0.6043944165896826\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 386: last loss = 0.06018\n",
      "eval step --\n",
      "\n",
      "Step 386: val_rewards = 0.16916742624164888 | baseline_reward = 0.6647957189755535\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 387: last loss = 0.24822\n",
      "eval step --\n",
      "\n",
      "Step 387: val_rewards = -0.11635564820324779 | baseline_reward = 0.14527408995758503\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 388: last loss = -0.06422\n",
      "eval step --\n",
      "\n",
      "Step 388: val_rewards = 0.17169490283130867 | baseline_reward = 0.1474706326515393\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 389: last loss = 0.71677\n",
      "eval step --\n",
      "\n",
      "Step 389: val_rewards = -0.06702293353404627 | baseline_reward = 0.2813440077057417\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 390: last loss = 0.46087\n",
      "eval step --\n",
      "\n",
      "Step 390: val_rewards = 0.1410679091121614 | baseline_reward = 0.2969451647615173\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 391: last loss = 0.17879\n",
      "eval step --\n",
      "\n",
      "Step 391: val_rewards = 0.13179325044258636 | baseline_reward = 0.6392639173998975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 392: last loss = 0.04280\n",
      "eval step --\n",
      "\n",
      "Step 392: val_rewards = -0.02468273303829586 | baseline_reward = 0.2881678125668414\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 393: last loss = 0.43795\n",
      "eval step --\n",
      "\n",
      "Step 393: val_rewards = -0.0014787194510930614 | baseline_reward = -0.16229108079929988\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 394: last loss = -0.01626\n",
      "eval step --\n",
      "\n",
      "Step 394: val_rewards = 0.0334710602496859 | baseline_reward = -0.06534074040975456\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 395: last loss = 0.08171\n",
      "eval step --\n",
      "\n",
      "Step 395: val_rewards = 0.20149783019158388 | baseline_reward = 0.7659950764657154\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 396: last loss = 0.15325\n",
      "eval step --\n",
      "\n",
      "Step 396: val_rewards = -0.15111785115911286 | baseline_reward = 0.5581830455091353\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 397: last loss = 0.55191\n",
      "eval step --\n",
      "\n",
      "Step 397: val_rewards = 0.0824514768237229 | baseline_reward = -0.12430381157482846\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 398: last loss = 0.07581\n",
      "eval step --\n",
      "\n",
      "Step 398: val_rewards = 0.3816314905700182 | baseline_reward = 0.8802768070753949\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 399: last loss = 0.67464\n",
      "eval step --\n",
      "\n",
      "Step 399: val_rewards = 0.38951276587329964 | baseline_reward = 0.7881221759723845\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 400: last loss = 0.38936\n",
      "eval step --\n",
      "\n",
      "Step 400: val_rewards = 0.2203695554623433 | baseline_reward = 0.7441093993984506\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 401: last loss = 0.70826\n",
      "eval step --\n",
      "\n",
      "Step 401: val_rewards = 0.4108361531370577 | baseline_reward = 0.7866991214420845\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 402: last loss = 0.08003\n",
      "eval step --\n",
      "\n",
      "Step 402: val_rewards = 0.0076204795300714055 | baseline_reward = -0.2019959841622891\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 403: last loss = 1.33020\n",
      "eval step --\n",
      "\n",
      "Step 403: val_rewards = 0.2484784770962763 | baseline_reward = 0.6220038779555153\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 404: last loss = 0.68942\n",
      "eval step --\n",
      "\n",
      "Step 404: val_rewards = 0.016820113394831825 | baseline_reward = 0.3700797986477456\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 405: last loss = 0.90010\n",
      "eval step --\n",
      "\n",
      "Step 405: val_rewards = 0.14579680467092032 | baseline_reward = 0.15378637634495473\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 406: last loss = 0.44850\n",
      "eval step --\n",
      "\n",
      "Step 406: val_rewards = 0.25247476086937076 | baseline_reward = 0.4225871894435237\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 407: last loss = 0.69095\n",
      "eval step --\n",
      "\n",
      "Step 407: val_rewards = 0.3834655936787217 | baseline_reward = 0.1888250569240263\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 408: last loss = 1.08229\n",
      "eval step --\n",
      "\n",
      "Step 408: val_rewards = 0.12351500867675802 | baseline_reward = -0.16229108079929988\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 409: last loss = 0.24887\n",
      "eval step --\n",
      "\n",
      "Step 409: val_rewards = 0.06235290697133638 | baseline_reward = 0.3727889768350506\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 410: last loss = 0.21491\n",
      "eval step --\n",
      "\n",
      "Step 410: val_rewards = 0.08626072959168042 | baseline_reward = 0.4982005799136331\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 411: last loss = 0.87503\n",
      "eval step --\n",
      "\n",
      "Step 411: val_rewards = 0.10690495474390446 | baseline_reward = 0.24766792956249126\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 412: last loss = 0.19972\n",
      "eval step --\n",
      "\n",
      "Step 412: val_rewards = -0.06445655909218566 | baseline_reward = 0.33314026385318696\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 413: last loss = 0.48045\n",
      "eval step --\n",
      "\n",
      "Step 413: val_rewards = -0.11417730290469223 | baseline_reward = 0.3475072327521015\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 414: last loss = 0.16213\n",
      "eval step --\n",
      "\n",
      "Step 414: val_rewards = -0.07350693044737272 | baseline_reward = 0.4595972833246864\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 415: last loss = 0.20475\n",
      "eval step --\n",
      "\n",
      "Step 415: val_rewards = 0.11328355396696899 | baseline_reward = -0.2125110668214937\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 416: last loss = 0.17907\n",
      "eval step --\n",
      "\n",
      "Step 416: val_rewards = 0.23386706722053033 | baseline_reward = 0.387986503372861\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 417: last loss = 0.20315\n",
      "eval step --\n",
      "\n",
      "Step 417: val_rewards = 0.14735665711347554 | baseline_reward = 0.4347980544246517\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 418: last loss = 0.62232\n",
      "eval step --\n",
      "\n",
      "Step 418: val_rewards = 0.1291350161539755 | baseline_reward = 0.8069688222148205\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 419: last loss = 0.93010\n",
      "eval step --\n",
      "\n",
      "Step 419: val_rewards = 0.3874800373513962 | baseline_reward = 0.58349669716649\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 420: last loss = 0.38357\n",
      "eval step --\n",
      "\n",
      "Step 420: val_rewards = 0.5141718099245975 | baseline_reward = 0.14934381229045454\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 421: last loss = 0.23124\n",
      "eval step --\n",
      "\n",
      "Step 421: val_rewards = 0.17034213947273158 | baseline_reward = -0.003409516596186424\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 422: last loss = 0.31588\n",
      "eval step --\n",
      "\n",
      "Step 422: val_rewards = 0.06048011828883275 | baseline_reward = 0.4117356102694612\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 423: last loss = 0.97749\n",
      "eval step --\n",
      "\n",
      "Step 423: val_rewards = 0.13862239266934262 | baseline_reward = 0.5111717062227966\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 424: last loss = 1.04000\n",
      "eval step --\n",
      "\n",
      "Step 424: val_rewards = 0.10035078034855727 | baseline_reward = -0.03618619235511578\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 425: last loss = 0.99261\n",
      "eval step --\n",
      "\n",
      "Step 425: val_rewards = 0.05614823869212212 | baseline_reward = -0.2255425204676616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 426: last loss = 0.53331\n",
      "eval step --\n",
      "\n",
      "Step 426: val_rewards = 0.1058794884396372 | baseline_reward = -0.13266265820572704\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 427: last loss = 0.67379\n",
      "eval step --\n",
      "\n",
      "Step 427: val_rewards = -0.061944855102560946 | baseline_reward = -0.21418759877625257\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 428: last loss = 0.03431\n",
      "eval step --\n",
      "\n",
      "Step 428: val_rewards = 0.7023720060530622 | baseline_reward = 0.5899313153949148\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 429: last loss = 0.77568\n",
      "eval step --\n",
      "\n",
      "Step 429: val_rewards = -0.03332401997734824 | baseline_reward = 0.2190904192544018\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 430: last loss = 0.32676\n",
      "eval step --\n",
      "\n",
      "Step 430: val_rewards = 0.040668401800024116 | baseline_reward = -0.12004309054607247\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 431: last loss = 0.30505\n",
      "eval step --\n",
      "\n",
      "Step 431: val_rewards = -0.040678027256147414 | baseline_reward = -0.24302001281267305\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 432: last loss = 0.03521\n",
      "eval step --\n",
      "\n",
      "Step 432: val_rewards = 0.1811295814442852 | baseline_reward = 0.2759578983022423\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 433: last loss = 0.64180\n",
      "eval step --\n",
      "\n",
      "Step 433: val_rewards = 0.14892344541285743 | baseline_reward = 0.3727889768350506\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 434: last loss = 0.99112\n",
      "eval step --\n",
      "\n",
      "Step 434: val_rewards = 0.15521343333363785 | baseline_reward = 0.7393541260102793\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 435: last loss = -0.09713\n",
      "eval step --\n",
      "\n",
      "Step 435: val_rewards = 0.23790680869013942 | baseline_reward = 0.3390507335185886\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 436: last loss = 0.15589\n",
      "eval step --\n",
      "\n",
      "Step 436: val_rewards = 0.12171193630014718 | baseline_reward = -0.04421807519264951\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 437: last loss = 0.62977\n",
      "eval step --\n",
      "\n",
      "Step 437: val_rewards = 0.08029886782729964 | baseline_reward = -0.020216636464273522\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 438: last loss = -0.18339\n",
      "eval step --\n",
      "\n",
      "Step 438: val_rewards = 0.9367205368725341 | baseline_reward = 1.0688939544432159\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 439: last loss = 0.83748\n",
      "eval step --\n",
      "\n",
      "Step 439: val_rewards = 0.3626438916383186 | baseline_reward = 0.7775086599707709\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 440: last loss = 0.50857\n",
      "eval step --\n",
      "\n",
      "Step 440: val_rewards = 0.24213780617850048 | baseline_reward = 0.44493492314737587\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 441: last loss = 0.68673\n",
      "eval step --\n",
      "\n",
      "Step 441: val_rewards = 0.03230648676875754 | baseline_reward = 0.19092567740642546\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 442: last loss = -0.01622\n",
      "eval step --\n",
      "\n",
      "Step 442: val_rewards = -0.09308697359634875 | baseline_reward = 0.5111717062227966\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 443: last loss = 0.80988\n",
      "eval step --\n",
      "\n",
      "Step 443: val_rewards = 0.14369648215699732 | baseline_reward = 0.7775086599707709\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 444: last loss = 0.26007\n",
      "eval step --\n",
      "\n",
      "Step 444: val_rewards = 0.15909457026930032 | baseline_reward = 0.5581830455091353\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 445: last loss = 0.19080\n",
      "eval step --\n",
      "\n",
      "Step 445: val_rewards = 0.040145962861977344 | baseline_reward = -0.08881734215636583\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 446: last loss = 0.08128\n",
      "eval step --\n",
      "\n",
      "Step 446: val_rewards = 0.007227372131384769 | baseline_reward = -0.10561581099855756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 447: last loss = 0.29697\n",
      "eval step --\n",
      "\n",
      "Step 447: val_rewards = 0.10340391886025356 | baseline_reward = 0.03229356862306869\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 448: last loss = 0.89709\n",
      "eval step --\n",
      "\n",
      "Step 448: val_rewards = 0.4253382267960512 | baseline_reward = 0.5773945595001121\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 449: last loss = 0.10561\n",
      "eval step --\n",
      "\n",
      "Step 449: val_rewards = 0.23559823875158373 | baseline_reward = -0.03618619235511578\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 450: last loss = 0.12947\n",
      "eval step --\n",
      "\n",
      "Step 450: val_rewards = 0.2340673243732334 | baseline_reward = 0.15617878480221317\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 451: last loss = 0.21991\n",
      "eval step --\n",
      "\n",
      "Step 451: val_rewards = 0.026914847140728726 | baseline_reward = 0.16603698525017901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 452: last loss = 1.12984\n",
      "eval step --\n",
      "\n",
      "Step 452: val_rewards = 0.17633623243620836 | baseline_reward = 0.4680217543380443\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 453: last loss = 0.29494\n",
      "eval step --\n",
      "\n",
      "Step 453: val_rewards = 0.15424786076360153 | baseline_reward = 0.1477246880925813\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 454: last loss = 0.54742\n",
      "eval step --\n",
      "\n",
      "Step 454: val_rewards = 0.1326294162405652 | baseline_reward = 0.008074204506293097\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 455: last loss = 0.90815\n",
      "eval step --\n",
      "\n",
      "Step 455: val_rewards = 0.03239296707744415 | baseline_reward = -0.06875045936192646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 456: last loss = 0.03520\n",
      "eval step --\n",
      "\n",
      "Step 456: val_rewards = 0.22504772694762656 | baseline_reward = 0.5766095402630447\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 457: last loss = 0.12445\n",
      "eval step --\n",
      "\n",
      "Step 457: val_rewards = 0.22559904448969154 | baseline_reward = 0.13954921367937764\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 458: last loss = -0.05133\n",
      "eval step --\n",
      "\n",
      "Step 458: val_rewards = 0.4783779030657049 | baseline_reward = 0.8148157844775424\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 459: last loss = 0.24092\n",
      "eval step --\n",
      "\n",
      "Step 459: val_rewards = 0.1278793579994395 | baseline_reward = -0.13918292868994334\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 460: last loss = 0.06306\n",
      "eval step --\n",
      "\n",
      "Step 460: val_rewards = -0.06713380277539155 | baseline_reward = -0.08023269704425942\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 461: last loss = 0.45630\n",
      "eval step --\n",
      "\n",
      "Step 461: val_rewards = 0.35465411593470225 | baseline_reward = 0.7239937256719368\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 462: last loss = 0.06934\n",
      "eval step --\n",
      "\n",
      "Step 462: val_rewards = 0.12167326285382211 | baseline_reward = 0.6755254114198141\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 463: last loss = 0.51170\n",
      "eval step --\n",
      "\n",
      "Step 463: val_rewards = 0.12231624840238459 | baseline_reward = 0.2823729329137771\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 464: last loss = 0.22744\n",
      "eval step --\n",
      "\n",
      "Step 464: val_rewards = 0.26080565299407665 | baseline_reward = 0.7775086599707709\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 465: last loss = 0.72864\n",
      "eval step --\n",
      "\n",
      "Step 465: val_rewards = 0.005806428626925792 | baseline_reward = -0.14745649769542646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 466: last loss = 0.36346\n",
      "eval step --\n",
      "\n",
      "Step 466: val_rewards = 0.07202623292320978 | baseline_reward = -0.09351335608180286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 467: last loss = 0.13776\n",
      "eval step --\n",
      "\n",
      "Step 467: val_rewards = 0.675387380786295 | baseline_reward = 0.6256085574161052\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 468: last loss = 1.04081\n",
      "eval step --\n",
      "\n",
      "Step 468: val_rewards = -0.014429834523614782 | baseline_reward = 0.7232169146542707\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 469: last loss = 0.91832\n",
      "eval step --\n",
      "\n",
      "Step 469: val_rewards = 0.2547858325114664 | baseline_reward = 0.39943155544143466\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 470: last loss = 0.15441\n",
      "eval step --\n",
      "\n",
      "Step 470: val_rewards = -0.030367970222886925 | baseline_reward = 0.43771144453979627\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 471: last loss = 0.00732\n",
      "eval step --\n",
      "\n",
      "Step 471: val_rewards = -0.04161137099318 | baseline_reward = 0.4119648023467082\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 472: last loss = 0.20153\n",
      "eval step --\n",
      "\n",
      "Step 472: val_rewards = 0.11940541450291006 | baseline_reward = 0.3921332160111007\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 473: last loss = 0.78527\n",
      "eval step --\n",
      "\n",
      "Step 473: val_rewards = -0.06231818503606842 | baseline_reward = 0.5142365399713867\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 474: last loss = 0.17971\n",
      "eval step --\n",
      "\n",
      "Step 474: val_rewards = 0.13228688189468638 | baseline_reward = 0.5968752165536757\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 475: last loss = 0.05587\n",
      "eval step --\n",
      "\n",
      "Step 475: val_rewards = 0.2617205565370033 | baseline_reward = -0.02119186696164961\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 476: last loss = 0.99299\n",
      "eval step --\n",
      "\n",
      "Step 476: val_rewards = 0.19168663613690184 | baseline_reward = -0.04421807519264951\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 477: last loss = 0.14530\n",
      "eval step --\n",
      "\n",
      "Step 477: val_rewards = 0.1288609207468132 | baseline_reward = 0.6662998548169972\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 478: last loss = 0.71051\n",
      "eval step --\n",
      "\n",
      "Step 478: val_rewards = 0.26768081088316414 | baseline_reward = 0.56619154414774\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 479: last loss = 0.11285\n",
      "eval step --\n",
      "\n",
      "Step 479: val_rewards = 0.006614141447146 | baseline_reward = -0.03618619235511578\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 480: last loss = 1.25797\n",
      "eval step --\n",
      "\n",
      "Step 480: val_rewards = -0.13712830566923662 | baseline_reward = -0.011238943939511286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 481: last loss = 0.86128\n",
      "eval step --\n",
      "\n",
      "Step 481: val_rewards = 0.17172887329579142 | baseline_reward = -0.05421979556190041\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 482: last loss = 0.75306\n",
      "eval step --\n",
      "\n",
      "Step 482: val_rewards = 0.33604025619375366 | baseline_reward = 0.17156898417402686\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 483: last loss = 0.26731\n",
      "eval step --\n",
      "\n",
      "Step 483: val_rewards = 0.11422932127986025 | baseline_reward = -0.031455106902401365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 484: last loss = 0.09403\n",
      "eval step --\n",
      "\n",
      "Step 484: val_rewards = 0.1577505858114477 | baseline_reward = 0.7277122264121534\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 485: last loss = 0.30400\n",
      "eval step --\n",
      "\n",
      "Step 485: val_rewards = 0.33164432528003424 | baseline_reward = 0.4534508177953611\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 486: last loss = 0.59739\n",
      "eval step --\n",
      "\n",
      "Step 486: val_rewards = 0.03215495189115266 | baseline_reward = -0.1066508569717053\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 487: last loss = 0.50911\n",
      "eval step --\n",
      "\n",
      "Step 487: val_rewards = 0.5024629525904144 | baseline_reward = 0.7669808496689923\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 488: last loss = 0.71525\n",
      "eval step --\n",
      "\n",
      "Step 488: val_rewards = 0.30816497951694855 | baseline_reward = 0.5048208808647582\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 489: last loss = -0.01032\n",
      "eval step --\n",
      "\n",
      "Step 489: val_rewards = -0.09375328415786612 | baseline_reward = -0.08558860669238108\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 490: last loss = 0.21483\n",
      "eval step --\n",
      "\n",
      "Step 490: val_rewards = -0.04470576242493677 | baseline_reward = -0.07636668264342028\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 491: last loss = 0.31078\n",
      "eval step --\n",
      "\n",
      "Step 491: val_rewards = 0.4025253852076105 | baseline_reward = 0.08159079228309284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 492: last loss = 0.39706\n",
      "eval step --\n",
      "\n",
      "Step 492: val_rewards = 0.2396230438269754 | baseline_reward = 0.7812418817917319\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 493: last loss = 0.68376\n",
      "eval step --\n",
      "\n",
      "Step 493: val_rewards = 0.29727431901294293 | baseline_reward = 0.15907879776329997\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 494: last loss = 0.11892\n",
      "eval step --\n",
      "\n",
      "Step 494: val_rewards = 0.27576665413660545 | baseline_reward = 0.22777368070532983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 495: last loss = 0.97393\n",
      "eval step --\n",
      "\n",
      "Step 495: val_rewards = 0.18914843507377013 | baseline_reward = 0.07414195517992758\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 496: last loss = 0.61816\n",
      "eval step --\n",
      "\n",
      "Step 496: val_rewards = -0.019327780208812324 | baseline_reward = 0.11261059360014673\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 497: last loss = 0.76661\n",
      "eval step --\n",
      "\n",
      "Step 497: val_rewards = 0.30356080724648016 | baseline_reward = 0.2738009290954773\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 498: last loss = 0.74418\n",
      "eval step --\n",
      "\n",
      "Step 498: val_rewards = 0.02350396901800205 | baseline_reward = -0.05421979556190041\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 499: last loss = 0.16658\n",
      "eval step --\n",
      "\n",
      "Step 499: val_rewards = 0.18169240112323146 | baseline_reward = 0.4225871894435237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/21 14:08:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = f\"v5_training_{tid}\") as run:\n",
    "    params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"train_step\": train_step,\n",
    "            \"eval_step\": eval_step,\n",
    "            \"metric_function\": 'sharpe',\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \n",
    "            \"symbol_universe\" : symbol_universe,\n",
    "            \"feature_set\" : feature_set,\n",
    "            \"d_model\" : d_model,\n",
    "            \"nheads\" : nheads,\n",
    "            \"num_transformer_layers\" : num_transformer_layers,\n",
    "\n",
    "            \"episode_duration\" : 12,    \n",
    "            \"holding_period\" : 1,\n",
    "            \"train_test_split\" : 0.8,\n",
    "            \"symbol_universe\" : list(symbol_universe),\n",
    "            \"feature_set\" : feature_set,\n",
    "\n",
    "            \"training_data_path\" : data_path\n",
    "\n",
    "        }\n",
    "    # Log training parameters.\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    portfolio_constructor = PortfolioConstructor(\n",
    "        device = device,\n",
    "        symbol_universe= params['symbol_universe'],\n",
    "        num_features= len(params['feature_set']),\n",
    "        d_model = params['d_model'],\n",
    "        nheads = params['nheads'],\n",
    "        num_transformer_layers = params['num_transformer_layers'],\n",
    "    )\n",
    "\n",
    "    market_env = MarketEnvironment(\n",
    "        device = device,\n",
    "        data_path = data_path,\n",
    "        holding_period = params['holding_period'],\n",
    "        episode_duration = params['episode_duration'],\n",
    "        train_test_split = params['train_test_split'],\n",
    "        symbol_universe = params['symbol_universe'],\n",
    "        feature_set = params['feature_set']\n",
    "        )\n",
    "\n",
    "    portfolio_constructor.cuda()\n",
    "    portfolio_constructor.train()\n",
    "    market_env.reset(mode = \"train\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(portfolio_constructor.parameters(), lr = learning_rate)\n",
    "\n",
    "    max_reward = -1\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        is_end = False\n",
    "        returns = []\n",
    "        tran_costs = []\n",
    "        nlls = []\n",
    "        all_allocations = []\n",
    "\n",
    "        market_env.reset(mode = \"train\", transaction_cost= 1e-7)\n",
    "        state = market_env.get_state()\n",
    "        \n",
    "        while not is_end:\n",
    "            symbol_idx, allocations = portfolio_constructor(state)\n",
    "            state, return_, _, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "            all_allocations.append(allocations)\n",
    "            returns.append(return_)\n",
    "            tran_costs.append(tran_cost)\n",
    "            nlls.append(torch.log(allocations.abs()/2 + 1e-9))\n",
    "            \n",
    "        sharp_ratio = sharp_ratio_(returns, tran_costs)\n",
    "\n",
    "        loss = sharp_ratio\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        if (episode + 1) % train_step == 0:\n",
    "\n",
    "                    print(\"-------------------------------------\")\n",
    "                    print(\"training model --\")\n",
    "                    print('Step {}: last loss = {:.5f}\\r'.format(episode, loss), end='')\n",
    "                    print()\n",
    "                    mlflow.log_metric(\"train loss\", f\"{loss:2f}\", step=episode)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = 0\n",
    "                    \n",
    "        if (episode + 1) % eval_step == 0:\n",
    "            print(\"eval step --\")\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                portfolio_constructor.eval()\n",
    "                reward_val, baseline_val, portfolio_constructor = evaluate(portfolio_constructor, market_env)\n",
    "                portfolio_constructor.train()\n",
    "\n",
    "                print('Step {}: val_rewards = {} | baseline_reward = {}'.format(episode, reward_val, baseline_val))\n",
    "                mlflow.log_metric(\"eval_sharpe\", f\"{reward_val:2f}\", step=episode)\n",
    "                mlflow.log_metric(\"baseline_sharpe\", f\"{baseline_val:2f}\", step=episode)\n",
    "\n",
    "                if max_reward < reward_val:\n",
    "                    max_reward = reward_val\n",
    "\n",
    "                    print(\"*** found better model ***\")\n",
    "                print()\n",
    "    mlflow.pytorch.log_model(portfolio_constructor, f\"portfolio_constructor_{tid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_allocations[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/11 10:47:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'portfolio-constructor-v3'.\n",
      "Created version '1' of model 'portfolio-constructor-v3'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7fc41191ac70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.pytorch.log_model(\n",
    "        pytorch_model=portfolio_constructor,\n",
    "        artifact_path = \"portfolio_constructor_{tid}\",\n",
    "        # input_example = market_env.get_random_state(),\n",
    "        registered_model_name=\"portfolio-constructor-v3\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
