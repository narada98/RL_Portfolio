{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PortfolioConstructor import PortfolioConstructor\n",
    "from ExchnageEnv import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda') \n",
    "    torch.get_default_device()\n",
    "    device = 'cuda'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/naradaw/dev/Charles_Schwab/data/symbol_universe/snp_unique_100_2019\", \"rb\") as fp:\n",
    "    symbol_universe = pickle.load(fp)\n",
    "    \n",
    "symbol_universe = symbol_universe[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_path = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v1/2024_10_31/feature_set_2024_10_31_11_18.pkl\"\n",
    "\n",
    "with open(feature_set_path, 'rb') as f:\n",
    "    feature_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v1/2024_10_31/dataset_sqs_2024_10_31_11_18.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/naradaw/dev/Charles_Schwab/code/RnD/v3/mlflow_experiments/168050635922118841', creation_time=1730954894954, experiment_id='168050635922118841', last_update_time=1730954894954, lifecycle_stage='active', name='/portfolio-contructor-v3', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_tracking_uri = 'file:/home/naradaw/dev/Charles_Schwab/code/RnD/v3/mlflow_experiments'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "experiment_name = \"/portfolio-contructor-v3\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "eval_step = 8\n",
    "train_step = 8\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "symbol_universe= symbol_universe\n",
    "num_features= len(feature_set)\n",
    "d_model = 88\n",
    "nheads = 2\n",
    "num_transformer_layers = 2\n",
    "\n",
    "episode_duration= 12   \n",
    "holding_period = 1\n",
    "train_test_split= 0.8\n",
    "symbol_universe = symbol_universe\n",
    "feature_set= feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_universe = random.choices(symbol_universe, k = 20)\n",
    "# symbol_universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "sharpe ratio measures the excess return of the portfolio over the \n",
    "volatility of it -> risk adjusted performance\n",
    "'''\n",
    "\n",
    "def sharp_ratio_(rewards, tran_costs):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env):\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    tran_costs = []\n",
    "    \n",
    "    env.reset(mode = \"test\")\n",
    "    state = env.get_state()\n",
    "\n",
    "    print(\"\")\n",
    "    while not is_end:\n",
    "        _, allocations = model(state)\n",
    "        state, reward, is_end, tran_cost = env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "\n",
    "    return sharp_ratio, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# mlflow.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "training model --\n",
      "Step 7: last loss = -0.02440\n",
      "eval step --\n",
      "\n",
      "Step 7: val_rewards = 0.43522491002873515\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 15: last loss = -0.32245\n",
      "eval step --\n",
      "\n",
      "Step 15: val_rewards = 0.6998399908446481\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 23: last loss = -0.23476\n",
      "eval step --\n",
      "\n",
      "Step 23: val_rewards = 0.705493728692874\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 31: last loss = -0.92923\n",
      "eval step --\n",
      "\n",
      "Step 31: val_rewards = 0.7042781802077541\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 39: last loss = -0.03586\n",
      "eval step --\n",
      "\n",
      "Step 39: val_rewards = 0.7049416497022167\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 47: last loss = -0.32728\n",
      "eval step --\n",
      "\n",
      "Step 47: val_rewards = 0.6978848293674798\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 55: last loss = 0.09547\n",
      "eval step --\n",
      "\n",
      "Step 55: val_rewards = 0.6660863542024725\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 63: last loss = -0.14206\n",
      "eval step --\n",
      "\n",
      "Step 63: val_rewards = 0.6073930181541575\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 71: last loss = -0.25992\n",
      "eval step --\n",
      "\n",
      "Step 71: val_rewards = 0.6843842019459984\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 79: last loss = -0.96335\n",
      "eval step --\n",
      "\n",
      "Step 79: val_rewards = 0.7056524498017251\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 87: last loss = -0.37371\n",
      "eval step --\n",
      "\n",
      "Step 87: val_rewards = 0.7049296259749428\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 95: last loss = -0.10051\n",
      "eval step --\n",
      "\n",
      "Step 95: val_rewards = 0.7041411184381698\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 103: last loss = -0.24681\n",
      "eval step --\n",
      "\n",
      "Step 103: val_rewards = 0.7045001368123556\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 111: last loss = -0.97671\n",
      "eval step --\n",
      "\n",
      "Step 111: val_rewards = 0.7044926845667601\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 119: last loss = 0.00696\n",
      "eval step --\n",
      "\n",
      "Step 119: val_rewards = 0.7048386530305968\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 127: last loss = 0.04515\n",
      "eval step --\n",
      "\n",
      "Step 127: val_rewards = 0.706061842261871\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 135: last loss = -0.20318\n",
      "eval step --\n",
      "\n",
      "Step 135: val_rewards = 0.701088386359561\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 143: last loss = -1.16687\n",
      "eval step --\n",
      "\n",
      "Step 143: val_rewards = 0.6750399094047145\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 151: last loss = -1.30836\n",
      "eval step --\n",
      "\n",
      "Step 151: val_rewards = 0.6820589114606807\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 159: last loss = -0.00434\n",
      "eval step --\n",
      "\n",
      "Step 159: val_rewards = 0.6975777015346684\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 167: last loss = -0.04886\n",
      "eval step --\n",
      "\n",
      "Step 167: val_rewards = 0.6917338507596189\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 175: last loss = -0.21579\n",
      "eval step --\n",
      "\n",
      "Step 175: val_rewards = 0.6941165234995252\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 183: last loss = -1.22788\n",
      "eval step --\n",
      "\n",
      "Step 183: val_rewards = 0.6938275979114614\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 191: last loss = -0.31524\n",
      "eval step --\n",
      "\n",
      "Step 191: val_rewards = 0.697413921454224\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 199: last loss = -0.03775\n",
      "eval step --\n",
      "\n",
      "Step 199: val_rewards = 0.6830056626655462\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 207: last loss = -0.85425\n",
      "eval step --\n",
      "\n",
      "Step 207: val_rewards = 0.6858046270649663\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 215: last loss = -0.02037\n",
      "eval step --\n",
      "\n",
      "Step 215: val_rewards = 0.5733788745634426\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 223: last loss = 0.03438\n",
      "eval step --\n",
      "\n",
      "Step 223: val_rewards = 0.6848724066544899\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 231: last loss = 0.02914\n",
      "eval step --\n",
      "\n",
      "Step 231: val_rewards = 0.6819176704701673\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 239: last loss = -1.19021\n",
      "eval step --\n",
      "\n",
      "Step 239: val_rewards = 0.6768723434767002\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 247: last loss = -0.77786\n",
      "eval step --\n",
      "\n",
      "Step 247: val_rewards = 0.674513530750963\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 255: last loss = -0.39727\n",
      "eval step --\n",
      "\n",
      "Step 255: val_rewards = 0.6821670889193971\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 263: last loss = -1.13342\n",
      "eval step --\n",
      "\n",
      "Step 263: val_rewards = 0.6770978420700414\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 271: last loss = -0.19046\n",
      "eval step --\n",
      "\n",
      "Step 271: val_rewards = 0.6721531361150528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 279: last loss = -0.00652\n",
      "eval step --\n",
      "\n",
      "Step 279: val_rewards = 0.6930748886533411\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 287: last loss = -0.25042\n",
      "eval step --\n",
      "\n",
      "Step 287: val_rewards = 0.6961940414803793\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 295: last loss = -0.98942\n",
      "eval step --\n",
      "\n",
      "Step 295: val_rewards = 0.6688806382643443\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 303: last loss = -0.19964\n",
      "eval step --\n",
      "\n",
      "Step 303: val_rewards = 0.6527950638738104\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 311: last loss = -0.09170\n",
      "eval step --\n",
      "\n",
      "Step 311: val_rewards = 0.6658599318677881\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 319: last loss = -0.83067\n",
      "eval step --\n",
      "\n",
      "Step 319: val_rewards = 0.6811975488565738\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 327: last loss = -0.21009\n",
      "eval step --\n",
      "\n",
      "Step 327: val_rewards = 0.7044872410922131\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 335: last loss = -0.16344\n",
      "eval step --\n",
      "\n",
      "Step 335: val_rewards = 0.7049880220718227\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 343: last loss = -0.99292\n",
      "eval step --\n",
      "\n",
      "Step 343: val_rewards = 0.7021376213896149\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 351: last loss = -0.12025\n",
      "eval step --\n",
      "\n",
      "Step 351: val_rewards = 0.6710671071913926\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 359: last loss = -1.72747\n",
      "eval step --\n",
      "\n",
      "Step 359: val_rewards = 0.659945374953848\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 367: last loss = -0.10157\n",
      "eval step --\n",
      "\n",
      "Step 367: val_rewards = 0.6643899195192989\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 375: last loss = -0.78554\n",
      "eval step --\n",
      "\n",
      "Step 375: val_rewards = 0.6761882820572547\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 383: last loss = -0.86072\n",
      "eval step --\n",
      "\n",
      "Step 383: val_rewards = 0.691739673022945\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 391: last loss = -0.29850\n",
      "eval step --\n",
      "\n",
      "Step 391: val_rewards = 0.6904695636219483\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 399: last loss = -0.16815\n",
      "eval step --\n",
      "\n",
      "Step 399: val_rewards = 0.7011785802445285\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 407: last loss = -1.25049\n",
      "eval step --\n",
      "\n",
      "Step 407: val_rewards = 0.7019475265063604\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 415: last loss = -0.38091\n",
      "eval step --\n",
      "\n",
      "Step 415: val_rewards = 0.70301449815676\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 423: last loss = -0.27212\n",
      "eval step --\n",
      "\n",
      "Step 423: val_rewards = 0.7091740396563898\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 431: last loss = -0.33568\n",
      "eval step --\n",
      "\n",
      "Step 431: val_rewards = 0.7052955043477096\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 439: last loss = -0.24983\n",
      "eval step --\n",
      "\n",
      "Step 439: val_rewards = 0.7016363538512583\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 447: last loss = -0.24706\n",
      "eval step --\n",
      "\n",
      "Step 447: val_rewards = 0.698584843159857\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 455: last loss = -0.69676\n",
      "eval step --\n",
      "\n",
      "Step 455: val_rewards = 0.6944533945861243\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 463: last loss = -0.34315\n",
      "eval step --\n",
      "\n",
      "Step 463: val_rewards = 0.6901043606936897\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 471: last loss = -0.25327\n",
      "eval step --\n",
      "\n",
      "Step 471: val_rewards = 0.6815691694348256\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 479: last loss = -0.43253\n",
      "eval step --\n",
      "\n",
      "Step 479: val_rewards = 0.682451315102051\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 487: last loss = -1.20193\n",
      "eval step --\n",
      "\n",
      "Step 487: val_rewards = 0.688525231687521\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 495: last loss = -0.81104\n",
      "eval step --\n",
      "\n",
      "Step 495: val_rewards = 0.6891601843440258\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 503: last loss = -0.51846\n",
      "eval step --\n",
      "\n",
      "Step 503: val_rewards = 0.6918867046244918\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 511: last loss = -0.94416\n",
      "eval step --\n",
      "\n",
      "Step 511: val_rewards = 0.6944607850936311\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 519: last loss = -0.42492\n",
      "eval step --\n",
      "\n",
      "Step 519: val_rewards = 0.6962252460389873\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 527: last loss = -0.85520\n",
      "eval step --\n",
      "\n",
      "Step 527: val_rewards = 0.6950935198213133\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 535: last loss = -0.23103\n",
      "eval step --\n",
      "\n",
      "Step 535: val_rewards = 0.6932716414694925\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 543: last loss = -0.54770\n",
      "eval step --\n",
      "\n",
      "Step 543: val_rewards = 0.692258095582262\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 551: last loss = -1.13400\n",
      "eval step --\n",
      "\n",
      "Step 551: val_rewards = 0.6878484669376956\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 559: last loss = -0.99277\n",
      "eval step --\n",
      "\n",
      "Step 559: val_rewards = 0.6829563504220184\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 567: last loss = -1.47675\n",
      "eval step --\n",
      "\n",
      "Step 567: val_rewards = 0.6806474709902567\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 575: last loss = -0.54261\n",
      "eval step --\n",
      "\n",
      "Step 575: val_rewards = 0.6729949769922658\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 583: last loss = -0.57087\n",
      "eval step --\n",
      "\n",
      "Step 583: val_rewards = 0.6700300299776554\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 591: last loss = -1.44495\n",
      "eval step --\n",
      "\n",
      "Step 591: val_rewards = 0.6678689447255166\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 599: last loss = -0.69302\n",
      "eval step --\n",
      "\n",
      "Step 599: val_rewards = 0.6670994831144537\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 607: last loss = -0.52754\n",
      "eval step --\n",
      "\n",
      "Step 607: val_rewards = 0.6748876856178934\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 615: last loss = -1.42222\n",
      "eval step --\n",
      "\n",
      "Step 615: val_rewards = 0.6572005059751822\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 623: last loss = -1.19987\n",
      "eval step --\n",
      "\n",
      "Step 623: val_rewards = 0.6538483368063335\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 631: last loss = -0.75692\n",
      "eval step --\n",
      "\n",
      "Step 631: val_rewards = 0.6552300528577688\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 639: last loss = -1.10901\n",
      "eval step --\n",
      "\n",
      "Step 639: val_rewards = 0.7175408359798\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 647: last loss = -0.59952\n",
      "eval step --\n",
      "\n",
      "Step 647: val_rewards = 0.7131000192990119\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 655: last loss = -1.33186\n",
      "eval step --\n",
      "\n",
      "Step 655: val_rewards = 0.7073635967055241\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 663: last loss = -1.37174\n",
      "eval step --\n",
      "\n",
      "Step 663: val_rewards = 0.6990162656526944\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 671: last loss = -0.50890\n",
      "eval step --\n",
      "\n",
      "Step 671: val_rewards = 0.7089368468000637\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 679: last loss = -1.27644\n",
      "eval step --\n",
      "\n",
      "Step 679: val_rewards = 0.7249881948245717\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 687: last loss = -0.77996\n",
      "eval step --\n",
      "\n",
      "Step 687: val_rewards = 0.7395480435357471\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 695: last loss = -0.60374\n",
      "eval step --\n",
      "\n",
      "Step 695: val_rewards = 0.7450513618319355\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 703: last loss = -0.81327\n",
      "eval step --\n",
      "\n",
      "Step 703: val_rewards = 0.7460156609554447\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 711: last loss = -0.34440\n",
      "eval step --\n",
      "\n",
      "Step 711: val_rewards = 0.7435837284118756\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 719: last loss = -0.51535\n",
      "eval step --\n",
      "\n",
      "Step 719: val_rewards = 0.7386835598362892\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 727: last loss = -0.43407\n",
      "eval step --\n",
      "\n",
      "Step 727: val_rewards = 0.7334572282129787\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 735: last loss = -0.84608\n",
      "eval step --\n",
      "\n",
      "Step 735: val_rewards = 0.7346276713537507\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 743: last loss = -0.81181\n",
      "eval step --\n",
      "\n",
      "Step 743: val_rewards = 0.7417518383136834\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 751: last loss = -1.63192\n",
      "eval step --\n",
      "\n",
      "Step 751: val_rewards = 0.743141543994214\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 759: last loss = -0.19479\n",
      "eval step --\n",
      "\n",
      "Step 759: val_rewards = 0.7349770586807434\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 767: last loss = -0.86949\n",
      "eval step --\n",
      "\n",
      "Step 767: val_rewards = 0.7366009573113255\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 775: last loss = -0.58004\n",
      "eval step --\n",
      "\n",
      "Step 775: val_rewards = 0.7419956795585039\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 783: last loss = -0.87494\n",
      "eval step --\n",
      "\n",
      "Step 783: val_rewards = 0.7430542933033386\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 791: last loss = -2.06091\n",
      "eval step --\n",
      "\n",
      "Step 791: val_rewards = 0.7365223770284471\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 799: last loss = -0.41726\n",
      "eval step --\n",
      "\n",
      "Step 799: val_rewards = 0.7233446391148529\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 807: last loss = -0.37832\n",
      "eval step --\n",
      "\n",
      "Step 807: val_rewards = 0.7077808250059008\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 815: last loss = -0.44511\n",
      "eval step --\n",
      "\n",
      "Step 815: val_rewards = 0.6963376557362487\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 823: last loss = -0.97211\n",
      "eval step --\n",
      "\n",
      "Step 823: val_rewards = 0.7042882282787095\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 831: last loss = -0.39583\n",
      "eval step --\n",
      "\n",
      "Step 831: val_rewards = 0.7179633882417664\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 839: last loss = -0.40258\n",
      "eval step --\n",
      "\n",
      "Step 839: val_rewards = 0.7249516777232157\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 847: last loss = -0.54017\n",
      "eval step --\n",
      "\n",
      "Step 847: val_rewards = 0.7305678529760735\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 855: last loss = -1.30817\n",
      "eval step --\n",
      "\n",
      "Step 855: val_rewards = 0.7392910689608346\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 863: last loss = -1.32861\n",
      "eval step --\n",
      "\n",
      "Step 863: val_rewards = 0.7378711791985426\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 871: last loss = -0.61403\n",
      "eval step --\n",
      "\n",
      "Step 871: val_rewards = 0.7374105361007554\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 879: last loss = -0.60512\n",
      "eval step --\n",
      "\n",
      "Step 879: val_rewards = 0.7336910513392076\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 887: last loss = -0.78400\n",
      "eval step --\n",
      "\n",
      "Step 887: val_rewards = 0.7312508527233648\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 895: last loss = -0.85477\n",
      "eval step --\n",
      "\n",
      "Step 895: val_rewards = 0.729671767723359\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 903: last loss = -1.15419\n",
      "eval step --\n",
      "\n",
      "Step 903: val_rewards = 0.7282315666126258\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 911: last loss = -0.34616\n",
      "eval step --\n",
      "\n",
      "Step 911: val_rewards = 0.7245502951442844\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 919: last loss = -0.68549\n",
      "eval step --\n",
      "\n",
      "Step 919: val_rewards = 0.7240498312451769\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 927: last loss = -0.12849\n",
      "eval step --\n",
      "\n",
      "Step 927: val_rewards = 0.7249556883110309\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 935: last loss = -1.07277\n",
      "eval step --\n",
      "\n",
      "Step 935: val_rewards = 0.7266194835694119\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 943: last loss = -0.82835\n",
      "eval step --\n",
      "\n",
      "Step 943: val_rewards = 0.7286839673927619\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 951: last loss = -1.27662\n",
      "eval step --\n",
      "\n",
      "Step 951: val_rewards = 0.7268922681932618\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 959: last loss = -0.41936\n",
      "eval step --\n",
      "\n",
      "Step 959: val_rewards = 0.7222244591432545\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 967: last loss = -1.48403\n",
      "eval step --\n",
      "\n",
      "Step 967: val_rewards = 0.7199963809690094\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 975: last loss = -0.85639\n",
      "eval step --\n",
      "\n",
      "Step 975: val_rewards = 0.7213365748723303\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 983: last loss = -0.96206\n",
      "eval step --\n",
      "\n",
      "Step 983: val_rewards = 0.7242158233377539\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 991: last loss = -1.37675\n",
      "eval step --\n",
      "\n",
      "Step 991: val_rewards = 0.7315500461924355\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 999: last loss = -1.20222\n",
      "eval step --\n",
      "\n",
      "Step 999: val_rewards = 0.7385109831324375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/08 16:57:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "tid = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "with mlflow.start_run(run_name = f\"v3_training_{tid}\") as run:\n",
    "    params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"train_step\": train_step,\n",
    "            \"eval_step\": eval_step,\n",
    "            \"metric_function\": 'sharpe',\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \n",
    "            \"symbol_universe\" : symbol_universe,\n",
    "            \"feature_set\" : feature_set,\n",
    "            \"d_model\" : d_model,\n",
    "            \"nheads\" : nheads,\n",
    "            \"num_transformer_layers\" : num_transformer_layers,\n",
    "\n",
    "            \"episode_duration\" : 12,    \n",
    "            \"holding_period\" : 1,\n",
    "            \"train_test_split\" : 0.8,\n",
    "            \"symbol_universe\" : symbol_universe,\n",
    "            \"feature_set\" : feature_set,\n",
    "\n",
    "        }\n",
    "    # Log training parameters.\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    portfolio_constructor = PortfolioConstructor(\n",
    "        device = device,\n",
    "        symbol_universe= params['symbol_universe'],\n",
    "        num_features= len(params['feature_set']),\n",
    "        d_model = params['d_model'],\n",
    "        nheads = params['nheads'],\n",
    "        num_transformer_layers = params['num_transformer_layers'],\n",
    "    )\n",
    "\n",
    "    market_env = MarketEnvironment(\n",
    "        device = device,\n",
    "        data_path = data_path,\n",
    "        holding_period = params['holding_period'],\n",
    "        episode_duration = params['episode_duration'],\n",
    "        train_test_split = params['train_test_split'],\n",
    "        symbol_universe = params['symbol_universe'],\n",
    "        feature_set = params['feature_set']\n",
    "        )\n",
    "\n",
    "    portfolio_constructor.cuda()\n",
    "    portfolio_constructor.train()\n",
    "    market_env.reset(mode = \"train\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(portfolio_constructor.parameters(), lr = learning_rate)\n",
    "\n",
    "    max_reward = -1\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        is_end = False\n",
    "        returns = []\n",
    "        tran_costs = []\n",
    "        nlls = []\n",
    "        all_allocations = []\n",
    "\n",
    "        market_env.reset(mode = \"train\", transaction_cost= 1e-7)\n",
    "        state = market_env.get_state()\n",
    "        \n",
    "        while not is_end:\n",
    "            symbol_idx, allocations = portfolio_constructor(state)\n",
    "            state, return_, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "            all_allocations.append(allocations)\n",
    "            returns.append(return_)\n",
    "            tran_costs.append(tran_cost)\n",
    "\n",
    "        sharp_ratio = sharp_ratio_(returns, tran_costs)\n",
    "\n",
    "        loss = -sharp_ratio\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        if (episode + 1) % train_step == 0:\n",
    "\n",
    "                    print(\"-------------------------------------\")\n",
    "                    print(\"training model --\")\n",
    "                    print('Step {}: last loss = {:.5f}\\r'.format(episode, loss), end='')\n",
    "                    print()\n",
    "                    mlflow.log_metric(\"train loss\", f\"{loss:2f}\", step=episode)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = 0\n",
    "                    \n",
    "        if (episode + 1) % eval_step == 0:\n",
    "            print(\"eval step --\")\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                portfolio_constructor.eval()\n",
    "                reward_val, portfolio_constructor = evaluate(portfolio_constructor, market_env)\n",
    "                portfolio_constructor.train()\n",
    "\n",
    "                print('Step {}: val_rewards = {}'.format(episode, reward_val))\n",
    "                mlflow.log_metric(\"eval_sharpe\", f\"{reward_val:2f}\", step=episode)\n",
    "\n",
    "                if max_reward < reward_val:\n",
    "                    max_reward = reward_val\n",
    "\n",
    "                    print(\"*** found better model ***\")\n",
    "                print()\n",
    "    mlflow.pytorch.log_model(portfolio_constructor, f\"portfolio_constructor_{tid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0314, 0.0000, 0.0000, 0.0000, 0.0843, 0.0000, 0.0000, 0.0000, 0.0730,\n",
       "         0.0456, 0.0405, 0.0000, 0.1077, 0.0000, 0.0417, 0.4126, 0.0000, 0.0589,\n",
       "         0.1043, 0.0000], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0901, 0.0000, 0.0000, 0.0217, 0.0453,\n",
       "         0.0299, 0.0235, 0.0000, 0.1528, 0.0000, 0.0285, 0.4658, 0.0000, 0.0526,\n",
       "         0.0898, 0.0000], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0227, 0.0627, 0.0000, 0.0000, 0.0106, 0.0101,\n",
       "         0.0083, 0.0000, 0.0000, 0.6669, 0.0000, 0.0137, 0.1019, 0.0000, 0.0574,\n",
       "         0.0458, 0.0000], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0359, 0.0310, 0.0000, 0.0000, 0.0163, 0.0103,\n",
       "         0.0111, 0.0000, 0.0000, 0.6846, 0.0000, 0.0198, 0.0984, 0.0000, 0.0567,\n",
       "         0.0357, 0.0000], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4465e-03, 3.2114e-04, 0.0000e+00,\n",
       "         2.4732e-04, 4.2657e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.0522e-01, 4.8799e-01, 2.8329e-04, 2.6683e-03, 0.0000e+00, 6.6324e-04,\n",
       "         7.4217e-04, 0.0000e+00], device='cuda:0', grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_allocations[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
