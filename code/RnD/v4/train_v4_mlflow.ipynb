{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PortfolioConstructor import PortfolioConstructor\n",
    "from ExchnageEnv import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda') \n",
    "    torch.get_default_device()\n",
    "    device = 'cuda'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_loc = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v2/2024_11_19/2024_11_19_09_21\"\n",
    "data_path = f\"{data_base_loc}/dataset_sqs.pkl\"\n",
    "feature_set_path = f\"{data_base_loc}/feature_set.pkl\"\n",
    "symbol_universe_path = f\"{data_base_loc}/symbol_universe.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AJG', 'MA', 'NVDA', 'NRG', 'NOC', 'NCLH', 'MU', 'MRO', 'MMM', 'MLM',\n",
       "       'MDLZ', 'MCK', 'LYB', 'ALB', 'LRCX', 'KR', 'KO', 'KMX', 'KMB', 'KHC',\n",
       "       'JPM', 'JNJ', 'JKHY', 'JCI', 'NWS', 'OMC', 'ORLY', 'PNC', 'WEC', 'VZ',\n",
       "       'VTR', 'VRSN', 'UAL', 'TTWO', 'TSN', 'TAP', 'SWKS', 'SHW', 'SBUX',\n",
       "       'RSG', 'ROST', 'ROP', 'RMD', 'RF', 'REG', 'REGN', 'QRVO', 'PYPL', 'PNR',\n",
       "       'IRM', 'IPG', 'INTC', 'CSCO', 'CMI', 'CINF', 'CHTR', 'CHD', 'CF', 'CE',\n",
       "       'CAH', 'CAG', 'BIIB', 'BEN', 'BDX', 'BBY', 'A', 'AXP', 'AWK', 'APH',\n",
       "       'APD', 'APA', 'ANET', 'ALL', 'ALLE', 'CPB', 'CTAS', 'IFF', 'DAL', 'HUM',\n",
       "       'HRL', 'HCA', 'GWW', 'GL', 'GLW', 'GE', 'FRT', 'FMC', 'EXPE', 'WM',\n",
       "       'ETN', 'ES', 'EMR', 'EL', 'EIX', 'ED', 'EBAY', 'DHI', 'DG', 'DFS'],\n",
       "      dtype='object', name='symbol')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(symbol_universe_path, \"rb\") as fp:\n",
    "    symbol_universe = pickle.load(fp)\n",
    "    \n",
    "symbol_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feature_set_path, 'rb') as f:\n",
    "    feature_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/naradaw/dev/Charles_Schwab/code/RnD/v4/mlflow_experiments/589106785306301247', creation_time=1731319215217, experiment_id='589106785306301247', last_update_time=1731319215217, lifecycle_stage='active', name='/portfolio-contructor-v4', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_tracking_uri = 'file:/home/naradaw/dev/Charles_Schwab/code/RnD/v4/mlflow_experiments'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "experiment_name = \"/portfolio-contructor-v4\"\n",
    "mlflow.set_experiment(experiment_name)2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "eval_step = 1\n",
    "train_step = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "symbol_universe= symbol_universe\n",
    "num_features= len(feature_set)\n",
    "d_model = 88\n",
    "nheads = 1\n",
    "num_transformer_layers = 2\n",
    "\n",
    "episode_duration= 12   \n",
    "holding_period = 1\n",
    "train_test_split= 0.7\n",
    "symbol_universe = symbol_universe\n",
    "feature_set= feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_universe = random.choices(symbol_universe, k = 20)\n",
    "# symbol_universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "sharpe ratio measures the excess return of the portfolio over the \n",
    "volatility of it -> risk adjusted performance\n",
    "'''\n",
    "\n",
    "def sharp_ratio_(rewards, tran_costs):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env):\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    baseline_returns = []\n",
    "    tran_costs = []\n",
    "    \n",
    "    env.reset(mode = \"val\")\n",
    "    state = env.get_state()\n",
    "\n",
    "    print(\"\")\n",
    "    while not is_end:\n",
    "        _, allocations = model(state)\n",
    "        state, reward, baseline_return, is_end, tran_cost = env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "        baseline_returns.append(baseline_return)\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "    baseline_sharp_ratio = sharp_ratio_(baseline_returns, tran_costs)\n",
    "    return sharp_ratio, baseline_sharp_ratio, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "training model --\n",
      "Step 0: last loss = 0.03890\n",
      "eval step --\n",
      "\n",
      "Step 0: val_rewards = 0.11613801849620946 | baseline_reward = 0.228423187242766\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 1: last loss = 0.19215\n",
      "eval step --\n",
      "\n",
      "Step 1: val_rewards = 0.000380847259117461 | baseline_reward = -0.16229108079929988\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 2: last loss = 0.18415\n",
      "eval step --\n",
      "\n",
      "Step 2: val_rewards = 0.3238481530072141 | baseline_reward = 0.5567011827009365\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 3: last loss = 0.12596\n",
      "eval step --\n",
      "\n",
      "Step 3: val_rewards = 0.15319863906508097 | baseline_reward = 0.23321706193745353\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 4: last loss = 0.29188\n",
      "eval step --\n",
      "\n",
      "Step 4: val_rewards = 0.1163046655931611 | baseline_reward = 0.31717491102312284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 5: last loss = 0.10518\n",
      "eval step --\n",
      "\n",
      "Step 5: val_rewards = 0.25043342810591407 | baseline_reward = 0.44352376289444034\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 6: last loss = -0.00078\n",
      "eval step --\n",
      "\n",
      "Step 6: val_rewards = 0.06152379783011047 | baseline_reward = -0.003409516596186424\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 7: last loss = 0.17638\n",
      "eval step --\n",
      "\n",
      "Step 7: val_rewards = 0.10507593742011824 | baseline_reward = 0.11731911556164225\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 8: last loss = 0.21185\n",
      "eval step --\n",
      "\n",
      "Step 8: val_rewards = 0.20281209095378505 | baseline_reward = 0.3702117888496327\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 9: last loss = 0.21404\n",
      "eval step --\n",
      "\n",
      "Step 9: val_rewards = -0.0011287160926360103 | baseline_reward = -0.07456214172507493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 10: last loss = 0.10462\n",
      "eval step --\n",
      "\n",
      "Step 10: val_rewards = 0.2270665179072219 | baseline_reward = 0.3122829893956993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 11: last loss = 0.17042\n",
      "eval step --\n",
      "\n",
      "Step 11: val_rewards = 0.8593578441931872 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 12: last loss = 0.29390\n",
      "eval step --\n",
      "\n",
      "Step 12: val_rewards = 0.2743063760843469 | baseline_reward = 0.2607676345559731\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 13: last loss = 0.91239\n",
      "eval step --\n",
      "\n",
      "Step 13: val_rewards = 0.31981301697725656 | baseline_reward = 0.6755254114198141\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 14: last loss = 0.90930\n",
      "eval step --\n",
      "\n",
      "Step 14: val_rewards = 0.41582412471842495 | baseline_reward = 0.7821505652965205\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 15: last loss = 0.72250\n",
      "eval step --\n",
      "\n",
      "Step 15: val_rewards = 0.18038978977999803 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 16: last loss = 1.01300\n",
      "eval step --\n",
      "\n",
      "Step 16: val_rewards = 0.14729060817003517 | baseline_reward = 0.17436261906408138\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 17: last loss = 0.67386\n",
      "eval step --\n",
      "\n",
      "Step 17: val_rewards = 0.14280376639023926 | baseline_reward = 0.215384017778709\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 18: last loss = 0.80194\n",
      "eval step --\n",
      "\n",
      "Step 18: val_rewards = 0.6294670726054878 | baseline_reward = 0.7239937256719368\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 19: last loss = 0.78924\n",
      "eval step --\n",
      "\n",
      "Step 19: val_rewards = 0.220848834150284 | baseline_reward = 0.5142365399713867\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 20: last loss = 1.03757\n",
      "eval step --\n",
      "\n",
      "Step 20: val_rewards = 0.3043538423198408 | baseline_reward = 0.585014475106025\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 21: last loss = 0.52033\n",
      "eval step --\n",
      "\n",
      "Step 21: val_rewards = 0.8339688610263539 | baseline_reward = 1.0688939544432159\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 22: last loss = 0.17759\n",
      "eval step --\n",
      "\n",
      "Step 22: val_rewards = 0.14186889437357006 | baseline_reward = 0.15907879776329997\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 23: last loss = 1.03525\n",
      "eval step --\n",
      "\n",
      "Step 23: val_rewards = 0.14420326855473045 | baseline_reward = 0.1474706326515393\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 24: last loss = 0.21689\n",
      "eval step --\n",
      "\n",
      "Step 24: val_rewards = 0.501288277255057 | baseline_reward = 0.6090966240642443\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 25: last loss = 0.10907\n",
      "eval step --\n",
      "\n",
      "Step 25: val_rewards = 0.2209615915724239 | baseline_reward = 0.39753697407656724\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 26: last loss = -0.07231\n",
      "eval step --\n",
      "\n",
      "Step 26: val_rewards = 0.29159421383456274 | baseline_reward = 0.6024305476419072\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 27: last loss = 0.14019\n",
      "eval step --\n",
      "\n",
      "Step 27: val_rewards = 0.3246704623599872 | baseline_reward = 0.5766095402630447\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 28: last loss = 0.19952\n",
      "eval step --\n",
      "\n",
      "Step 28: val_rewards = 0.41590494528573374 | baseline_reward = 0.6611900589510209\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 29: last loss = 0.05152\n",
      "eval step --\n",
      "\n",
      "Step 29: val_rewards = 0.22617083406421162 | baseline_reward = 0.3672249022147251\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 30: last loss = 0.15237\n",
      "eval step --\n",
      "\n",
      "Step 30: val_rewards = 0.17862207667145952 | baseline_reward = 0.4225871894435237\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 31: last loss = 0.58702\n",
      "eval step --\n",
      "\n",
      "Step 31: val_rewards = 0.13068123197235634 | baseline_reward = 0.43422096382035263\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 32: last loss = 0.13030\n",
      "eval step --\n",
      "\n",
      "Step 32: val_rewards = 0.013220245591813359 | baseline_reward = -0.05811074564081976\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 33: last loss = 2.32511\n",
      "eval step --\n",
      "\n",
      "Step 33: val_rewards = 0.3585790761760854 | baseline_reward = 0.4538587421274766\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 34: last loss = 0.50391\n",
      "eval step --\n",
      "\n",
      "Step 34: val_rewards = 0.5419087427186218 | baseline_reward = 0.895889172658081\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 35: last loss = 0.82611\n",
      "eval step --\n",
      "\n",
      "Step 35: val_rewards = 0.32546667239862825 | baseline_reward = 0.31571909219847616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 36: last loss = 0.10676\n",
      "eval step --\n",
      "\n",
      "Step 36: val_rewards = 0.07428684078105229 | baseline_reward = -0.010577587927711345\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 37: last loss = 0.67149\n",
      "eval step --\n",
      "\n",
      "Step 37: val_rewards = 0.5378121942862985 | baseline_reward = 0.9100759627035324\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 38: last loss = 0.99726\n",
      "eval step --\n",
      "\n",
      "Step 38: val_rewards = 0.04210737103631576 | baseline_reward = -0.012112130666527696\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 39: last loss = 0.14808\n",
      "eval step --\n",
      "\n",
      "Step 39: val_rewards = 0.5703211998173753 | baseline_reward = 0.6846530134363902\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 40: last loss = 0.67054\n",
      "eval step --\n",
      "\n",
      "Step 40: val_rewards = 0.05331048910157569 | baseline_reward = -0.031455106902401365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 41: last loss = 0.82498\n",
      "eval step --\n",
      "\n",
      "Step 41: val_rewards = 0.2312885320268548 | baseline_reward = 0.4117356102694612\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 42: last loss = 0.02090\n",
      "eval step --\n",
      "\n",
      "Step 42: val_rewards = 0.2660301337339032 | baseline_reward = 0.26453190562615236\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 43: last loss = 0.05140\n",
      "eval step --\n",
      "\n",
      "Step 43: val_rewards = 0.0003946707498858063 | baseline_reward = -0.09309637816407894\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 44: last loss = 1.04127\n",
      "eval step --\n",
      "\n",
      "Step 44: val_rewards = 0.3164575315489319 | baseline_reward = 0.5005914990654504\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 45: last loss = 0.17690\n",
      "eval step --\n",
      "\n",
      "Step 45: val_rewards = 0.3518707862234362 | baseline_reward = 0.6404916988404056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 46: last loss = 0.73972\n",
      "eval step --\n",
      "\n",
      "Step 46: val_rewards = 0.07179108520403203 | baseline_reward = -0.06703971028350392\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 47: last loss = 0.14763\n",
      "eval step --\n",
      "\n",
      "Step 47: val_rewards = 0.008923927385040742 | baseline_reward = -0.05442783963941461\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 48: last loss = 0.25787\n",
      "eval step --\n",
      "\n",
      "Step 48: val_rewards = 0.13148767352184013 | baseline_reward = 0.20034468310807002\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 49: last loss = 0.15670\n",
      "eval step --\n",
      "\n",
      "Step 49: val_rewards = 0.6268822245239197 | baseline_reward = 0.7221405546661565\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 50: last loss = 0.17988\n",
      "eval step --\n",
      "\n",
      "Step 50: val_rewards = 0.11874300060688421 | baseline_reward = 0.1477246880925813\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 51: last loss = 0.63590\n",
      "eval step --\n",
      "\n",
      "Step 51: val_rewards = 0.3293448785892056 | baseline_reward = 0.31717606811587057\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 52: last loss = 0.16703\n",
      "eval step --\n",
      "\n",
      "Step 52: val_rewards = 0.18156713759841375 | baseline_reward = 0.6298522013287414\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 53: last loss = 0.05363\n",
      "eval step --\n",
      "\n",
      "Step 53: val_rewards = 0.25218093598100827 | baseline_reward = 0.2881678125668414\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 54: last loss = 0.61472\n",
      "eval step --\n",
      "\n",
      "Step 54: val_rewards = 0.18118416060935633 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 55: last loss = 0.19982\n",
      "eval step --\n",
      "\n",
      "Step 55: val_rewards = 0.3031748997552492 | baseline_reward = 0.5324035047867532\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 56: last loss = 0.21595\n",
      "eval step --\n",
      "\n",
      "Step 56: val_rewards = 0.1767058381175267 | baseline_reward = 0.2823729329137771\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 57: last loss = 0.07674\n",
      "eval step --\n",
      "\n",
      "Step 57: val_rewards = 0.00993704314543875 | baseline_reward = -0.009088879188255738\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 58: last loss = 0.01493\n",
      "eval step --\n",
      "\n",
      "Step 58: val_rewards = 0.3253259749661152 | baseline_reward = 0.5968752165536757\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 59: last loss = 0.83592\n",
      "eval step --\n",
      "\n",
      "Step 59: val_rewards = -0.01783209969137983 | baseline_reward = -0.15277082932901595\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 60: last loss = -0.11047\n",
      "eval step --\n",
      "\n",
      "Step 60: val_rewards = 0.5869502377584568 | baseline_reward = 0.8734328424121787\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 61: last loss = 1.14752\n",
      "eval step --\n",
      "\n",
      "Step 61: val_rewards = 0.11770292470054049 | baseline_reward = 0.14062813107180086\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 62: last loss = 0.52102\n",
      "eval step --\n",
      "\n",
      "Step 62: val_rewards = -0.014749705148033516 | baseline_reward = -0.15277082932901595\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 63: last loss = 0.17774\n",
      "eval step --\n",
      "\n",
      "Step 63: val_rewards = 0.12884873449471312 | baseline_reward = 0.31524342901713415\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 64: last loss = 0.14632\n",
      "eval step --\n",
      "\n",
      "Step 64: val_rewards = 0.14213233565818764 | baseline_reward = 0.15378637634495473\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 65: last loss = 1.47614\n",
      "eval step --\n",
      "\n",
      "Step 65: val_rewards = 0.413741446030251 | baseline_reward = 0.7821505652965205\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 66: last loss = 0.72738\n",
      "eval step --\n",
      "\n",
      "Step 66: val_rewards = 0.2514720383100123 | baseline_reward = 0.3739355473845528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 67: last loss = 0.07062\n",
      "eval step --\n",
      "\n",
      "Step 67: val_rewards = 0.33975056360533573 | baseline_reward = 0.6962037494991399\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 68: last loss = -0.06309\n",
      "eval step --\n",
      "\n",
      "Step 68: val_rewards = 0.4436570375591424 | baseline_reward = 0.7047432984001528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 69: last loss = 0.18241\n",
      "eval step --\n",
      "\n",
      "Step 69: val_rewards = 0.10490783572979939 | baseline_reward = 0.1233275802676543\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 70: last loss = 0.17593\n",
      "eval step --\n",
      "\n",
      "Step 70: val_rewards = 0.25519495710967227 | baseline_reward = 0.2881678125668414\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 71: last loss = 0.88517\n",
      "eval step --\n",
      "\n",
      "Step 71: val_rewards = 0.17928058650709108 | baseline_reward = 0.4396546023261596\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 72: last loss = 0.87301\n",
      "eval step --\n",
      "\n",
      "Step 72: val_rewards = 0.07580176529381097 | baseline_reward = -0.010577587927711345\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 73: last loss = 0.55355\n",
      "eval step --\n",
      "\n",
      "Step 73: val_rewards = 0.019860746967832715 | baseline_reward = -0.06909818671718444\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 74: last loss = 0.35425\n",
      "eval step --\n",
      "\n",
      "Step 74: val_rewards = 0.27388826377944675 | baseline_reward = 0.2607676345559731\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 75: last loss = 0.79836\n",
      "eval step --\n",
      "\n",
      "Step 75: val_rewards = 0.2228256006757454 | baseline_reward = 0.5142365399713867\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 76: last loss = 0.25687\n",
      "eval step --\n",
      "\n",
      "Step 76: val_rewards = 0.21588070057345038 | baseline_reward = 0.23027423595052635\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 77: last loss = 0.15816\n",
      "eval step --\n",
      "\n",
      "Step 77: val_rewards = 0.1819574450802828 | baseline_reward = 0.2823729329137771\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 78: last loss = -0.05425\n",
      "eval step --\n",
      "\n",
      "Step 78: val_rewards = 0.1670822369160543 | baseline_reward = 0.4170299714102012\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 79: last loss = 0.15580\n",
      "eval step --\n",
      "\n",
      "Step 79: val_rewards = 0.038879117733531574 | baseline_reward = -0.02399293385146675\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 80: last loss = 0.19145\n",
      "eval step --\n",
      "\n",
      "Step 80: val_rewards = 0.2213394452946161 | baseline_reward = 0.2124053309131323\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 81: last loss = 2.03965\n",
      "eval step --\n",
      "\n",
      "Step 81: val_rewards = 0.34201519247474577 | baseline_reward = 0.5933428545608107\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 82: last loss = 0.30034\n",
      "eval step --\n",
      "\n",
      "Step 82: val_rewards = 0.5750588157147376 | baseline_reward = 0.6846530134363902\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 83: last loss = 0.16261\n",
      "eval step --\n",
      "\n",
      "Step 83: val_rewards = 0.09514367902186831 | baseline_reward = -0.02119186696164961\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 84: last loss = 0.97621\n",
      "eval step --\n",
      "\n",
      "Step 84: val_rewards = 0.07488859060454324 | baseline_reward = -0.07936404126055921\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 85: last loss = 0.59490\n",
      "eval step --\n",
      "\n",
      "Step 85: val_rewards = -0.051955584565825 | baseline_reward = -0.24620996402409803\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 86: last loss = 0.28825\n",
      "eval step --\n",
      "\n",
      "Step 86: val_rewards = 0.14432271884733588 | baseline_reward = 0.15907879776329997\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 87: last loss = 0.81414\n",
      "eval step --\n",
      "\n",
      "Step 87: val_rewards = 0.36286213040070125 | baseline_reward = 0.4538587421274766\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 88: last loss = 0.20661\n",
      "eval step --\n",
      "\n",
      "Step 88: val_rewards = -0.058591049645453634 | baseline_reward = -0.2316945935310149\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 89: last loss = 0.83526\n",
      "eval step --\n",
      "\n",
      "Step 89: val_rewards = 0.3680228612244526 | baseline_reward = 0.590498180397619\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 90: last loss = 1.15310\n",
      "eval step --\n",
      "\n",
      "Step 90: val_rewards = 0.16016765813558995 | baseline_reward = 0.2190904192544018\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 91: last loss = 0.36156\n",
      "eval step --\n",
      "\n",
      "Step 91: val_rewards = 0.4050430751473054 | baseline_reward = 0.7998046578887152\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 92: last loss = 0.50077\n",
      "eval step --\n",
      "\n",
      "Step 92: val_rewards = -0.010653051302505136 | baseline_reward = -0.10096929719687234\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 93: last loss = 0.13853\n",
      "eval step --\n",
      "\n",
      "Step 93: val_rewards = 0.22913601957110233 | baseline_reward = 0.3599848102006836\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 94: last loss = 0.19801\n",
      "eval step --\n",
      "\n",
      "Step 94: val_rewards = 0.28055768757180466 | baseline_reward = 0.45829068686681407\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 95: last loss = 0.98055\n",
      "eval step --\n",
      "\n",
      "Step 95: val_rewards = 0.2799508273723777 | baseline_reward = 0.4261693151487616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 96: last loss = 0.62945\n",
      "eval step --\n",
      "\n",
      "Step 96: val_rewards = 0.11238800196555286 | baseline_reward = 0.3231605434124334\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 97: last loss = 0.06202\n",
      "eval step --\n",
      "\n",
      "Step 97: val_rewards = 0.05518095791801928 | baseline_reward = -0.08558860669238108\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 98: last loss = 0.55274\n",
      "eval step --\n",
      "\n",
      "Step 98: val_rewards = 0.30297315061555813 | baseline_reward = 0.2995999336319901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 99: last loss = 0.65781\n",
      "eval step --\n",
      "\n",
      "Step 99: val_rewards = 0.17076416000404446 | baseline_reward = 0.33314026385318696\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 100: last loss = 0.14970\n",
      "eval step --\n",
      "\n",
      "Step 100: val_rewards = 0.25519702023017293 | baseline_reward = 0.5828692064032645\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 101: last loss = -0.07767\n",
      "eval step --\n",
      "\n",
      "Step 101: val_rewards = 0.0067601028081472605 | baseline_reward = -0.13239103663513663\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 102: last loss = 0.39812\n",
      "eval step --\n",
      "\n",
      "Step 102: val_rewards = 0.18246581465760509 | baseline_reward = 0.2823729329137771\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 103: last loss = 1.05610\n",
      "eval step --\n",
      "\n",
      "Step 103: val_rewards = 0.20895586436851263 | baseline_reward = 0.43942054660951957\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 104: last loss = 2.27761\n",
      "eval step --\n",
      "\n",
      "Step 104: val_rewards = 0.16885009798558712 | baseline_reward = 0.4043101570866993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 105: last loss = 0.12663\n",
      "eval step --\n",
      "\n",
      "Step 105: val_rewards = 0.734034677621144 | baseline_reward = 0.9440461755678171\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 106: last loss = 0.72423\n",
      "eval step --\n",
      "\n",
      "Step 106: val_rewards = 0.12350909845743523 | baseline_reward = 0.17486070796494071\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 107: last loss = 0.85777\n",
      "eval step --\n",
      "\n",
      "Step 107: val_rewards = 0.2346015943985041 | baseline_reward = 0.3677524999648196\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 108: last loss = 0.79664\n",
      "eval step --\n",
      "\n",
      "Step 108: val_rewards = 0.23260821164878315 | baseline_reward = 0.4117356102694612\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 109: last loss = 0.16709\n",
      "eval step --\n",
      "\n",
      "Step 109: val_rewards = 0.020283494631625106 | baseline_reward = -0.06909818671718444\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 110: last loss = 0.16080\n",
      "eval step --\n",
      "\n",
      "Step 110: val_rewards = 0.24869193669940265 | baseline_reward = 0.602770560327721\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 111: last loss = 0.16919\n",
      "eval step --\n",
      "\n",
      "Step 111: val_rewards = 0.038936692819188906 | baseline_reward = -0.0908382162825835\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 112: last loss = 0.25887\n",
      "eval step --\n",
      "\n",
      "Step 112: val_rewards = 0.12252832994618691 | baseline_reward = 0.14477155423780128\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 113: last loss = 0.71630\n",
      "eval step --\n",
      "\n",
      "Step 113: val_rewards = -0.00015237706201315856 | baseline_reward = -0.17945907838489839\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 114: last loss = 0.65778\n",
      "eval step --\n",
      "\n",
      "Step 114: val_rewards = -0.03276691446191599 | baseline_reward = -0.21418759877625257\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 115: last loss = 0.09689\n",
      "eval step --\n",
      "\n",
      "Step 115: val_rewards = -0.007522396412405514 | baseline_reward = -0.16904979893747296\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 116: last loss = 0.98043\n",
      "eval step --\n",
      "\n",
      "Step 116: val_rewards = 0.22560233798928184 | baseline_reward = 0.3122829893956993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 117: last loss = 0.20407\n",
      "eval step --\n",
      "\n",
      "Step 117: val_rewards = -0.002648027467365205 | baseline_reward = -0.16466445292668708\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 118: last loss = 0.19729\n",
      "eval step --\n",
      "\n",
      "Step 118: val_rewards = 0.08036223569634249 | baseline_reward = -0.02608481350295613\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 119: last loss = 1.16349\n",
      "eval step --\n",
      "\n",
      "Step 119: val_rewards = 0.14030255190030022 | baseline_reward = 0.24461665486811268\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 120: last loss = 1.04217\n",
      "eval step --\n",
      "\n",
      "Step 120: val_rewards = 0.16628163618682876 | baseline_reward = 0.39052407351688634\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 121: last loss = 0.18204\n",
      "eval step --\n",
      "\n",
      "Step 121: val_rewards = 0.36705108084054 | baseline_reward = 0.7659950764657154\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 122: last loss = 0.11782\n",
      "eval step --\n",
      "\n",
      "Step 122: val_rewards = 0.327441938426612 | baseline_reward = 0.78621589067038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 123: last loss = 0.15672\n",
      "eval step --\n",
      "\n",
      "Step 123: val_rewards = 0.4014773419482727 | baseline_reward = 0.5671239306855195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 124: last loss = 0.20627\n",
      "eval step --\n",
      "\n",
      "Step 124: val_rewards = 0.6317712452104328 | baseline_reward = 0.7221405546661565\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 125: last loss = 0.18600\n",
      "eval step --\n",
      "\n",
      "Step 125: val_rewards = 0.7605160865959818 | baseline_reward = 0.6767681972226866\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 126: last loss = 0.11602\n",
      "eval step --\n",
      "\n",
      "Step 126: val_rewards = 0.0424226430959778 | baseline_reward = -0.09904704338521142\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 127: last loss = 0.68674\n",
      "eval step --\n",
      "\n",
      "Step 127: val_rewards = 0.155950082341433 | baseline_reward = 0.23321706193745353\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 128: last loss = 0.21904\n",
      "eval step --\n",
      "\n",
      "Step 128: val_rewards = 0.2675729841903106 | baseline_reward = 0.2702599925690532\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 129: last loss = 0.20025\n",
      "eval step --\n",
      "\n",
      "Step 129: val_rewards = 0.32546975666526573 | baseline_reward = 0.4444138742060351\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 130: last loss = 0.14100\n",
      "eval step --\n",
      "\n",
      "Step 130: val_rewards = 0.30373877884241923 | baseline_reward = 0.2995999336319901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 131: last loss = 1.01252\n",
      "eval step --\n",
      "\n",
      "Step 131: val_rewards = 0.16476559707522404 | baseline_reward = 0.4060583580745704\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 132: last loss = 0.63861\n",
      "eval step --\n",
      "\n",
      "Step 132: val_rewards = 0.3214499810423059 | baseline_reward = 0.568294715501061\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 133: last loss = 0.13089\n",
      "eval step --\n",
      "\n",
      "Step 133: val_rewards = 0.19312804366830605 | baseline_reward = 0.44764654664767\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 134: last loss = 0.18285\n",
      "eval step --\n",
      "\n",
      "Step 134: val_rewards = 0.32286165591372956 | baseline_reward = 0.568294715501061\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 135: last loss = 0.89319\n",
      "eval step --\n",
      "\n",
      "Step 135: val_rewards = 0.3904529485134926 | baseline_reward = 0.861823392070796\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 136: last loss = 0.19822\n",
      "eval step --\n",
      "\n",
      "Step 136: val_rewards = 0.10359718727540114 | baseline_reward = 0.11731911556164225\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 137: last loss = 0.10075\n",
      "eval step --\n",
      "\n",
      "Step 137: val_rewards = 0.32636839417779057 | baseline_reward = 0.78621589067038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 138: last loss = 0.24915\n",
      "eval step --\n",
      "\n",
      "Step 138: val_rewards = 0.005223529299733492 | baseline_reward = -0.07213108081473303\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 139: last loss = 0.93230\n",
      "eval step --\n",
      "\n",
      "Step 139: val_rewards = 0.597794656426995 | baseline_reward = 0.7866991214420845\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 140: last loss = 0.20702\n",
      "eval step --\n",
      "\n",
      "Step 140: val_rewards = 0.1592829971469462 | baseline_reward = 0.3118687685849124\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 141: last loss = 0.12149\n",
      "eval step --\n",
      "\n",
      "Step 141: val_rewards = 0.2230232397565381 | baseline_reward = 0.2124053309131323\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 142: last loss = 0.24174\n",
      "eval step --\n",
      "\n",
      "Step 142: val_rewards = 0.9559188837531312 | baseline_reward = 1.0493573060254198\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 143: last loss = 0.28565\n",
      "eval step --\n",
      "\n",
      "Step 143: val_rewards = 0.5044259689592112 | baseline_reward = 0.6090966240642443\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 144: last loss = 0.07377\n",
      "eval step --\n",
      "\n",
      "Step 144: val_rewards = -0.007321960782354045 | baseline_reward = -0.04377323144370932\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 145: last loss = -0.03926\n",
      "eval step --\n",
      "\n",
      "Step 145: val_rewards = 0.3598121926733283 | baseline_reward = 0.6413037700199038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 146: last loss = 0.66214\n",
      "eval step --\n",
      "\n",
      "Step 146: val_rewards = 0.41718835346580163 | baseline_reward = 0.6611900589510209\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 147: last loss = 0.40240\n",
      "eval step --\n",
      "\n",
      "Step 147: val_rewards = 0.07252170038183398 | baseline_reward = -0.06703971028350392\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 148: last loss = 0.16558\n",
      "eval step --\n",
      "\n",
      "Step 148: val_rewards = 0.28181755546782794 | baseline_reward = 0.5243451811803399\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 149: last loss = 0.55295\n",
      "eval step --\n",
      "\n",
      "Step 149: val_rewards = -0.05859842250622414 | baseline_reward = -0.2316945935310149\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 150: last loss = 1.07566\n",
      "eval step --\n",
      "\n",
      "Step 150: val_rewards = 0.33424947998285925 | baseline_reward = 0.3095780964054054\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 151: last loss = 0.79028\n",
      "eval step --\n",
      "\n",
      "Step 151: val_rewards = 0.5354177746050145 | baseline_reward = 0.9026240930820785\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 152: last loss = 0.58655\n",
      "eval step --\n",
      "\n",
      "Step 152: val_rewards = 0.10234812938502043 | baseline_reward = 0.14527408995758503\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 153: last loss = 0.75608\n",
      "eval step --\n",
      "\n",
      "Step 153: val_rewards = 0.4170511861484786 | baseline_reward = 0.6611900589510209\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 154: last loss = 0.72057\n",
      "eval step --\n",
      "\n",
      "Step 154: val_rewards = 0.2979075358443522 | baseline_reward = 0.6966527538884659\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 155: last loss = -0.02601\n",
      "eval step --\n",
      "\n",
      "Step 155: val_rewards = 0.020340846566632598 | baseline_reward = 0.0002751260614751749\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 156: last loss = 1.13115\n",
      "eval step --\n",
      "\n",
      "Step 156: val_rewards = 0.042271501121566975 | baseline_reward = -0.09904704338521142\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 157: last loss = 0.08503\n",
      "eval step --\n",
      "\n",
      "Step 157: val_rewards = 0.1833066760889934 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 158: last loss = 0.58033\n",
      "eval step --\n",
      "\n",
      "Step 158: val_rewards = 0.19626194415110987 | baseline_reward = 0.46218582008508496\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 159: last loss = 0.30333\n",
      "eval step --\n",
      "\n",
      "Step 159: val_rewards = 0.07282629054221124 | baseline_reward = -0.06703971028350392\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 160: last loss = 0.20018\n",
      "eval step --\n",
      "\n",
      "Step 160: val_rewards = 0.19091236864562253 | baseline_reward = 0.2155564799881694\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 161: last loss = 0.07932\n",
      "eval step --\n",
      "\n",
      "Step 161: val_rewards = 0.20951637152032687 | baseline_reward = 0.3618142119531705\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 162: last loss = 0.62815\n",
      "eval step --\n",
      "\n",
      "Step 162: val_rewards = 0.06032453328666309 | baseline_reward = 0.04985099102552516\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 163: last loss = 0.10701\n",
      "eval step --\n",
      "\n",
      "Step 163: val_rewards = 0.8389102737778128 | baseline_reward = 1.0688939544432159\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 164: last loss = 0.17933\n",
      "eval step --\n",
      "\n",
      "Step 164: val_rewards = 0.3110413635971886 | baseline_reward = 0.5313125594188126\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 165: last loss = 0.07704\n",
      "eval step --\n",
      "\n",
      "Step 165: val_rewards = 0.40259595928502845 | baseline_reward = 0.5671239306855195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 166: last loss = 1.47496\n",
      "eval step --\n",
      "\n",
      "Step 166: val_rewards = 0.27578994249198857 | baseline_reward = 0.2607676345559731\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 167: last loss = 0.16718\n",
      "eval step --\n",
      "\n",
      "Step 167: val_rewards = 0.13166249168548727 | baseline_reward = 0.15617878480221317\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 168: last loss = 0.15136\n",
      "eval step --\n",
      "\n",
      "Step 168: val_rewards = 0.048706100322622944 | baseline_reward = 0.08159079228309284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 169: last loss = 0.16609\n",
      "eval step --\n",
      "\n",
      "Step 169: val_rewards = 0.3294602850568966 | baseline_reward = 0.31571909219847616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 170: last loss = 0.94071\n",
      "eval step --\n",
      "\n",
      "Step 170: val_rewards = 0.17966219110487708 | baseline_reward = 0.4396546023261596\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 171: last loss = -0.10305\n",
      "eval step --\n",
      "\n",
      "Step 171: val_rewards = 0.005384577376441479 | baseline_reward = -0.07213108081473303\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 172: last loss = 0.14258\n",
      "eval step --\n",
      "\n",
      "Step 172: val_rewards = 0.1836409253077254 | baseline_reward = 0.1904116984870983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 173: last loss = 0.21294\n",
      "eval step --\n",
      "\n",
      "Step 173: val_rewards = 0.3146820627039784 | baseline_reward = 0.6043944165896826\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 174: last loss = 0.02581\n",
      "eval step --\n",
      "\n",
      "Step 174: val_rewards = 0.2465003019397087 | baseline_reward = 0.39943155544143466\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 175: last loss = -0.09808\n",
      "eval step --\n",
      "\n",
      "Step 175: val_rewards = 0.3705170870267914 | baseline_reward = 0.7634730344525157\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 176: last loss = 0.19989\n",
      "eval step --\n",
      "\n",
      "Step 176: val_rewards = 0.2444557148623777 | baseline_reward = 0.5153802792007658\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 177: last loss = 0.22511\n",
      "eval step --\n",
      "\n",
      "Step 177: val_rewards = 0.14725040651811291 | baseline_reward = 0.1474706326515393\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 178: last loss = 0.59675\n",
      "eval step --\n",
      "\n",
      "Step 178: val_rewards = 0.15038316362781334 | baseline_reward = 0.4977655979174772\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 179: last loss = 1.24335\n",
      "eval step --\n",
      "\n",
      "Step 179: val_rewards = 0.00936040494092335 | baseline_reward = -0.08881734215636583\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 180: last loss = 1.44341\n",
      "eval step --\n",
      "\n",
      "Step 180: val_rewards = 0.11232478203958539 | baseline_reward = 0.14562644791279072\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 181: last loss = 0.19381\n",
      "eval step --\n",
      "\n",
      "Step 181: val_rewards = 0.1837050226988613 | baseline_reward = 0.1904116984870983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 182: last loss = 0.89397\n",
      "eval step --\n",
      "\n",
      "Step 182: val_rewards = 0.14210017536134537 | baseline_reward = 0.17187821155951552\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 183: last loss = 0.93802\n",
      "eval step --\n",
      "\n",
      "Step 183: val_rewards = -0.016272071722325483 | baseline_reward = -0.08023269704425942\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 184: last loss = 0.04267\n",
      "eval step --\n",
      "\n",
      "Step 184: val_rewards = 0.27998044404943967 | baseline_reward = 0.45829068686681407\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 185: last loss = 0.02281\n",
      "eval step --\n",
      "\n",
      "Step 185: val_rewards = 0.19161487962183088 | baseline_reward = 0.2155564799881694\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 186: last loss = -0.09317\n",
      "eval step --\n",
      "\n",
      "Step 186: val_rewards = 0.16001136220738862 | baseline_reward = 0.30276011091662647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 187: last loss = 0.44761\n",
      "eval step --\n",
      "\n",
      "Step 187: val_rewards = 0.38821394022198924 | baseline_reward = 0.7194619417697877\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 188: last loss = 0.35349\n",
      "eval step --\n",
      "\n",
      "Step 188: val_rewards = 0.1503228228125053 | baseline_reward = 0.17436261906408138\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 189: last loss = 1.07831\n",
      "eval step --\n",
      "\n",
      "Step 189: val_rewards = 0.3444300451762358 | baseline_reward = 0.5933428545608107\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 190: last loss = 0.16458\n",
      "eval step --\n",
      "\n",
      "Step 190: val_rewards = 0.32877567836752425 | baseline_reward = 0.6585371418915216\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 191: last loss = 0.09860\n",
      "eval step --\n",
      "\n",
      "Step 191: val_rewards = 0.20284322846973016 | baseline_reward = 0.3702117888496327\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 192: last loss = 0.17212\n",
      "eval step --\n",
      "\n",
      "Step 192: val_rewards = 0.14616955301580947 | baseline_reward = 0.1888250569240263\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 193: last loss = 0.13843\n",
      "eval step --\n",
      "\n",
      "Step 193: val_rewards = 0.05772290678346228 | baseline_reward = -0.09917551120496682\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 194: last loss = 0.54779\n",
      "eval step --\n",
      "\n",
      "Step 194: val_rewards = 0.6443384675748305 | baseline_reward = 0.755995986508099\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 195: last loss = 1.15125\n",
      "eval step --\n",
      "\n",
      "Step 195: val_rewards = -0.015893361451011535 | baseline_reward = -0.07780050973361903\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 196: last loss = 0.05195\n",
      "eval step --\n",
      "\n",
      "Step 196: val_rewards = 0.15231503804343127 | baseline_reward = 0.20296428271805433\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 197: last loss = 1.14769\n",
      "eval step --\n",
      "\n",
      "Step 197: val_rewards = -0.04329695104044069 | baseline_reward = -0.1440646095582647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 198: last loss = 0.19638\n",
      "eval step --\n",
      "\n",
      "Step 198: val_rewards = 0.16972425097964156 | baseline_reward = 0.4043101570866993\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 199: last loss = 0.16652\n",
      "eval step --\n",
      "\n",
      "Step 199: val_rewards = 0.1926528303400751 | baseline_reward = 0.3904089531078603\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 200: last loss = 0.14450\n",
      "eval step --\n",
      "\n",
      "Step 200: val_rewards = 0.15206840614892636 | baseline_reward = 0.40153176897500614\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 201: last loss = 0.07086\n",
      "eval step --\n",
      "\n",
      "Step 201: val_rewards = 0.1998739480606623 | baseline_reward = 0.3921332160111007\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 202: last loss = 0.16395\n",
      "eval step --\n",
      "\n",
      "Step 202: val_rewards = 0.2776158081203735 | baseline_reward = 0.6164913848726767\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 203: last loss = 0.79129\n",
      "eval step --\n",
      "\n",
      "Step 203: val_rewards = 0.3195959195572175 | baseline_reward = 0.5770104337391783\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 204: last loss = 0.63129\n",
      "eval step --\n",
      "\n",
      "Step 204: val_rewards = 0.27576708154026747 | baseline_reward = 0.4044658951966814\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 205: last loss = 0.13981\n",
      "eval step --\n",
      "\n",
      "Step 205: val_rewards = 0.8557619912745819 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 206: last loss = 0.55085\n",
      "eval step --\n",
      "\n",
      "Step 206: val_rewards = 0.27048758051659894 | baseline_reward = 0.2759578983022423\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 207: last loss = 1.03210\n",
      "eval step --\n",
      "\n",
      "Step 207: val_rewards = 0.26708688213302084 | baseline_reward = 0.40042586228915794\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 208: last loss = 0.81676\n",
      "eval step --\n",
      "\n",
      "Step 208: val_rewards = 0.0702114952060183 | baseline_reward = -0.06875045936192646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 209: last loss = 0.61261\n",
      "eval step --\n",
      "\n",
      "Step 209: val_rewards = 0.17163709875391575 | baseline_reward = 0.4271139492873292\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 210: last loss = 0.89629\n",
      "eval step --\n",
      "\n",
      "Step 210: val_rewards = 0.08781914054824805 | baseline_reward = 0.13219777075656255\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 211: last loss = 0.87247\n",
      "eval step --\n",
      "\n",
      "Step 211: val_rewards = 0.06957196377809947 | baseline_reward = -0.06875045936192646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 212: last loss = 0.38742\n",
      "eval step --\n",
      "\n",
      "Step 212: val_rewards = 0.2819201691769405 | baseline_reward = 0.2738009290954773\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 213: last loss = 0.14049\n",
      "eval step --\n",
      "\n",
      "Step 213: val_rewards = 0.3018737021693673 | baseline_reward = 0.3426191679595786\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 214: last loss = 0.88002\n",
      "eval step --\n",
      "\n",
      "Step 214: val_rewards = 0.18142669048689825 | baseline_reward = 0.4225871894435237\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 215: last loss = 1.15789\n",
      "eval step --\n",
      "\n",
      "Step 215: val_rewards = 0.4094840223791779 | baseline_reward = 0.5391757276544343\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 216: last loss = 0.24843\n",
      "eval step --\n",
      "\n",
      "Step 216: val_rewards = 0.3181387987335962 | baseline_reward = 0.30585187174118195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 217: last loss = 0.91679\n",
      "eval step --\n",
      "\n",
      "Step 217: val_rewards = 0.26745608245808816 | baseline_reward = 0.26453190562615236\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 218: last loss = 0.15800\n",
      "eval step --\n",
      "\n",
      "Step 218: val_rewards = 0.02707936731042534 | baseline_reward = -0.09608712917273023\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 219: last loss = 0.78854\n",
      "eval step --\n",
      "\n",
      "Step 219: val_rewards = 0.055087055872286655 | baseline_reward = -0.08558860669238108\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 220: last loss = 0.31989\n",
      "eval step --\n",
      "\n",
      "Step 220: val_rewards = 0.18291424627704417 | baseline_reward = 0.2432610208392248\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 221: last loss = 0.19338\n",
      "eval step --\n",
      "\n",
      "Step 221: val_rewards = 0.038926037399737955 | baseline_reward = -0.0908382162825835\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 222: last loss = 0.16780\n",
      "eval step --\n",
      "\n",
      "Step 222: val_rewards = 0.48653648789727894 | baseline_reward = 0.709124352651483\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 223: last loss = 0.22712\n",
      "eval step --\n",
      "\n",
      "Step 223: val_rewards = 0.06095119256171767 | baseline_reward = 0.04985099102552516\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 224: last loss = 0.79122\n",
      "eval step --\n",
      "\n",
      "Step 224: val_rewards = 0.18720222059162345 | baseline_reward = 0.4680217543380443\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 225: last loss = 1.02287\n",
      "eval step --\n",
      "\n",
      "Step 225: val_rewards = 0.31997364530873024 | baseline_reward = 0.5770104337391783\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 226: last loss = 0.18006\n",
      "eval step --\n",
      "\n",
      "Step 226: val_rewards = 0.27671287446259146 | baseline_reward = 0.5483738482333121\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 227: last loss = 1.00003\n",
      "eval step --\n",
      "\n",
      "Step 227: val_rewards = 0.35975671667196024 | baseline_reward = 0.5661528162848493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 228: last loss = 0.12773\n",
      "eval step --\n",
      "\n",
      "Step 228: val_rewards = 0.40670806950706195 | baseline_reward = 0.7441093993984506\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 229: last loss = 2.32789\n",
      "eval step --\n",
      "\n",
      "Step 229: val_rewards = 0.26015204245769674 | baseline_reward = 0.25289264723208765\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 230: last loss = 0.18335\n",
      "eval step --\n",
      "\n",
      "Step 230: val_rewards = 0.03841709445049414 | baseline_reward = 0.008074204506293097\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 231: last loss = 0.15973\n",
      "eval step --\n",
      "\n",
      "Step 231: val_rewards = 0.32583242162152265 | baseline_reward = 0.6647957189755535\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 232: last loss = 0.83983\n",
      "eval step --\n",
      "\n",
      "Step 232: val_rewards = 0.23265959327485794 | baseline_reward = 0.4381560332401185\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 233: last loss = 0.24727\n",
      "eval step --\n",
      "\n",
      "Step 233: val_rewards = 0.19414447670518864 | baseline_reward = 0.3904089531078603\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 234: last loss = 0.99598\n",
      "eval step --\n",
      "\n",
      "Step 234: val_rewards = 0.28228994073331515 | baseline_reward = 0.4261693151487616\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 235: last loss = 0.15091\n",
      "eval step --\n",
      "\n",
      "Step 235: val_rewards = -0.02337036731778879 | baseline_reward = -0.2024200961693741\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 236: last loss = 0.18207\n",
      "eval step --\n",
      "\n",
      "Step 236: val_rewards = -0.008929236229193753 | baseline_reward = -0.10096929719687234\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 237: last loss = 0.09688\n",
      "eval step --\n",
      "\n",
      "Step 237: val_rewards = 0.12286476840270111 | baseline_reward = 0.3564550550721981\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 238: last loss = 0.19846\n",
      "eval step --\n",
      "\n",
      "Step 238: val_rewards = 0.2681384543902867 | baseline_reward = 0.26453190562615236\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 239: last loss = 0.15677\n",
      "eval step --\n",
      "\n",
      "Step 239: val_rewards = 0.3605554711210511 | baseline_reward = 0.6413037700199038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 240: last loss = 0.61630\n",
      "eval step --\n",
      "\n",
      "Step 240: val_rewards = -0.007081659525068654 | baseline_reward = -0.04377323144370932\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 241: last loss = 0.09424\n",
      "eval step --\n",
      "\n",
      "Step 241: val_rewards = 0.02023032617371787 | baseline_reward = 0.0002751260614751749\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 242: last loss = 0.93566\n",
      "eval step --\n",
      "\n",
      "Step 242: val_rewards = 0.00019922647666648495 | baseline_reward = -0.16229108079929988\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 243: last loss = 0.17213\n",
      "eval step --\n",
      "\n",
      "Step 243: val_rewards = 0.06954708060691965 | baseline_reward = -0.06875045936192646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 244: last loss = 0.11388\n",
      "eval step --\n",
      "\n",
      "Step 244: val_rewards = 0.3532209197882049 | baseline_reward = 0.5394294703705088\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 245: last loss = 0.20032\n",
      "eval step --\n",
      "\n",
      "Step 245: val_rewards = 0.2888850104600609 | baseline_reward = 0.4284104212671813\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 246: last loss = 0.16392\n",
      "eval step --\n",
      "\n",
      "Step 246: val_rewards = 0.30770045854461536 | baseline_reward = 0.468248880948304\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 247: last loss = 0.21552\n",
      "eval step --\n",
      "\n",
      "Step 247: val_rewards = 0.2569430270157973 | baseline_reward = 0.5828692064032645\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 248: last loss = 0.17235\n",
      "eval step --\n",
      "\n",
      "Step 248: val_rewards = 0.04131083818067879 | baseline_reward = -0.01684594218315476\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 249: last loss = 0.45493\n",
      "eval step --\n",
      "\n",
      "Step 249: val_rewards = 0.1312216293356066 | baseline_reward = 0.13325769088418363\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 250: last loss = -0.02603\n",
      "eval step --\n",
      "\n",
      "Step 250: val_rewards = 0.4045504449160981 | baseline_reward = 0.7998046578887152\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 251: last loss = 0.19319\n",
      "eval step --\n",
      "\n",
      "Step 251: val_rewards = -0.03207962642180923 | baseline_reward = -0.24302001281267305\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 252: last loss = 0.19216\n",
      "eval step --\n",
      "\n",
      "Step 252: val_rewards = 0.053494749962202714 | baseline_reward = -0.031455106902401365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 253: last loss = 0.95883\n",
      "eval step --\n",
      "\n",
      "Step 253: val_rewards = 0.14065067025046607 | baseline_reward = 0.24461665486811268\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 254: last loss = 0.24022\n",
      "eval step --\n",
      "\n",
      "Step 254: val_rewards = 0.3277483305618515 | baseline_reward = 0.7393541260102793\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 255: last loss = 0.11648\n",
      "eval step --\n",
      "\n",
      "Step 255: val_rewards = 0.1394131834181272 | baseline_reward = 0.24461665486811268\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 256: last loss = 0.83756\n",
      "eval step --\n",
      "\n",
      "Step 256: val_rewards = 0.37180396403087856 | baseline_reward = 0.6685638503863266\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 257: last loss = 0.73570\n",
      "eval step --\n",
      "\n",
      "Step 257: val_rewards = 0.2677376389286064 | baseline_reward = 0.40042586228915794\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 258: last loss = 0.21240\n",
      "eval step --\n",
      "\n",
      "Step 258: val_rewards = 0.32672008600237035 | baseline_reward = 0.7393541260102793\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 259: last loss = 0.03276\n",
      "eval step --\n",
      "\n",
      "Step 259: val_rewards = 0.18276289234669782 | baseline_reward = 0.4225871894435237\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 260: last loss = 0.19986\n",
      "eval step --\n",
      "\n",
      "Step 260: val_rewards = 0.18142825104692722 | baseline_reward = 0.5111717062227966\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 261: last loss = 0.59555\n",
      "eval step --\n",
      "\n",
      "Step 261: val_rewards = 0.18115063391919753 | baseline_reward = 0.5111717062227966\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 262: last loss = 0.21972\n",
      "eval step --\n",
      "\n",
      "Step 262: val_rewards = 0.5911776071201932 | baseline_reward = 0.7754039079169707\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 263: last loss = 0.32872\n",
      "eval step --\n",
      "\n",
      "Step 263: val_rewards = 0.05108383683315584 | baseline_reward = 0.03231802212210823\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 264: last loss = 0.20504\n",
      "eval step --\n",
      "\n",
      "Step 264: val_rewards = 1.0893446039148817 | baseline_reward = 1.1910070511535245\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 265: last loss = 0.82404\n",
      "eval step --\n",
      "\n",
      "Step 265: val_rewards = 0.4014688716730597 | baseline_reward = 0.5671239306855195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 266: last loss = 0.13600\n",
      "eval step --\n",
      "\n",
      "Step 266: val_rewards = 0.49229614943040567 | baseline_reward = 0.7836268693875773\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 267: last loss = 0.82390\n",
      "eval step --\n",
      "\n",
      "Step 267: val_rewards = 0.496276723794662 | baseline_reward = 0.8543664175585357\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 268: last loss = 0.10306\n",
      "eval step --\n",
      "\n",
      "Step 268: val_rewards = 0.14443501445759985 | baseline_reward = 0.17499492880561787\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 269: last loss = 0.72632\n",
      "eval step --\n",
      "\n",
      "Step 269: val_rewards = 0.3687850628478961 | baseline_reward = 0.590498180397619\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 270: last loss = 1.31842\n",
      "eval step --\n",
      "\n",
      "Step 270: val_rewards = 0.18499484783401904 | baseline_reward = 0.6298522013287414\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 271: last loss = 0.07005\n",
      "eval step --\n",
      "\n",
      "Step 271: val_rewards = 0.18315458566588375 | baseline_reward = 0.1904116984870983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 272: last loss = 0.14077\n",
      "eval step --\n",
      "\n",
      "Step 272: val_rewards = 0.00036531623503267873 | baseline_reward = -0.09309637816407894\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 273: last loss = 0.74601\n",
      "eval step --\n",
      "\n",
      "Step 273: val_rewards = 0.4540161251846758 | baseline_reward = 0.5464799477140734\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 274: last loss = 0.19480\n",
      "eval step --\n",
      "\n",
      "Step 274: val_rewards = 0.17294918581452595 | baseline_reward = 0.43172369358075907\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 275: last loss = 0.19416\n",
      "eval step --\n",
      "\n",
      "Step 275: val_rewards = -0.014287248457046384 | baseline_reward = -0.15277082932901595\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 276: last loss = 2.06573\n",
      "eval step --\n",
      "\n",
      "Step 276: val_rewards = 0.33069371089833083 | baseline_reward = 0.6682625788014249\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 277: last loss = 0.18250\n",
      "eval step --\n",
      "\n",
      "Step 277: val_rewards = 0.21655795435718067 | baseline_reward = 0.23027423595052635\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 278: last loss = 0.14189\n",
      "eval step --\n",
      "\n",
      "Step 278: val_rewards = 0.1448007653169049 | baseline_reward = 0.15907879776329997\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 279: last loss = 0.91312\n",
      "eval step --\n",
      "\n",
      "Step 279: val_rewards = 0.09665012402923534 | baseline_reward = -0.02119186696164961\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 280: last loss = 0.29807\n",
      "eval step --\n",
      "\n",
      "Step 280: val_rewards = 0.018028700341105965 | baseline_reward = -0.07636668264342028\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 281: last loss = -0.10371\n",
      "eval step --\n",
      "\n",
      "Step 281: val_rewards = 0.391032385113284 | baseline_reward = 0.861823392070796\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 282: last loss = 0.21469\n",
      "eval step --\n",
      "\n",
      "Step 282: val_rewards = 0.060225154288725505 | baseline_reward = -0.03275317317793365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 283: last loss = 0.17904\n",
      "eval step --\n",
      "\n",
      "Step 283: val_rewards = 0.2980310115277452 | baseline_reward = 0.6220125350710067\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 284: last loss = 0.09024\n",
      "eval step --\n",
      "\n",
      "Step 284: val_rewards = 0.0314646102936784 | baseline_reward = -0.040765858608859395\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 285: last loss = -0.05231\n",
      "eval step --\n",
      "\n",
      "Step 285: val_rewards = 0.3441000082679343 | baseline_reward = 0.5933428545608107\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 286: last loss = 1.10167\n",
      "eval step --\n",
      "\n",
      "Step 286: val_rewards = 0.4707568106590359 | baseline_reward = 0.48672117965628475\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 287: last loss = 0.11829\n",
      "eval step --\n",
      "\n",
      "Step 287: val_rewards = 0.19546119833534192 | baseline_reward = 0.45225390939114807\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 288: last loss = 0.13580\n",
      "eval step --\n",
      "\n",
      "Step 288: val_rewards = 0.7130184145461322 | baseline_reward = 0.9287126686351588\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 289: last loss = 1.20377\n",
      "eval step --\n",
      "\n",
      "Step 289: val_rewards = 0.11811252819982045 | baseline_reward = 0.37125188513479357\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 290: last loss = 0.15689\n",
      "eval step --\n",
      "\n",
      "Step 290: val_rewards = 0.5432649364524742 | baseline_reward = 0.9100759627035324\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 291: last loss = 1.35984\n",
      "eval step --\n",
      "\n",
      "Step 291: val_rewards = 0.31157189714542854 | baseline_reward = 0.58349669716649\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 292: last loss = 0.26939\n",
      "eval step --\n",
      "\n",
      "Step 292: val_rewards = 0.371643520525281 | baseline_reward = 0.7634730344525157\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 293: last loss = 0.18283\n",
      "eval step --\n",
      "\n",
      "Step 293: val_rewards = -0.058795452794336865 | baseline_reward = -0.2316945935310149\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 294: last loss = 0.55824\n",
      "eval step --\n",
      "\n",
      "Step 294: val_rewards = 0.3193228730930661 | baseline_reward = 0.5770104337391783\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 295: last loss = 0.25758\n",
      "eval step --\n",
      "\n",
      "Step 295: val_rewards = 0.5768348865497829 | baseline_reward = 0.8967784146707821\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 296: last loss = 0.11648\n",
      "eval step --\n",
      "\n",
      "Step 296: val_rewards = 0.353923536377378 | baseline_reward = 0.6220038779555153\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 297: last loss = 1.15182\n",
      "eval step --\n",
      "\n",
      "Step 297: val_rewards = 0.49886620810629667 | baseline_reward = 0.8802768070753949\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 298: last loss = 1.54962\n",
      "eval step --\n",
      "\n",
      "Step 298: val_rewards = -0.028659123386359463 | baseline_reward = -0.2114658332810284\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 299: last loss = 0.62121\n",
      "eval step --\n",
      "\n",
      "Step 299: val_rewards = 0.4787229073384583 | baseline_reward = 0.8402781731484693\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 300: last loss = 0.21838\n",
      "eval step --\n",
      "\n",
      "Step 300: val_rewards = 0.36412022188148363 | baseline_reward = 0.5882721255930647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 301: last loss = 0.11079\n",
      "eval step --\n",
      "\n",
      "Step 301: val_rewards = 0.6944043064324893 | baseline_reward = 0.8244034374954186\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 302: last loss = 0.94058\n",
      "eval step --\n",
      "\n",
      "Step 302: val_rewards = 0.18333485735701677 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 303: last loss = 0.12660\n",
      "eval step --\n",
      "\n",
      "Step 303: val_rewards = 0.20191779490069978 | baseline_reward = 0.2946550987041576\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 304: last loss = 0.09695\n",
      "eval step --\n",
      "\n",
      "Step 304: val_rewards = 0.3342650776261581 | baseline_reward = 0.3095780964054054\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 305: last loss = 0.20659\n",
      "eval step --\n",
      "\n",
      "Step 305: val_rewards = 0.3041670686337918 | baseline_reward = 0.2995999336319901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 306: last loss = 0.08866\n",
      "eval step --\n",
      "\n",
      "Step 306: val_rewards = 0.17019822937329496 | baseline_reward = 0.33314026385318696\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 307: last loss = 0.28558\n",
      "eval step --\n",
      "\n",
      "Step 307: val_rewards = 0.16928722182089073 | baseline_reward = 0.29129226906421013\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 308: last loss = 1.12796\n",
      "eval step --\n",
      "\n",
      "Step 308: val_rewards = 0.05829345531140214 | baseline_reward = -0.04421807519264951\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 309: last loss = 0.24197\n",
      "eval step --\n",
      "\n",
      "Step 309: val_rewards = 0.180535731617543 | baseline_reward = 0.4396546023261596\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 310: last loss = 0.62429\n",
      "eval step --\n",
      "\n",
      "Step 310: val_rewards = 0.09642594250581416 | baseline_reward = -0.02119186696164961\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 311: last loss = 0.35248\n",
      "eval step --\n",
      "\n",
      "Step 311: val_rewards = 0.1521131065417579 | baseline_reward = 0.20296428271805433\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 312: last loss = 0.09562\n",
      "eval step --\n",
      "\n",
      "Step 312: val_rewards = -0.05915828346974693 | baseline_reward = -0.2316945935310149\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 313: last loss = 0.11951\n",
      "eval step --\n",
      "\n",
      "Step 313: val_rewards = 0.5123189673154647 | baseline_reward = 0.8223658920534244\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 314: last loss = 0.12235\n",
      "eval step --\n",
      "\n",
      "Step 314: val_rewards = 0.4026920776767284 | baseline_reward = 0.5671239306855195\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 315: last loss = 1.30590\n",
      "eval step --\n",
      "\n",
      "Step 315: val_rewards = 0.14760151717573808 | baseline_reward = 0.1474706326515393\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 316: last loss = 0.18847\n",
      "eval step --\n",
      "\n",
      "Step 316: val_rewards = 0.21375071583595848 | baseline_reward = 0.37677520511393603\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 317: last loss = 1.42462\n",
      "eval step --\n",
      "\n",
      "Step 317: val_rewards = 0.3089646890953897 | baseline_reward = 0.304900303834946\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 318: last loss = 0.39812\n",
      "eval step --\n",
      "\n",
      "Step 318: val_rewards = 0.16687459581821196 | baseline_reward = 0.4170299714102012\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 319: last loss = 0.74244\n",
      "eval step --\n",
      "\n",
      "Step 319: val_rewards = 0.366638238773861 | baseline_reward = 0.7659950764657154\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 320: last loss = 0.15624\n",
      "eval step --\n",
      "\n",
      "Step 320: val_rewards = 0.7075408935845534 | baseline_reward = 0.8361679075437747\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 321: last loss = -0.12310\n",
      "eval step --\n",
      "\n",
      "Step 321: val_rewards = 0.7370344262067182 | baseline_reward = 0.9440461755678171\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 322: last loss = 0.75543\n",
      "eval step --\n",
      "\n",
      "Step 322: val_rewards = -0.045577567700187324 | baseline_reward = -0.13266265820572704\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 323: last loss = 1.42246\n",
      "eval step --\n",
      "\n",
      "Step 323: val_rewards = 0.04812068065853502 | baseline_reward = -0.016511857148819385\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 324: last loss = 0.37139\n",
      "eval step --\n",
      "\n",
      "Step 324: val_rewards = 0.33039910355229435 | baseline_reward = 0.8254197826811209\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 325: last loss = 0.13222\n",
      "eval step --\n",
      "\n",
      "Step 325: val_rewards = 0.08070734279541208 | baseline_reward = -0.02608481350295613\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 326: last loss = 0.10468\n",
      "eval step --\n",
      "\n",
      "Step 326: val_rewards = 0.5134249085875376 | baseline_reward = 0.8223658920534244\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 327: last loss = 0.14161\n",
      "eval step --\n",
      "\n",
      "Step 327: val_rewards = 0.051602772408992775 | baseline_reward = 0.03231802212210823\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 328: last loss = 0.12709\n",
      "eval step --\n",
      "\n",
      "Step 328: val_rewards = 0.042976768470738155 | baseline_reward = -0.09904704338521142\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 329: last loss = 0.13683\n",
      "eval step --\n",
      "\n",
      "Step 329: val_rewards = 0.21681047777449639 | baseline_reward = 0.23027423595052635\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 330: last loss = 0.13113\n",
      "eval step --\n",
      "\n",
      "Step 330: val_rewards = 0.06075802139039274 | baseline_reward = -0.003409516596186424\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 331: last loss = 0.14670\n",
      "eval step --\n",
      "\n",
      "Step 331: val_rewards = 0.28363608418549036 | baseline_reward = 0.5813186457512338\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 332: last loss = 0.35869\n",
      "eval step --\n",
      "\n",
      "Step 332: val_rewards = 0.00901415932456989 | baseline_reward = -0.06794982988207018\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 333: last loss = 0.19953\n",
      "eval step --\n",
      "\n",
      "Step 333: val_rewards = 0.29680917380648913 | baseline_reward = 0.6966527538884659\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 334: last loss = 1.07604\n",
      "eval step --\n",
      "\n",
      "Step 334: val_rewards = -0.017016619215331714 | baseline_reward = -0.19178819050120557\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 335: last loss = 0.15625\n",
      "eval step --\n",
      "\n",
      "Step 335: val_rewards = 0.16599298383361955 | baseline_reward = 0.4170299714102012\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 336: last loss = 1.10076\n",
      "eval step --\n",
      "\n",
      "Step 336: val_rewards = 0.44313199391753516 | baseline_reward = 0.5899313153949148\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 337: last loss = 0.28448\n",
      "eval step --\n",
      "\n",
      "Step 337: val_rewards = 0.18337403259520701 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 338: last loss = 0.18208\n",
      "eval step --\n",
      "\n",
      "Step 338: val_rewards = 0.0006877889275582746 | baseline_reward = -0.16229108079929988\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 339: last loss = 0.12223\n",
      "eval step --\n",
      "\n",
      "Step 339: val_rewards = -0.04431960548360012 | baseline_reward = -0.207119952210963\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 340: last loss = -0.02532\n",
      "eval step --\n",
      "\n",
      "Step 340: val_rewards = 0.03818538338872326 | baseline_reward = 0.007091603586160839\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 341: last loss = 0.05129\n",
      "eval step --\n",
      "\n",
      "Step 341: val_rewards = 0.3972623763828025 | baseline_reward = 0.6662548797360394\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 342: last loss = 0.81450\n",
      "eval step --\n",
      "\n",
      "Step 342: val_rewards = 0.42201996257312135 | baseline_reward = 0.6588783630444365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 343: last loss = 0.41914\n",
      "eval step --\n",
      "\n",
      "Step 343: val_rewards = 0.1979034898399175 | baseline_reward = 0.38995650003014914\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 344: last loss = 0.88373\n",
      "eval step --\n",
      "\n",
      "Step 344: val_rewards = 0.14771376280362358 | baseline_reward = 0.1474706326515393\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 345: last loss = 0.16746\n",
      "eval step --\n",
      "\n",
      "Step 345: val_rewards = 0.1492552702149776 | baseline_reward = 0.3727889768350506\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 346: last loss = 0.83726\n",
      "eval step --\n",
      "\n",
      "Step 346: val_rewards = 0.1308621552533294 | baseline_reward = 0.16603698525017901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 347: last loss = 0.17146\n",
      "eval step --\n",
      "\n",
      "Step 347: val_rewards = 0.28649115346033827 | baseline_reward = 0.2813440077057417\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 348: last loss = 0.17972\n",
      "eval step --\n",
      "\n",
      "Step 348: val_rewards = 0.14280041573084415 | baseline_reward = 0.19092567740642546\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 349: last loss = 0.23988\n",
      "eval step --\n",
      "\n",
      "Step 349: val_rewards = 0.25506962021577734 | baseline_reward = 0.5828692064032645\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 350: last loss = 1.23063\n",
      "eval step --\n",
      "\n",
      "Step 350: val_rewards = 0.44188808302040566 | baseline_reward = 0.7047432984001528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 351: last loss = -0.13502\n",
      "eval step --\n",
      "\n",
      "Step 351: val_rewards = 0.336887693148344 | baseline_reward = 0.4202919897083644\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 352: last loss = 0.19820\n",
      "eval step --\n",
      "\n",
      "Step 352: val_rewards = 0.1942466592164478 | baseline_reward = 0.45225390939114807\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 353: last loss = 0.56837\n",
      "eval step --\n",
      "\n",
      "Step 353: val_rewards = -0.018807582505900106 | baseline_reward = -0.1794920735327776\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 354: last loss = 1.00337\n",
      "eval step --\n",
      "\n",
      "Step 354: val_rewards = 0.3068552684921653 | baseline_reward = 0.4890891992614051\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 355: last loss = 1.09108\n",
      "eval step --\n",
      "\n",
      "Step 355: val_rewards = 0.15901365446663465 | baseline_reward = 0.30276011091662647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 356: last loss = 0.09127\n",
      "eval step --\n",
      "\n",
      "Step 356: val_rewards = 0.15134643908028675 | baseline_reward = 0.4977655979174772\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 357: last loss = 0.51928\n",
      "eval step --\n",
      "\n",
      "Step 357: val_rewards = 0.30419830654964164 | baseline_reward = 0.585014475106025\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 358: last loss = 0.32911\n",
      "eval step --\n",
      "\n",
      "Step 358: val_rewards = 0.1851861426973046 | baseline_reward = 0.38540356820892374\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 359: last loss = 0.32972\n",
      "eval step --\n",
      "\n",
      "Step 359: val_rewards = 0.4236710895190383 | baseline_reward = 0.6588783630444365\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 360: last loss = 0.17489\n",
      "eval step --\n",
      "\n",
      "Step 360: val_rewards = 0.7601239245161435 | baseline_reward = 0.6767681972226866\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 361: last loss = 1.01301\n",
      "eval step --\n",
      "\n",
      "Step 361: val_rewards = 0.48594902916355787 | baseline_reward = 0.709124352651483\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 362: last loss = 0.22087\n",
      "eval step --\n",
      "\n",
      "Step 362: val_rewards = 0.12364588127771613 | baseline_reward = 0.3656101919665796\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 363: last loss = 0.59158\n",
      "eval step --\n",
      "\n",
      "Step 363: val_rewards = 0.011990561857609171 | baseline_reward = -0.12465234709331634\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 364: last loss = 0.97475\n",
      "eval step --\n",
      "\n",
      "Step 364: val_rewards = 0.1694123143422575 | baseline_reward = 0.3546388738531832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 365: last loss = 0.19475\n",
      "eval step --\n",
      "\n",
      "Step 365: val_rewards = 0.07681117391723762 | baseline_reward = -0.06534074040975456\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 366: last loss = 0.59433\n",
      "eval step --\n",
      "\n",
      "Step 366: val_rewards = 0.016162598095569514 | baseline_reward = -0.07733072873150211\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 367: last loss = 0.18804\n",
      "eval step --\n",
      "\n",
      "Step 367: val_rewards = 0.07030379041419409 | baseline_reward = 0.046967505744021296\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 368: last loss = 0.91797\n",
      "eval step --\n",
      "\n",
      "Step 368: val_rewards = 0.2663340294228644 | baseline_reward = 0.4486967135653549\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 369: last loss = 0.27560\n",
      "eval step --\n",
      "\n",
      "Step 369: val_rewards = 0.0752523247778155 | baseline_reward = 0.07414195517992758\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 370: last loss = 0.70277\n",
      "eval step --\n",
      "\n",
      "Step 370: val_rewards = 0.09988603107504453 | baseline_reward = 0.12380159765707535\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 371: last loss = 1.14375\n",
      "eval step --\n",
      "\n",
      "Step 371: val_rewards = 0.4523019415971303 | baseline_reward = 0.5464799477140734\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 372: last loss = 0.90595\n",
      "eval step --\n",
      "\n",
      "Step 372: val_rewards = 0.34333032339479164 | baseline_reward = 0.608802754917565\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 373: last loss = 1.40899\n",
      "eval step --\n",
      "\n",
      "Step 373: val_rewards = 0.5164834668041285 | baseline_reward = 0.7881221759723845\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 374: last loss = -0.10176\n",
      "eval step --\n",
      "\n",
      "Step 374: val_rewards = 0.1579017515429512 | baseline_reward = 0.23321706193745353\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 375: last loss = 0.21631\n",
      "eval step --\n",
      "\n",
      "Step 375: val_rewards = 0.3594312630881552 | baseline_reward = 0.5661528162848493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 376: last loss = 0.19610\n",
      "eval step --\n",
      "\n",
      "Step 376: val_rewards = 0.2929693128056163 | baseline_reward = 0.5712652447171785\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 377: last loss = 0.22196\n",
      "eval step --\n",
      "\n",
      "Step 377: val_rewards = 0.08719012113738925 | baseline_reward = 0.13219777075656255\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 378: last loss = 0.02683\n",
      "eval step --\n",
      "\n",
      "Step 378: val_rewards = 0.007636787028258403 | baseline_reward = -0.12004309054607247\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 379: last loss = 0.73619\n",
      "eval step --\n",
      "\n",
      "Step 379: val_rewards = -0.017612393412564948 | baseline_reward = -0.1779046340768385\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 380: last loss = 2.34601\n",
      "eval step --\n",
      "\n",
      "Step 380: val_rewards = 0.3767358157665843 | baseline_reward = 0.66855901561777\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 381: last loss = 0.29392\n",
      "eval step --\n",
      "\n",
      "Step 381: val_rewards = 0.018460000450154147 | baseline_reward = -0.09351335608180286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 382: last loss = 0.78988\n",
      "eval step --\n",
      "\n",
      "Step 382: val_rewards = -0.05309292089304551 | baseline_reward = -0.24620996402409803\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 383: last loss = 0.36447\n",
      "eval step --\n",
      "\n",
      "Step 383: val_rewards = 0.4437411880719275 | baseline_reward = 0.7181084330239024\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 384: last loss = 1.08194\n",
      "eval step --\n",
      "\n",
      "Step 384: val_rewards = 0.1518934255976955 | baseline_reward = 0.40153176897500614\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 385: last loss = 0.20066\n",
      "eval step --\n",
      "\n",
      "Step 385: val_rewards = -0.04512429422735673 | baseline_reward = -0.1440646095582647\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 386: last loss = 0.23236\n",
      "eval step --\n",
      "\n",
      "Step 386: val_rewards = -0.025800910655774154 | baseline_reward = -0.2024200961693741\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 387: last loss = 0.33882\n",
      "eval step --\n",
      "\n",
      "Step 387: val_rewards = 0.22620095250982106 | baseline_reward = 0.387986503372861\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 388: last loss = 0.61073\n",
      "eval step --\n",
      "\n",
      "Step 388: val_rewards = 0.14786409056528615 | baseline_reward = 0.1888250569240263\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 389: last loss = 0.00708\n",
      "eval step --\n",
      "\n",
      "Step 389: val_rewards = 0.3020392097626569 | baseline_reward = 0.3295727787766805\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 390: last loss = 0.95617\n",
      "eval step --\n",
      "\n",
      "Step 390: val_rewards = 0.05812366860604333 | baseline_reward = -0.04421807519264951\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 391: last loss = 0.14761\n",
      "eval step --\n",
      "\n",
      "Step 391: val_rewards = 0.3258050231188228 | baseline_reward = 0.6647957189755535\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 392: last loss = 0.70678\n",
      "eval step --\n",
      "\n",
      "Step 392: val_rewards = 0.02448412230336462 | baseline_reward = -0.05038019433944224\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 393: last loss = 1.20046\n",
      "eval step --\n",
      "\n",
      "Step 393: val_rewards = 0.38730349049793883 | baseline_reward = 0.5132182369572957\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 394: last loss = 0.16389\n",
      "eval step --\n",
      "\n",
      "Step 394: val_rewards = 0.1316159299918788 | baseline_reward = 0.18117789420446537\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 395: last loss = 0.81105\n",
      "eval step --\n",
      "\n",
      "Step 395: val_rewards = 0.10293174707715037 | baseline_reward = 0.14527408995758503\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 396: last loss = 0.11143\n",
      "eval step --\n",
      "\n",
      "Step 396: val_rewards = 0.25013851583946856 | baseline_reward = 0.3739355473845528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 397: last loss = 0.32628\n",
      "eval step --\n",
      "\n",
      "Step 397: val_rewards = 0.30551961658461646 | baseline_reward = 0.7609543791645151\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 398: last loss = 0.25382\n",
      "eval step --\n",
      "\n",
      "Step 398: val_rewards = -0.0009039971617915989 | baseline_reward = -0.07456214172507493\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 399: last loss = 0.15330\n",
      "eval step --\n",
      "\n",
      "Step 399: val_rewards = 0.1035100225710276 | baseline_reward = 0.14934381229045454\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 400: last loss = 0.14654\n",
      "eval step --\n",
      "\n",
      "Step 400: val_rewards = 0.3595328949260215 | baseline_reward = 0.6413037700199038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 401: last loss = 0.15966\n",
      "eval step --\n",
      "\n",
      "Step 401: val_rewards = 0.2158814735851578 | baseline_reward = 0.36685312996470437\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 402: last loss = 1.01382\n",
      "eval step --\n",
      "\n",
      "Step 402: val_rewards = 0.0515671417116642 | baseline_reward = 0.03231802212210823\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 403: last loss = 0.13978\n",
      "eval step --\n",
      "\n",
      "Step 403: val_rewards = 0.03938748222996412 | baseline_reward = 0.008074204506293097\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 404: last loss = 0.15625\n",
      "eval step --\n",
      "\n",
      "Step 404: val_rewards = 0.28116594463104105 | baseline_reward = 0.5243451811803399\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 405: last loss = 0.30151\n",
      "eval step --\n",
      "\n",
      "Step 405: val_rewards = 0.11443615249280363 | baseline_reward = 0.12561727019884172\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 406: last loss = 1.07162\n",
      "eval step --\n",
      "\n",
      "Step 406: val_rewards = 0.4510279142369011 | baseline_reward = 0.5464799477140734\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 407: last loss = 0.00091\n",
      "eval step --\n",
      "\n",
      "Step 407: val_rewards = 0.2178573746703604 | baseline_reward = 0.23027423595052635\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 408: last loss = 0.19961\n",
      "eval step --\n",
      "\n",
      "Step 408: val_rewards = 0.8400312920250259 | baseline_reward = 1.0688939544432159\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 409: last loss = 0.95673\n",
      "eval step --\n",
      "\n",
      "Step 409: val_rewards = 0.5437282968762179 | baseline_reward = 0.6898221717064092\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 410: last loss = 0.19130\n",
      "eval step --\n",
      "\n",
      "Step 410: val_rewards = 0.3524921768247703 | baseline_reward = 0.6220038779555153\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 411: last loss = 0.13621\n",
      "eval step --\n",
      "\n",
      "Step 411: val_rewards = 0.20893271633139643 | baseline_reward = 0.22777368070532983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 412: last loss = 0.17727\n",
      "eval step --\n",
      "\n",
      "Step 412: val_rewards = 0.020900128633872098 | baseline_reward = -0.06909818671718444\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 413: last loss = 0.21481\n",
      "eval step --\n",
      "\n",
      "Step 413: val_rewards = 0.17737716699688616 | baseline_reward = 0.2992559300978452\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 414: last loss = 1.87417\n",
      "eval step --\n",
      "\n",
      "Step 414: val_rewards = 0.04105585347830739 | baseline_reward = -0.011238943939511286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 415: last loss = 0.33153\n",
      "eval step --\n",
      "\n",
      "Step 415: val_rewards = 0.2504708855560725 | baseline_reward = 0.3739355473845528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 416: last loss = 0.19279\n",
      "eval step --\n",
      "\n",
      "Step 416: val_rewards = 0.31335138523851647 | baseline_reward = 0.3090145715378005\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 417: last loss = 0.02857\n",
      "eval step --\n",
      "\n",
      "Step 417: val_rewards = 0.004592331702532955 | baseline_reward = -0.0484675220347934\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 418: last loss = 0.81446\n",
      "eval step --\n",
      "\n",
      "Step 418: val_rewards = 0.3189603692699137 | baseline_reward = 0.6755254114198141\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 419: last loss = 0.17849\n",
      "eval step --\n",
      "\n",
      "Step 419: val_rewards = 0.1643324245985339 | baseline_reward = 0.198247059029331\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 420: last loss = 0.97533\n",
      "eval step --\n",
      "\n",
      "Step 420: val_rewards = 0.0010552441530841514 | baseline_reward = -0.09309637816407894\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 421: last loss = 0.22037\n",
      "eval step --\n",
      "\n",
      "Step 421: val_rewards = 0.36060876563527217 | baseline_reward = 0.6413037700199038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 422: last loss = -0.02632\n",
      "eval step --\n",
      "\n",
      "Step 422: val_rewards = 0.1533965699012372 | baseline_reward = 0.23888400621369194\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 423: last loss = 0.32029\n",
      "eval step --\n",
      "\n",
      "Step 423: val_rewards = 0.2915360602977174 | baseline_reward = 0.6430042584281015\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 424: last loss = 0.39900\n",
      "eval step --\n",
      "\n",
      "Step 424: val_rewards = 0.32449404552495403 | baseline_reward = 0.4917295336353529\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 425: last loss = 0.74306\n",
      "eval step --\n",
      "\n",
      "Step 425: val_rewards = 0.48545663478297063 | baseline_reward = 0.709124352651483\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 426: last loss = 0.29066\n",
      "eval step --\n",
      "\n",
      "Step 426: val_rewards = 0.07380628745686525 | baseline_reward = -0.07936404126055921\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 427: last loss = 0.67268\n",
      "eval step --\n",
      "\n",
      "Step 427: val_rewards = 0.3360224877888315 | baseline_reward = 0.4202919897083644\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 428: last loss = 0.12072\n",
      "eval step --\n",
      "\n",
      "Step 428: val_rewards = 0.3427257440414163 | baseline_reward = 0.5933428545608107\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 429: last loss = 0.03118\n",
      "eval step --\n",
      "\n",
      "Step 429: val_rewards = 0.03594578658852507 | baseline_reward = -0.11179834278066905\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 430: last loss = 0.62712\n",
      "eval step --\n",
      "\n",
      "Step 430: val_rewards = 0.44912370450709643 | baseline_reward = 0.6961346106700829\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 431: last loss = 0.36400\n",
      "eval step --\n",
      "\n",
      "Step 431: val_rewards = 0.4043402354168242 | baseline_reward = 0.5178446205859352\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 432: last loss = 0.14926\n",
      "eval step --\n",
      "\n",
      "Step 432: val_rewards = 0.3174659945530005 | baseline_reward = 0.7277122264121534\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 433: last loss = 0.35538\n",
      "eval step --\n",
      "\n",
      "Step 433: val_rewards = 0.08052911689624166 | baseline_reward = 0.07861588816807638\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 434: last loss = 1.22043\n",
      "eval step --\n",
      "\n",
      "Step 434: val_rewards = 0.01807526756275978 | baseline_reward = -0.09351335608180286\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 435: last loss = 1.62217\n",
      "eval step --\n",
      "\n",
      "Step 435: val_rewards = 0.07598244073617198 | baseline_reward = -0.010577587927711345\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 436: last loss = 0.17006\n",
      "eval step --\n",
      "\n",
      "Step 436: val_rewards = 0.13202727931455008 | baseline_reward = 0.16603698525017901\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 437: last loss = 1.78060\n",
      "eval step --\n",
      "\n",
      "Step 437: val_rewards = 0.8813761379296562 | baseline_reward = 1.0903738402242533\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 438: last loss = 0.49085\n",
      "eval step --\n",
      "\n",
      "Step 438: val_rewards = 0.030135540207949955 | baseline_reward = -0.040765858608859395\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 439: last loss = 1.78054\n",
      "eval step --\n",
      "\n",
      "Step 439: val_rewards = 0.4393286984616206 | baseline_reward = 0.6827058009019478\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 440: last loss = 0.78229\n",
      "eval step --\n",
      "\n",
      "Step 440: val_rewards = 0.2798536016295011 | baseline_reward = 0.4874748681648409\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 441: last loss = 0.09781\n",
      "eval step --\n",
      "\n",
      "Step 441: val_rewards = 0.27013907032523476 | baseline_reward = 0.2759578983022423\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 442: last loss = 0.83562\n",
      "eval step --\n",
      "\n",
      "Step 442: val_rewards = 0.18420580417571095 | baseline_reward = 0.1968252865225056\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 443: last loss = 0.04453\n",
      "eval step --\n",
      "\n",
      "Step 443: val_rewards = 0.18436048778055192 | baseline_reward = 0.5962754651604965\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 444: last loss = 0.13668\n",
      "eval step --\n",
      "\n",
      "Step 444: val_rewards = 0.30480480701165963 | baseline_reward = 0.7609543791645151\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 445: last loss = 0.15449\n",
      "eval step --\n",
      "\n",
      "Step 445: val_rewards = 0.1202803253441828 | baseline_reward = 0.14062813107180086\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 446: last loss = 0.20512\n",
      "eval step --\n",
      "\n",
      "Step 446: val_rewards = 0.3305068158438293 | baseline_reward = 0.6682625788014249\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 447: last loss = 0.42152\n",
      "eval step --\n",
      "\n",
      "Step 447: val_rewards = 0.36933778583207083 | baseline_reward = 0.685461084543975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 448: last loss = 0.16228\n",
      "eval step --\n",
      "\n",
      "Step 448: val_rewards = 0.12060053800373484 | baseline_reward = 0.14062813107180086\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 449: last loss = 0.10680\n",
      "eval step --\n",
      "\n",
      "Step 449: val_rewards = 0.10361220873565644 | baseline_reward = 0.14934381229045454\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 450: last loss = 0.89058\n",
      "eval step --\n",
      "\n",
      "Step 450: val_rewards = 0.06237612810180055 | baseline_reward = -0.06670678504242841\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 451: last loss = 0.16880\n",
      "eval step --\n",
      "\n",
      "Step 451: val_rewards = 0.3528187389359656 | baseline_reward = 0.5394294703705088\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 452: last loss = 0.14116\n",
      "eval step --\n",
      "\n",
      "Step 452: val_rewards = 0.35808677877531203 | baseline_reward = 0.6413037700199038\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 453: last loss = 0.10348\n",
      "eval step --\n",
      "\n",
      "Step 453: val_rewards = 0.03938611346357507 | baseline_reward = -0.02399293385146675\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 454: last loss = 0.16646\n",
      "eval step --\n",
      "\n",
      "Step 454: val_rewards = -0.012186624648482093 | baseline_reward = -0.2019959841622891\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 455: last loss = 0.48084\n",
      "eval step --\n",
      "\n",
      "Step 455: val_rewards = 0.3591442584404508 | baseline_reward = 0.7682499609317\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 456: last loss = 0.16895\n",
      "eval step --\n",
      "\n",
      "Step 456: val_rewards = 0.6322785747512537 | baseline_reward = 0.7221405546661565\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 457: last loss = 0.17283\n",
      "eval step --\n",
      "\n",
      "Step 457: val_rewards = 0.1087155338392068 | baseline_reward = 0.14699959597029763\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 458: last loss = -0.12145\n",
      "eval step --\n",
      "\n",
      "Step 458: val_rewards = 0.369101907211558 | baseline_reward = 0.685461084543975\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 459: last loss = 0.06850\n",
      "eval step --\n",
      "\n",
      "Step 459: val_rewards = 0.1503350017604854 | baseline_reward = 0.3727889768350506\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 460: last loss = -0.04517\n",
      "eval step --\n",
      "\n",
      "Step 460: val_rewards = 0.24989621486184893 | baseline_reward = 0.3739355473845528\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 461: last loss = 0.16542\n",
      "eval step --\n",
      "\n",
      "Step 461: val_rewards = 0.01735074222003119 | baseline_reward = -0.07733072873150211\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 462: last loss = 1.09633\n",
      "eval step --\n",
      "\n",
      "Step 462: val_rewards = 0.23622658851185604 | baseline_reward = 0.36576566373167874\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 463: last loss = 0.62982\n",
      "eval step --\n",
      "\n",
      "Step 463: val_rewards = 0.17739851487766511 | baseline_reward = 0.2992559300978452\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 464: last loss = 0.20577\n",
      "eval step --\n",
      "\n",
      "Step 464: val_rewards = 0.006504819681681213 | baseline_reward = -0.13239103663513663\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 465: last loss = 0.14727\n",
      "eval step --\n",
      "\n",
      "Step 465: val_rewards = 0.33634182789266076 | baseline_reward = 0.4202919897083644\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 466: last loss = -0.12099\n",
      "eval step --\n",
      "\n",
      "Step 466: val_rewards = 0.1866654197556253 | baseline_reward = 0.21159166371931457\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 467: last loss = 0.77409\n",
      "eval step --\n",
      "\n",
      "Step 467: val_rewards = 0.10382836957957949 | baseline_reward = 0.14934381229045454\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 468: last loss = -0.07718\n",
      "eval step --\n",
      "\n",
      "Step 468: val_rewards = 0.5332928157584118 | baseline_reward = 0.9026240930820785\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 469: last loss = 0.74713\n",
      "eval step --\n",
      "\n",
      "Step 469: val_rewards = 0.10362885243482134 | baseline_reward = 0.11731911556164225\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 470: last loss = 0.27524\n",
      "eval step --\n",
      "\n",
      "Step 470: val_rewards = 0.06853349821827809 | baseline_reward = -0.06875045936192646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 471: last loss = 0.99255\n",
      "eval step --\n",
      "\n",
      "Step 471: val_rewards = 0.335110797390817 | baseline_reward = 0.4202919897083644\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 472: last loss = 0.97655\n",
      "eval step --\n",
      "\n",
      "Step 472: val_rewards = 0.4593262590226524 | baseline_reward = 0.7056785084409496\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 473: last loss = -0.02588\n",
      "eval step --\n",
      "\n",
      "Step 473: val_rewards = 0.04065732921040179 | baseline_reward = 0.029112935778557482\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 474: last loss = 0.60191\n",
      "eval step --\n",
      "\n",
      "Step 474: val_rewards = 0.2510427039642678 | baseline_reward = 0.44352376289444034\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 475: last loss = 0.14808\n",
      "eval step --\n",
      "\n",
      "Step 475: val_rewards = 0.001772946910048814 | baseline_reward = -0.16060612298022475\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 476: last loss = 0.18066\n",
      "eval step --\n",
      "\n",
      "Step 476: val_rewards = 0.22099707827517398 | baseline_reward = 0.2124053309131323\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 477: last loss = 0.02767\n",
      "eval step --\n",
      "\n",
      "Step 477: val_rewards = 0.10455332211246238 | baseline_reward = 0.14934381229045454\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 478: last loss = 0.27087\n",
      "eval step --\n",
      "\n",
      "Step 478: val_rewards = 0.20891744702188664 | baseline_reward = 0.43942054660951957\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 479: last loss = 0.93003\n",
      "eval step --\n",
      "\n",
      "Step 479: val_rewards = 0.30208404528459404 | baseline_reward = 0.5032045996297826\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 480: last loss = 1.02594\n",
      "eval step --\n",
      "\n",
      "Step 480: val_rewards = 0.37182940756231625 | baseline_reward = 0.6685638503863266\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 481: last loss = 0.02574\n",
      "eval step --\n",
      "\n",
      "Step 481: val_rewards = 0.3431537810236927 | baseline_reward = 0.5933428545608107\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 482: last loss = 0.16640\n",
      "eval step --\n",
      "\n",
      "Step 482: val_rewards = 0.11227861653876851 | baseline_reward = 0.14562644791279072\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 483: last loss = 0.88375\n",
      "eval step --\n",
      "\n",
      "Step 483: val_rewards = 0.07218979422613679 | baseline_reward = -0.06703971028350392\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 484: last loss = 0.17934\n",
      "eval step --\n",
      "\n",
      "Step 484: val_rewards = 0.32018736720172053 | baseline_reward = 0.6755254114198141\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 485: last loss = 0.31590\n",
      "eval step --\n",
      "\n",
      "Step 485: val_rewards = 0.5148238710838957 | baseline_reward = 0.7881221759723845\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 486: last loss = 0.16921\n",
      "eval step --\n",
      "\n",
      "Step 486: val_rewards = 0.16112738973881452 | baseline_reward = 0.2190904192544018\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 487: last loss = 0.82439\n",
      "eval step --\n",
      "\n",
      "Step 487: val_rewards = 0.06995835196827672 | baseline_reward = -0.06875045936192646\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 488: last loss = 0.89232\n",
      "eval step --\n",
      "\n",
      "Step 488: val_rewards = 0.7592407458861762 | baseline_reward = 1.1560646431741626\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 489: last loss = 0.15340\n",
      "eval step --\n",
      "\n",
      "Step 489: val_rewards = 0.005190029401988066 | baseline_reward = -0.07213108081473303\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 490: last loss = 0.16585\n",
      "eval step --\n",
      "\n",
      "Step 490: val_rewards = 0.12153874895163418 | baseline_reward = 0.3538580199375594\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 491: last loss = 0.11588\n",
      "eval step --\n",
      "\n",
      "Step 491: val_rewards = 0.2874772748755768 | baseline_reward = 0.4284104212671813\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 492: last loss = 0.32746\n",
      "eval step --\n",
      "\n",
      "Step 492: val_rewards = 0.20768127215158116 | baseline_reward = 0.22777368070532983\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 493: last loss = 0.19257\n",
      "eval step --\n",
      "\n",
      "Step 493: val_rewards = 0.11860525167756387 | baseline_reward = 0.1481721159355382\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 494: last loss = 0.12649\n",
      "eval step --\n",
      "\n",
      "Step 494: val_rewards = 0.2539349607737994 | baseline_reward = 0.5925582127279282\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 495: last loss = 0.15365\n",
      "eval step --\n",
      "\n",
      "Step 495: val_rewards = 0.3969831374456416 | baseline_reward = 0.6662548797360394\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 496: last loss = 0.26174\n",
      "eval step --\n",
      "\n",
      "Step 496: val_rewards = -0.019926645014517116 | baseline_reward = -0.2125110668214937\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 497: last loss = -0.09964\n",
      "eval step --\n",
      "\n",
      "Step 497: val_rewards = -0.02946064018335896 | baseline_reward = -0.12430381157482846\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 498: last loss = 0.17587\n",
      "eval step --\n",
      "\n",
      "Step 498: val_rewards = 0.1528630508468618 | baseline_reward = 0.20296428271805433\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 499: last loss = 0.66221\n",
      "eval step --\n",
      "\n",
      "Step 499: val_rewards = 0.23898912077388507 | baseline_reward = 0.5495212447107057\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 15:50:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = f\"v4_training_{tid}\") as run:\n",
    "    params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"train_step\": train_step,\n",
    "            \"eval_step\": eval_step,\n",
    "            \"metric_function\": 'sharpe',\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \n",
    "            \"symbol_universe\" : symbol_universe,\n",
    "            \"feature_set\" : feature_set,\n",
    "            \"d_model\" : d_model,\n",
    "            \"nheads\" : nheads,\n",
    "            \"num_transformer_layers\" : num_transformer_layers,\n",
    "\n",
    "            \"episode_duration\" : 12,    \n",
    "            \"holding_period\" : 1,\n",
    "            \"train_test_split\" : 0.8,\n",
    "            \"symbol_universe\" : list(symbol_universe),\n",
    "            \"feature_set\" : feature_set,\n",
    "\n",
    "            \"training_data_path\" : data_path\n",
    "\n",
    "        }\n",
    "    # Log training parameters.\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    portfolio_constructor = PortfolioConstructor(\n",
    "        device = device,\n",
    "        symbol_universe= params['symbol_universe'],\n",
    "        num_features= len(params['feature_set']),\n",
    "        d_model = params['d_model'],\n",
    "        nheads = params['nheads'],\n",
    "        num_transformer_layers = params['num_transformer_layers'],\n",
    "    )\n",
    "\n",
    "    market_env = MarketEnvironment(\n",
    "        device = device,\n",
    "        data_path = data_path,\n",
    "        holding_period = params['holding_period'],\n",
    "        episode_duration = params['episode_duration'],\n",
    "        train_test_split = params['train_test_split'],\n",
    "        symbol_universe = params['symbol_universe'],\n",
    "        feature_set = params['feature_set']\n",
    "        )\n",
    "\n",
    "    portfolio_constructor.cuda()\n",
    "    portfolio_constructor.train()\n",
    "    market_env.reset(mode = \"train\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(portfolio_constructor.parameters(), lr = learning_rate)\n",
    "\n",
    "    max_reward = -1\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        is_end = False\n",
    "        returns = []\n",
    "        tran_costs = []\n",
    "        nlls = []\n",
    "        all_allocations = []\n",
    "\n",
    "        market_env.reset(mode = \"train\", transaction_cost= 1e-7)\n",
    "        state = market_env.get_state()\n",
    "        \n",
    "        while not is_end:\n",
    "            symbol_idx, allocations = portfolio_constructor(state)\n",
    "            state, return_, _, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "            all_allocations.append(allocations)\n",
    "            returns.append(return_)\n",
    "            tran_costs.append(tran_cost)\n",
    "            nlls.append(torch.log(allocations.abs()/2 + 1e-9))\n",
    "        sharp_ratio = sharp_ratio_(returns, tran_costs)\n",
    "\n",
    "        loss = sharp_ratio\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        if (episode + 1) % train_step == 0:\n",
    "\n",
    "                    print(\"-------------------------------------\")\n",
    "                    print(\"training model --\")\n",
    "                    print('Step {}: last loss = {:.5f}\\r'.format(episode, loss), end='')\n",
    "                    print()\n",
    "                    mlflow.log_metric(\"train loss\", f\"{loss:2f}\", step=episode)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = 0\n",
    "                    \n",
    "        if (episode + 1) % eval_step == 0:\n",
    "            print(\"eval step --\")\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                portfolio_constructor.eval()\n",
    "                reward_val, baseline_val, portfolio_constructor = evaluate(portfolio_constructor, market_env)\n",
    "                portfolio_constructor.train()\n",
    "\n",
    "                print('Step {}: val_rewards = {} | baseline_reward = {}'.format(episode, reward_val, baseline_val))\n",
    "                mlflow.log_metric(\"eval_sharpe\", f\"{reward_val:2f}\", step=episode)\n",
    "                mlflow.log_metric(\"baseline_sharpe\", f\"{baseline_val:2f}\", step=episode)\n",
    "\n",
    "                if max_reward < reward_val:\n",
    "                    max_reward = reward_val\n",
    "\n",
    "                    print(\"*** found better model ***\")\n",
    "                print()\n",
    "    mlflow.pytorch.log_model(portfolio_constructor, f\"portfolio_constructor_{tid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0111, 0.0111, 0.0099, 0.0110, 0.0101, 0.0100, 0.0099, 0.0100, 0.0099,\n",
       "         0.0100, 0.0098, 0.0099, 0.0110, 0.0100, 0.0099, 0.0100, 0.0101, 0.0099,\n",
       "         0.0100, 0.0111, 0.0101, 0.0099, 0.0099, 0.0100, 0.0100, 0.0100, 0.0111,\n",
       "         0.0101, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100,\n",
       "         0.0099, 0.0101, 0.0100, 0.0102, 0.0099, 0.0098, 0.0100, 0.0101, 0.0100,\n",
       "         0.0100, 0.0099, 0.0100, 0.0098, 0.0101, 0.0098, 0.0100, 0.0100, 0.0098,\n",
       "         0.0100, 0.0098, 0.0110, 0.0100, 0.0112, 0.0100, 0.0100, 0.0100, 0.0111,\n",
       "         0.0101, 0.0099, 0.0100, 0.0102, 0.0100, 0.0100, 0.0101, 0.0101, 0.0100,\n",
       "         0.0100, 0.0100, 0.0101, 0.0100, 0.0100, 0.0101, 0.0101, 0.0101, 0.0100,\n",
       "         0.0100, 0.0098, 0.0101, 0.0111, 0.0099, 0.0099, 0.0100, 0.0101, 0.0101,\n",
       "         0.0101, 0.0100, 0.0099, 0.0099, 0.0099, 0.0100, 0.0099, 0.0100, 0.0101],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0099, 0.0101, 0.0100, 0.0101, 0.0100, 0.0100, 0.0100, 0.0110, 0.0100,\n",
       "         0.0101, 0.0111, 0.0099, 0.0099, 0.0100, 0.0099, 0.0099, 0.0101, 0.0100,\n",
       "         0.0100, 0.0110, 0.0099, 0.0101, 0.0110, 0.0100, 0.0101, 0.0101, 0.0100,\n",
       "         0.0099, 0.0100, 0.0099, 0.0101, 0.0099, 0.0099, 0.0100, 0.0098, 0.0112,\n",
       "         0.0099, 0.0099, 0.0101, 0.0101, 0.0101, 0.0100, 0.0100, 0.0099, 0.0110,\n",
       "         0.0099, 0.0101, 0.0099, 0.0101, 0.0098, 0.0100, 0.0100, 0.0101, 0.0098,\n",
       "         0.0099, 0.0112, 0.0099, 0.0100, 0.0102, 0.0103, 0.0101, 0.0100, 0.0111,\n",
       "         0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0099,\n",
       "         0.0100, 0.0100, 0.0100, 0.0101, 0.0100, 0.0111, 0.0101, 0.0109, 0.0100,\n",
       "         0.0101, 0.0100, 0.0101, 0.0100, 0.0100, 0.0102, 0.0100, 0.0098, 0.0099,\n",
       "         0.0102, 0.0098, 0.0099, 0.0100, 0.0101, 0.0100, 0.0101, 0.0100, 0.0098],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0099, 0.0099, 0.0099, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0109,\n",
       "         0.0099, 0.0112, 0.0100, 0.0110, 0.0100, 0.0100, 0.0099, 0.0100, 0.0101,\n",
       "         0.0098, 0.0110, 0.0101, 0.0101, 0.0100, 0.0098, 0.0100, 0.0099, 0.0100,\n",
       "         0.0099, 0.0101, 0.0102, 0.0099, 0.0100, 0.0110, 0.0099, 0.0100, 0.0109,\n",
       "         0.0100, 0.0101, 0.0101, 0.0102, 0.0098, 0.0098, 0.0099, 0.0101, 0.0100,\n",
       "         0.0101, 0.0099, 0.0099, 0.0099, 0.0102, 0.0100, 0.0100, 0.0099, 0.0099,\n",
       "         0.0099, 0.0100, 0.0101, 0.0102, 0.0099, 0.0101, 0.0097, 0.0100, 0.0112,\n",
       "         0.0099, 0.0101, 0.0101, 0.0100, 0.0099, 0.0099, 0.0101, 0.0100, 0.0099,\n",
       "         0.0100, 0.0100, 0.0101, 0.0101, 0.0099, 0.0110, 0.0100, 0.0101, 0.0100,\n",
       "         0.0102, 0.0101, 0.0099, 0.0109, 0.0101, 0.0097, 0.0099, 0.0099, 0.0099,\n",
       "         0.0100, 0.0102, 0.0102, 0.0100, 0.0100, 0.0111, 0.0099, 0.0101, 0.0100],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0101, 0.0101, 0.0100, 0.0099, 0.0098, 0.0100, 0.0101, 0.0099, 0.0102,\n",
       "         0.0099, 0.0101, 0.0102, 0.0099, 0.0100, 0.0112, 0.0109, 0.0099, 0.0101,\n",
       "         0.0099, 0.0110, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0100,\n",
       "         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0109, 0.0099, 0.0111, 0.0099,\n",
       "         0.0099, 0.0100, 0.0099, 0.0099, 0.0101, 0.0098, 0.0100, 0.0111, 0.0100,\n",
       "         0.0101, 0.0101, 0.0101, 0.0101, 0.0099, 0.0100, 0.0101, 0.0099, 0.0100,\n",
       "         0.0100, 0.0101, 0.0100, 0.0099, 0.0101, 0.0101, 0.0099, 0.0099, 0.0112,\n",
       "         0.0100, 0.0100, 0.0099, 0.0109, 0.0100, 0.0098, 0.0101, 0.0101, 0.0100,\n",
       "         0.0100, 0.0100, 0.0100, 0.0099, 0.0111, 0.0102, 0.0099, 0.0097, 0.0101,\n",
       "         0.0100, 0.0099, 0.0100, 0.0109, 0.0101, 0.0099, 0.0100, 0.0102, 0.0100,\n",
       "         0.0101, 0.0101, 0.0100, 0.0100, 0.0100, 0.0101, 0.0100, 0.0100, 0.0100],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099,\n",
       "         0.0099, 0.0099, 0.0100, 0.0100, 0.0100, 0.0110, 0.0101, 0.0100, 0.0100,\n",
       "         0.0101, 0.0111, 0.0102, 0.0100, 0.0101, 0.0100, 0.0100, 0.0101, 0.0100,\n",
       "         0.0110, 0.0099, 0.0099, 0.0099, 0.0100, 0.0099, 0.0110, 0.0101, 0.0099,\n",
       "         0.0110, 0.0100, 0.0099, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0101,\n",
       "         0.0110, 0.0099, 0.0100, 0.0099, 0.0100, 0.0101, 0.0101, 0.0098, 0.0099,\n",
       "         0.0100, 0.0099, 0.0101, 0.0098, 0.0100, 0.0102, 0.0100, 0.0101, 0.0102,\n",
       "         0.0100, 0.0100, 0.0099, 0.0110, 0.0100, 0.0101, 0.0100, 0.0101, 0.0099,\n",
       "         0.0102, 0.0100, 0.0100, 0.0101, 0.0110, 0.0102, 0.0100, 0.0098, 0.0100,\n",
       "         0.0098, 0.0109, 0.0099, 0.0099, 0.0100, 0.0100, 0.0101, 0.0102, 0.0109,\n",
       "         0.0101, 0.0100, 0.0099, 0.0099, 0.0099, 0.0100, 0.0100, 0.0099, 0.0100],\n",
       "        device='cuda:0', grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_allocations[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/11 10:47:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'portfolio-constructor-v3'.\n",
      "Created version '1' of model 'portfolio-constructor-v3'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7fc41191ac70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.pytorch.log_model(\n",
    "        pytorch_model=portfolio_constructor,\n",
    "        artifact_path = \"portfolio_constructor_{tid}\",\n",
    "        # input_example = market_env.get_random_state(),\n",
    "        registered_model_name=\"portfolio-constructor-v3\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
