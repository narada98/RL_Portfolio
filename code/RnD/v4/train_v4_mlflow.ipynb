{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PortfolioConstructor import PortfolioConstructor\n",
    "from ExchnageEnv import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda') \n",
    "    torch.get_default_device()\n",
    "    device = 'cuda'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_loc = \"/home/naradaw/dev/Charles_Schwab/data/w_features/v2/2024_11_18/2024_11_18_13_33\"\n",
    "data_path = f\"{data_base_loc}/dataset_sqs.pkl\"\n",
    "feature_set_path = f\"{data_base_loc}/feature_set.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/naradaw/dev/Charles_Schwab/data/symbol_universe/snp_unique_100_2019\", \"rb\") as fp:\n",
    "    symbol_universe = pickle.load(fp)\n",
    "    \n",
    "symbol_universe = symbol_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feature_set_path, 'rb') as f:\n",
    "    feature_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/naradaw/dev/Charles_Schwab/code/RnD/v4/mlflow_experiments/589106785306301247', creation_time=1731319215217, experiment_id='589106785306301247', last_update_time=1731319215217, lifecycle_stage='active', name='/portfolio-contructor-v4', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_tracking_uri = 'file:/home/naradaw/dev/Charles_Schwab/code/RnD/v4/mlflow_experiments'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "experiment_name = \"/portfolio-contructor-v4\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "eval_step = 1\n",
    "train_step = 1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "symbol_universe= symbol_universe\n",
    "num_features= len(feature_set)\n",
    "d_model = 88\n",
    "nheads = 1\n",
    "num_transformer_layers = 2\n",
    "\n",
    "episode_duration= 12   \n",
    "holding_period = 1\n",
    "train_test_split= 0.7\n",
    "symbol_universe = symbol_universe\n",
    "feature_set= feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol_universe = random.choices(symbol_universe, k = 20)\n",
    "# symbol_universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "sharpe ratio measures the excess return of the portfolio over the \n",
    "volatility of it -> risk adjusted performance\n",
    "'''\n",
    "\n",
    "def sharp_ratio_(rewards, tran_costs):\n",
    "\n",
    "\t# rewards = [r.detach().cpu().numpy() for r in rewards]\n",
    "\tmean = sum(rewards) / len(rewards)\n",
    "\tAt = sum(r - t for r, t in zip(rewards, tran_costs)) / len(rewards)\n",
    "\tvol = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n",
    "\tvol = vol ** 0.5\n",
    "\n",
    "\treturn (At - 1e-7) / (vol + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env):\n",
    "    is_end = False\n",
    "    rewards = []\n",
    "    baseline_returns = []\n",
    "    tran_costs = []\n",
    "    \n",
    "    env.reset(mode = \"val\")\n",
    "    state = env.get_state()\n",
    "\n",
    "    print(\"\")\n",
    "    while not is_end:\n",
    "        _, allocations = model(state)\n",
    "        state, reward, baseline_return, is_end, tran_cost = env.step(allocations)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        tran_costs.append(tran_cost)\n",
    "        baseline_returns.append(baseline_return)\n",
    "\n",
    "    sharp_ratio = sharp_ratio_(rewards, tran_costs)\n",
    "    baseline_sharp_ratio = sharp_ratio_(baseline_returns, tran_costs)\n",
    "    return sharp_ratio, baseline_sharp_ratio, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "training model --\n",
      "Step 0: last loss = -0.80523\n",
      "eval step --\n",
      "\n",
      "Step 0: val_rewards = 0.25671762542618193 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 1: last loss = -0.65647\n",
      "eval step --\n",
      "\n",
      "Step 1: val_rewards = 0.6168002710885436 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 2: last loss = -0.32497\n",
      "eval step --\n",
      "\n",
      "Step 2: val_rewards = 1.0307275962803193 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 3: last loss = -0.07299\n",
      "eval step --\n",
      "\n",
      "Step 3: val_rewards = 0.7831711473677626 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 4: last loss = -0.21348\n",
      "eval step --\n",
      "\n",
      "Step 4: val_rewards = 0.691484975030288 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 5: last loss = -0.11806\n",
      "eval step --\n",
      "\n",
      "Step 5: val_rewards = 0.16471223034090904 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 6: last loss = -0.27468\n",
      "eval step --\n",
      "\n",
      "Step 6: val_rewards = 0.2873116667024274 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 7: last loss = -1.03460\n",
      "eval step --\n",
      "\n",
      "Step 7: val_rewards = 0.15173325069655777 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 8: last loss = -0.05299\n",
      "eval step --\n",
      "\n",
      "Step 8: val_rewards = 0.14047623967990394 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 9: last loss = -0.27649\n",
      "eval step --\n",
      "\n",
      "Step 9: val_rewards = 0.13978698916011198 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 10: last loss = 0.28585\n",
      "eval step --\n",
      "\n",
      "Step 10: val_rewards = 0.29124446330204357 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 11: last loss = 0.08364\n",
      "eval step --\n",
      "\n",
      "Step 11: val_rewards = 0.34909659766579865 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 12: last loss = 0.00439\n",
      "eval step --\n",
      "\n",
      "Step 12: val_rewards = 0.18096989219894702 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 13: last loss = -1.01012\n",
      "eval step --\n",
      "\n",
      "Step 13: val_rewards = 0.20185234894213744 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 14: last loss = -0.60165\n",
      "eval step --\n",
      "\n",
      "Step 14: val_rewards = 0.23174139618587425 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 15: last loss = 0.11401\n",
      "eval step --\n",
      "\n",
      "Step 15: val_rewards = 0.2926295375259864 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 16: last loss = 0.15429\n",
      "eval step --\n",
      "\n",
      "Step 16: val_rewards = 0.43184774420918876 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 17: last loss = -0.32863\n",
      "eval step --\n",
      "\n",
      "Step 17: val_rewards = 0.3555280438873291 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 18: last loss = -0.19637\n",
      "eval step --\n",
      "\n",
      "Step 18: val_rewards = 0.23187975839069164 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 19: last loss = -0.07518\n",
      "eval step --\n",
      "\n",
      "Step 19: val_rewards = 0.24908079432354238 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 20: last loss = -0.62509\n",
      "eval step --\n",
      "\n",
      "Step 20: val_rewards = 0.3130779303412333 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 21: last loss = -0.41388\n",
      "eval step --\n",
      "\n",
      "Step 21: val_rewards = 0.3285493481946706 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 22: last loss = -0.12700\n",
      "eval step --\n",
      "\n",
      "Step 22: val_rewards = 0.3126347377436178 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 23: last loss = -0.26056\n",
      "eval step --\n",
      "\n",
      "Step 23: val_rewards = 0.40336272839937953 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 24: last loss = -0.50983\n",
      "eval step --\n",
      "\n",
      "Step 24: val_rewards = 0.41470446198209504 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 25: last loss = -0.54189\n",
      "eval step --\n",
      "\n",
      "Step 25: val_rewards = 0.34215066643599834 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 26: last loss = -0.59049\n",
      "eval step --\n",
      "\n",
      "Step 26: val_rewards = 0.2567257158322929 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 27: last loss = -0.01311\n",
      "eval step --\n",
      "\n",
      "Step 27: val_rewards = 0.19462923549780328 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 28: last loss = -0.09141\n",
      "eval step --\n",
      "\n",
      "Step 28: val_rewards = 0.2519053262366352 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 29: last loss = -0.23461\n",
      "eval step --\n",
      "\n",
      "Step 29: val_rewards = 0.10188546021764974 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 30: last loss = -0.05106\n",
      "eval step --\n",
      "\n",
      "Step 30: val_rewards = 0.04584919570671106 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 31: last loss = -0.40933\n",
      "eval step --\n",
      "\n",
      "Step 31: val_rewards = -0.06694874116707691 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 32: last loss = -0.44607\n",
      "eval step --\n",
      "\n",
      "Step 32: val_rewards = -0.2420786698430796 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 33: last loss = -0.06411\n",
      "eval step --\n",
      "\n",
      "Step 33: val_rewards = -0.24435796738475207 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 34: last loss = -0.71935\n",
      "eval step --\n",
      "\n",
      "Step 34: val_rewards = -0.2178623838393252 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 35: last loss = -0.04058\n",
      "eval step --\n",
      "\n",
      "Step 35: val_rewards = -0.2275457868237327 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 36: last loss = -0.53638\n",
      "eval step --\n",
      "\n",
      "Step 36: val_rewards = -0.2805377449882768 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 37: last loss = -0.40055\n",
      "eval step --\n",
      "\n",
      "Step 37: val_rewards = -0.11100283783781643 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 38: last loss = -0.29126\n",
      "eval step --\n",
      "\n",
      "Step 38: val_rewards = -0.0161544008026756 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 39: last loss = -0.98150\n",
      "eval step --\n",
      "\n",
      "Step 39: val_rewards = 0.004886975931865533 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 40: last loss = -0.10824\n",
      "eval step --\n",
      "\n",
      "Step 40: val_rewards = 0.056504614457269384 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 41: last loss = -0.25102\n",
      "eval step --\n",
      "\n",
      "Step 41: val_rewards = 0.006682837305247876 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 42: last loss = -0.41268\n",
      "eval step --\n",
      "\n",
      "Step 42: val_rewards = -0.02579276525885716 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 43: last loss = -0.56734\n",
      "eval step --\n",
      "\n",
      "Step 43: val_rewards = -0.0794081021199058 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 44: last loss = -0.26835\n",
      "eval step --\n",
      "\n",
      "Step 44: val_rewards = -0.17371109789347805 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 45: last loss = -0.55522\n",
      "eval step --\n",
      "\n",
      "Step 45: val_rewards = -0.17405260613068357 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 46: last loss = -0.50167\n",
      "eval step --\n",
      "\n",
      "Step 46: val_rewards = -0.20604591065473013 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 47: last loss = -0.77693\n",
      "eval step --\n",
      "\n",
      "Step 47: val_rewards = 0.13192551227619617 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 48: last loss = 0.11537\n",
      "eval step --\n",
      "\n",
      "Step 48: val_rewards = 0.2272960616046979 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 49: last loss = -0.99188\n",
      "eval step --\n",
      "\n",
      "Step 49: val_rewards = 0.23368031346246762 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 50: last loss = -0.51409\n",
      "eval step --\n",
      "\n",
      "Step 50: val_rewards = 0.21779546569167305 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 51: last loss = 0.01642\n",
      "eval step --\n",
      "\n",
      "Step 51: val_rewards = 0.19923235676559267 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 52: last loss = 0.14770\n",
      "eval step --\n",
      "\n",
      "Step 52: val_rewards = 0.27609685508518717 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 53: last loss = -0.35304\n",
      "eval step --\n",
      "\n",
      "Step 53: val_rewards = 0.20722583521745128 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 54: last loss = -0.61139\n",
      "eval step --\n",
      "\n",
      "Step 54: val_rewards = 0.1347143146646215 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 55: last loss = -0.07932\n",
      "eval step --\n",
      "\n",
      "Step 55: val_rewards = 0.186401279090792 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 56: last loss = -0.82692\n",
      "eval step --\n",
      "\n",
      "Step 56: val_rewards = 0.17658602312537017 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 57: last loss = -0.12823\n",
      "eval step --\n",
      "\n",
      "Step 57: val_rewards = 0.3000757185792896 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 58: last loss = -0.49808\n",
      "eval step --\n",
      "\n",
      "Step 58: val_rewards = 0.37623716888542286 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 59: last loss = -0.65370\n",
      "eval step --\n",
      "\n",
      "Step 59: val_rewards = 0.5777469856206766 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 60: last loss = -0.79812\n",
      "eval step --\n",
      "\n",
      "Step 60: val_rewards = 0.6805630530324766 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 61: last loss = -0.90314\n",
      "eval step --\n",
      "\n",
      "Step 61: val_rewards = 0.7563868881652527 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 62: last loss = 0.19786\n",
      "eval step --\n",
      "\n",
      "Step 62: val_rewards = 0.5435423280476068 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 63: last loss = -0.17305\n",
      "eval step --\n",
      "\n",
      "Step 63: val_rewards = 0.7191873449332004 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 64: last loss = -0.14291\n",
      "eval step --\n",
      "\n",
      "Step 64: val_rewards = 0.9292469028816615 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 65: last loss = -0.34942\n",
      "eval step --\n",
      "\n",
      "Step 65: val_rewards = 0.9891631810716726 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 66: last loss = -0.04473\n",
      "eval step --\n",
      "\n",
      "Step 66: val_rewards = 0.8476992953583926 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 67: last loss = 0.25711\n",
      "eval step --\n",
      "\n",
      "Step 67: val_rewards = 1.077561259462011 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 68: last loss = -0.93903\n",
      "eval step --\n",
      "\n",
      "Step 68: val_rewards = 0.8295812678957898 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 69: last loss = 0.16581\n",
      "eval step --\n",
      "\n",
      "Step 69: val_rewards = 0.4057988966097147 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 70: last loss = -0.72132\n",
      "eval step --\n",
      "\n",
      "Step 70: val_rewards = 0.5512124370085474 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 71: last loss = -0.48457\n",
      "eval step --\n",
      "\n",
      "Step 71: val_rewards = 0.6258922775134342 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 72: last loss = -0.03381\n",
      "eval step --\n",
      "\n",
      "Step 72: val_rewards = 0.6027589341910863 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 73: last loss = -0.68997\n",
      "eval step --\n",
      "\n",
      "Step 73: val_rewards = 0.7783343815808449 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 74: last loss = -0.75054\n",
      "eval step --\n",
      "\n",
      "Step 74: val_rewards = 0.9400641044465086 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 75: last loss = -0.73251\n",
      "eval step --\n",
      "\n",
      "Step 75: val_rewards = 0.8418711990571816 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 76: last loss = 0.01032\n",
      "eval step --\n",
      "\n",
      "Step 76: val_rewards = 0.9588416893602512 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 77: last loss = -0.68277\n",
      "eval step --\n",
      "\n",
      "Step 77: val_rewards = 0.9581045666279605 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 78: last loss = -0.14266\n",
      "eval step --\n",
      "\n",
      "Step 78: val_rewards = 0.9516732441145508 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 79: last loss = -0.31831\n",
      "eval step --\n",
      "\n",
      "Step 79: val_rewards = 0.8784416035855905 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 80: last loss = -0.05522\n",
      "eval step --\n",
      "\n",
      "Step 80: val_rewards = 1.0624168884598428 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 81: last loss = -0.01302\n",
      "eval step --\n",
      "\n",
      "Step 81: val_rewards = 1.2410309150356162 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 82: last loss = -0.56774\n",
      "eval step --\n",
      "\n",
      "Step 82: val_rewards = 1.2807646571285836 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 83: last loss = -0.40258\n",
      "eval step --\n",
      "\n",
      "Step 83: val_rewards = 1.3544363313546255 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 84: last loss = -0.06317\n",
      "eval step --\n",
      "\n",
      "Step 84: val_rewards = 1.342470127221859 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 85: last loss = -0.00880\n",
      "eval step --\n",
      "\n",
      "Step 85: val_rewards = 1.3127711531847803 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 86: last loss = -0.94540\n",
      "eval step --\n",
      "\n",
      "Step 86: val_rewards = 1.0947441088213812 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 87: last loss = -0.38115\n",
      "eval step --\n",
      "\n",
      "Step 87: val_rewards = 1.070130301072464 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 88: last loss = -0.76748\n",
      "eval step --\n",
      "\n",
      "Step 88: val_rewards = 1.0516457309919438 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 89: last loss = -0.46545\n",
      "eval step --\n",
      "\n",
      "Step 89: val_rewards = 1.0236479445914441 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 90: last loss = -0.01102\n",
      "eval step --\n",
      "\n",
      "Step 90: val_rewards = 1.0603079612148358 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 91: last loss = 0.03911\n",
      "eval step --\n",
      "\n",
      "Step 91: val_rewards = 1.0602771206363806 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 92: last loss = -0.23525\n",
      "eval step --\n",
      "\n",
      "Step 92: val_rewards = 1.142167146959421 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 93: last loss = -0.11544\n",
      "eval step --\n",
      "\n",
      "Step 93: val_rewards = 1.15123160805812 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 94: last loss = -0.03453\n",
      "eval step --\n",
      "\n",
      "Step 94: val_rewards = 1.1514898172946448 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 95: last loss = -0.41978\n",
      "eval step --\n",
      "\n",
      "Step 95: val_rewards = 1.1247154752388369 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 96: last loss = -0.93614\n",
      "eval step --\n",
      "\n",
      "Step 96: val_rewards = 1.08881315605004 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 97: last loss = -0.13333\n",
      "eval step --\n",
      "\n",
      "Step 97: val_rewards = 1.0524466191848492 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 98: last loss = 0.09852\n",
      "eval step --\n",
      "\n",
      "Step 98: val_rewards = 1.0467957417352145 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 99: last loss = 0.12706\n",
      "eval step --\n",
      "\n",
      "Step 99: val_rewards = 1.1189885322483055 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 100: last loss = -0.18354\n",
      "eval step --\n",
      "\n",
      "Step 100: val_rewards = 1.2339664608972893 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 101: last loss = -0.47433\n",
      "eval step --\n",
      "\n",
      "Step 101: val_rewards = 1.292391320581294 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 102: last loss = -1.04993\n",
      "eval step --\n",
      "\n",
      "Step 102: val_rewards = 1.3846489454923356 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 103: last loss = 0.28977\n",
      "eval step --\n",
      "\n",
      "Step 103: val_rewards = 1.4024274546192654 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 104: last loss = 0.17285\n",
      "eval step --\n",
      "\n",
      "Step 104: val_rewards = 1.4901500120415812 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 105: last loss = -0.29000\n",
      "eval step --\n",
      "\n",
      "Step 105: val_rewards = 1.6060425802115543 | baseline_reward = 1.1209681893688832\n",
      "*** found better model ***\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 106: last loss = -0.10010\n",
      "eval step --\n",
      "\n",
      "Step 106: val_rewards = 1.4498797620265056 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 107: last loss = -0.52675\n",
      "eval step --\n",
      "\n",
      "Step 107: val_rewards = 1.438522017384983 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 108: last loss = -0.24290\n",
      "eval step --\n",
      "\n",
      "Step 108: val_rewards = 1.4561842206412277 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 109: last loss = 0.24218\n",
      "eval step --\n",
      "\n",
      "Step 109: val_rewards = 1.4631042976371105 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 110: last loss = -0.08705\n",
      "eval step --\n",
      "\n",
      "Step 110: val_rewards = 1.4401703982045329 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 111: last loss = -0.30266\n",
      "eval step --\n",
      "\n",
      "Step 111: val_rewards = 1.4463446021535944 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 112: last loss = -0.27161\n",
      "eval step --\n",
      "\n",
      "Step 112: val_rewards = 1.3586597191177139 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 113: last loss = -0.46370\n",
      "eval step --\n",
      "\n",
      "Step 113: val_rewards = 1.2828873148548308 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 114: last loss = 0.21297\n",
      "eval step --\n",
      "\n",
      "Step 114: val_rewards = 1.3062899960325074 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 115: last loss = -0.11771\n",
      "eval step --\n",
      "\n",
      "Step 115: val_rewards = 1.280052860847302 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 116: last loss = -0.02617\n",
      "eval step --\n",
      "\n",
      "Step 116: val_rewards = 1.2889659853014004 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 117: last loss = 0.06348\n",
      "eval step --\n",
      "\n",
      "Step 117: val_rewards = 1.288965525873915 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 118: last loss = -0.06371\n",
      "eval step --\n",
      "\n",
      "Step 118: val_rewards = 1.1942927544336832 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 119: last loss = 0.08625\n",
      "eval step --\n",
      "\n",
      "Step 119: val_rewards = 1.1545360152983186 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 120: last loss = -0.39959\n",
      "eval step --\n",
      "\n",
      "Step 120: val_rewards = 1.1410777078027776 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 121: last loss = -0.12316\n",
      "eval step --\n",
      "\n",
      "Step 121: val_rewards = 1.127082200918189 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 122: last loss = -0.00176\n",
      "eval step --\n",
      "\n",
      "Step 122: val_rewards = 1.127081364396088 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 123: last loss = 0.05186\n",
      "eval step --\n",
      "\n",
      "Step 123: val_rewards = 1.1270806498334522 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 124: last loss = -0.36546\n",
      "eval step --\n",
      "\n",
      "Step 124: val_rewards = 1.1410741383468164 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 125: last loss = 0.09974\n",
      "eval step --\n",
      "\n",
      "Step 125: val_rewards = 1.1410729502894001 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 126: last loss = -0.79890\n",
      "eval step --\n",
      "\n",
      "Step 126: val_rewards = 1.2338551259387582 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 127: last loss = 0.05976\n",
      "eval step --\n",
      "\n",
      "Step 127: val_rewards = 1.2345368027698138 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 128: last loss = -0.58194\n",
      "eval step --\n",
      "\n",
      "Step 128: val_rewards = 1.2345357952061309 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 129: last loss = 0.15999\n",
      "eval step --\n",
      "\n",
      "Step 129: val_rewards = 1.2048495639469972 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 130: last loss = 0.11243\n",
      "eval step --\n",
      "\n",
      "Step 130: val_rewards = 1.199325481091733 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 131: last loss = -0.04516\n",
      "eval step --\n",
      "\n",
      "Step 131: val_rewards = 1.1935479345958508 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 132: last loss = -0.06328\n",
      "eval step --\n",
      "\n",
      "Step 132: val_rewards = 1.193547011792258 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 133: last loss = -0.13483\n",
      "eval step --\n",
      "\n",
      "Step 133: val_rewards = 1.1894808856103967 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 134: last loss = -0.65437\n",
      "eval step --\n",
      "\n",
      "Step 134: val_rewards = 1.1894797287870882 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 135: last loss = -0.54816\n",
      "eval step --\n",
      "\n",
      "Step 135: val_rewards = 1.189478932747797 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 136: last loss = -0.43603\n",
      "eval step --\n",
      "\n",
      "Step 136: val_rewards = 1.189478058931439 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 137: last loss = 0.16149\n",
      "eval step --\n",
      "\n",
      "Step 137: val_rewards = 1.1894774658179508 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 138: last loss = -0.02583\n",
      "eval step --\n",
      "\n",
      "Step 138: val_rewards = 1.1743365576766238 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 139: last loss = 0.10122\n",
      "eval step --\n",
      "\n",
      "Step 139: val_rewards = 1.159165526031306 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 140: last loss = 0.14288\n",
      "eval step --\n",
      "\n",
      "Step 140: val_rewards = 1.1591652351682535 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 141: last loss = -0.01732\n",
      "eval step --\n",
      "\n",
      "Step 141: val_rewards = 1.1965292729421801 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 142: last loss = -0.60353\n",
      "eval step --\n",
      "\n",
      "Step 142: val_rewards = 1.242708880623206 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 143: last loss = -0.44379\n",
      "eval step --\n",
      "\n",
      "Step 143: val_rewards = 1.2427073820313501 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 144: last loss = -0.47158\n",
      "eval step --\n",
      "\n",
      "Step 144: val_rewards = 1.2427060394211817 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 145: last loss = -0.74834\n",
      "eval step --\n",
      "\n",
      "Step 145: val_rewards = 1.2379629411280175 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 146: last loss = -0.02978\n",
      "eval step --\n",
      "\n",
      "Step 146: val_rewards = 1.3223856504828435 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 147: last loss = 0.18614\n",
      "eval step --\n",
      "\n",
      "Step 147: val_rewards = 1.3223823751909578 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 148: last loss = -0.11947\n",
      "eval step --\n",
      "\n",
      "Step 148: val_rewards = 1.3111859684011802 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 149: last loss = -0.52052\n",
      "eval step --\n",
      "\n",
      "Step 149: val_rewards = 1.2983849164177148 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 150: last loss = -0.43113\n",
      "eval step --\n",
      "\n",
      "Step 150: val_rewards = 1.1762088051462856 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 151: last loss = -0.25835\n",
      "eval step --\n",
      "\n",
      "Step 151: val_rewards = 1.1762054673432751 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 152: last loss = -0.88164\n",
      "eval step --\n",
      "\n",
      "Step 152: val_rewards = 1.1596833251431757 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 153: last loss = 0.04169\n",
      "eval step --\n",
      "\n",
      "Step 153: val_rewards = 1.178984599825116 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 154: last loss = -0.65634\n",
      "eval step --\n",
      "\n",
      "Step 154: val_rewards = 1.1836947536958917 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 155: last loss = -0.17193\n",
      "eval step --\n",
      "\n",
      "Step 155: val_rewards = 1.1205214454599206 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 156: last loss = 0.15676\n",
      "eval step --\n",
      "\n",
      "Step 156: val_rewards = 1.1393377429997285 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 157: last loss = -0.08673\n",
      "eval step --\n",
      "\n",
      "Step 157: val_rewards = 1.0534239185435885 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 158: last loss = 0.29422\n",
      "eval step --\n",
      "\n",
      "Step 158: val_rewards = 1.051820844036883 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 159: last loss = 0.15494\n",
      "eval step --\n",
      "\n",
      "Step 159: val_rewards = 1.1421539038505355 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 160: last loss = -0.30278\n",
      "eval step --\n",
      "\n",
      "Step 160: val_rewards = 1.0233078055640288 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 161: last loss = -0.44527\n",
      "eval step --\n",
      "\n",
      "Step 161: val_rewards = 0.9128843704344133 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 162: last loss = -0.10947\n",
      "eval step --\n",
      "\n",
      "Step 162: val_rewards = 0.9380563623722714 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 163: last loss = -1.15789\n",
      "eval step --\n",
      "\n",
      "Step 163: val_rewards = 0.9535136184072577 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 164: last loss = -0.11991\n",
      "eval step --\n",
      "\n",
      "Step 164: val_rewards = 0.9792864267620364 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 165: last loss = 0.00446\n",
      "eval step --\n",
      "\n",
      "Step 165: val_rewards = 0.981506196204245 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 166: last loss = -0.42292\n",
      "eval step --\n",
      "\n",
      "Step 166: val_rewards = 0.9861565146669908 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 167: last loss = 0.14545\n",
      "eval step --\n",
      "\n",
      "Step 167: val_rewards = 0.8625616845314734 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 168: last loss = -0.08855\n",
      "eval step --\n",
      "\n",
      "Step 168: val_rewards = 0.8457034896568134 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 169: last loss = -0.14602\n",
      "eval step --\n",
      "\n",
      "Step 169: val_rewards = 0.7117390112508795 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 170: last loss = 0.12349\n",
      "eval step --\n",
      "\n",
      "Step 170: val_rewards = 0.717691832396384 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 171: last loss = -0.91082\n",
      "eval step --\n",
      "\n",
      "Step 171: val_rewards = 0.7563442539090419 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 172: last loss = -0.43171\n",
      "eval step --\n",
      "\n",
      "Step 172: val_rewards = 0.7413476022803409 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 173: last loss = -0.11309\n",
      "eval step --\n",
      "\n",
      "Step 173: val_rewards = 0.8046924589148499 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 174: last loss = -0.13285\n",
      "eval step --\n",
      "\n",
      "Step 174: val_rewards = 0.8560764727211438 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 175: last loss = -0.17075\n",
      "eval step --\n",
      "\n",
      "Step 175: val_rewards = 0.8745510499528737 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 176: last loss = 0.12975\n",
      "eval step --\n",
      "\n",
      "Step 176: val_rewards = 0.7665770391257656 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 177: last loss = -0.85968\n",
      "eval step --\n",
      "\n",
      "Step 177: val_rewards = 0.7739740598385435 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 178: last loss = -0.35760\n",
      "eval step --\n",
      "\n",
      "Step 178: val_rewards = 0.7785700702208991 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 179: last loss = -0.27533\n",
      "eval step --\n",
      "\n",
      "Step 179: val_rewards = 0.7719225728683926 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 180: last loss = 0.08585\n",
      "eval step --\n",
      "\n",
      "Step 180: val_rewards = 0.7719207875657083 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 181: last loss = -0.33059\n",
      "eval step --\n",
      "\n",
      "Step 181: val_rewards = 0.8864665177611106 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 182: last loss = 0.06475\n",
      "eval step --\n",
      "\n",
      "Step 182: val_rewards = 0.8379563202554289 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 183: last loss = 0.16891\n",
      "eval step --\n",
      "\n",
      "Step 183: val_rewards = 0.9152867438433051 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 184: last loss = -0.60154\n",
      "eval step --\n",
      "\n",
      "Step 184: val_rewards = 0.8591505247561274 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 185: last loss = -0.75594\n",
      "eval step --\n",
      "\n",
      "Step 185: val_rewards = 0.8220035022435517 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 186: last loss = -0.53006\n",
      "eval step --\n",
      "\n",
      "Step 186: val_rewards = 0.8412979112479013 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 187: last loss = 0.13524\n",
      "eval step --\n",
      "\n",
      "Step 187: val_rewards = 0.8148215465022403 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 188: last loss = 0.15210\n",
      "eval step --\n",
      "\n",
      "Step 188: val_rewards = 0.7365433832796613 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 189: last loss = 0.11032\n",
      "eval step --\n",
      "\n",
      "Step 189: val_rewards = 0.8437009757665983 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 190: last loss = 0.00061\n",
      "eval step --\n",
      "\n",
      "Step 190: val_rewards = 0.8482757869691726 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 191: last loss = 0.14779\n",
      "eval step --\n",
      "\n",
      "Step 191: val_rewards = 0.8585908160904215 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 192: last loss = -0.40361\n",
      "eval step --\n",
      "\n",
      "Step 192: val_rewards = 0.824002947820936 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 193: last loss = -0.39341\n",
      "eval step --\n",
      "\n",
      "Step 193: val_rewards = 0.8416562350007428 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 194: last loss = 0.13082\n",
      "eval step --\n",
      "\n",
      "Step 194: val_rewards = 0.84442034419575 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 195: last loss = 0.10920\n",
      "eval step --\n",
      "\n",
      "Step 195: val_rewards = 0.8107494295329742 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 196: last loss = 0.15724\n",
      "eval step --\n",
      "\n",
      "Step 196: val_rewards = 0.8161727227751061 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 197: last loss = -0.26984\n",
      "eval step --\n",
      "\n",
      "Step 197: val_rewards = 0.850880801155128 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 198: last loss = -0.28519\n",
      "eval step --\n",
      "\n",
      "Step 198: val_rewards = 0.8924471558136613 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 199: last loss = 0.05217\n",
      "eval step --\n",
      "\n",
      "Step 199: val_rewards = 0.9999759999843112 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 200: last loss = -0.46639\n",
      "eval step --\n",
      "\n",
      "Step 200: val_rewards = 0.9218253731964952 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 201: last loss = -0.22247\n",
      "eval step --\n",
      "\n",
      "Step 201: val_rewards = 0.7443334087461497 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 202: last loss = 0.02289\n",
      "eval step --\n",
      "\n",
      "Step 202: val_rewards = 0.7550548721104655 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 203: last loss = 0.04990\n",
      "eval step --\n",
      "\n",
      "Step 203: val_rewards = 0.8014345319135826 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 204: last loss = -0.04913\n",
      "eval step --\n",
      "\n",
      "Step 204: val_rewards = 1.081592877869467 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 205: last loss = 0.05786\n",
      "eval step --\n",
      "\n",
      "Step 205: val_rewards = 1.1012561820271605 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 206: last loss = -0.28873\n",
      "eval step --\n",
      "\n",
      "Step 206: val_rewards = 1.0069837785574032 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 207: last loss = -0.15396\n",
      "eval step --\n",
      "\n",
      "Step 207: val_rewards = 0.9877446301213115 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 208: last loss = -0.65499\n",
      "eval step --\n",
      "\n",
      "Step 208: val_rewards = 1.0094143271843006 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 209: last loss = -0.17545\n",
      "eval step --\n",
      "\n",
      "Step 209: val_rewards = 1.0035643320367296 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 210: last loss = -0.04498\n",
      "eval step --\n",
      "\n",
      "Step 210: val_rewards = 1.071155123727858 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 211: last loss = -0.40167\n",
      "eval step --\n",
      "\n",
      "Step 211: val_rewards = 1.0020116738223925 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 212: last loss = 0.08505\n",
      "eval step --\n",
      "\n",
      "Step 212: val_rewards = 0.9933666307155924 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 213: last loss = -0.26319\n",
      "eval step --\n",
      "\n",
      "Step 213: val_rewards = 0.8345800589738205 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 214: last loss = -0.55300\n",
      "eval step --\n",
      "\n",
      "Step 214: val_rewards = 0.8292859131439543 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 215: last loss = 0.25037\n",
      "eval step --\n",
      "\n",
      "Step 215: val_rewards = 0.7850827934520708 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 216: last loss = -0.11382\n",
      "eval step --\n",
      "\n",
      "Step 216: val_rewards = 0.7773968030441627 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 217: last loss = -0.05923\n",
      "eval step --\n",
      "\n",
      "Step 217: val_rewards = 0.7715814164772449 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 218: last loss = -0.63873\n",
      "eval step --\n",
      "\n",
      "Step 218: val_rewards = 0.7154832383627382 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 219: last loss = -0.15419\n",
      "eval step --\n",
      "\n",
      "Step 219: val_rewards = 0.6741554822072976 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 220: last loss = 0.20768\n",
      "eval step --\n",
      "\n",
      "Step 220: val_rewards = 0.7511202499622764 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 221: last loss = -0.45671\n",
      "eval step --\n",
      "\n",
      "Step 221: val_rewards = 0.7857666970087318 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 222: last loss = -0.22406\n",
      "eval step --\n",
      "\n",
      "Step 222: val_rewards = 0.774938264778306 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 223: last loss = 0.17172\n",
      "eval step --\n",
      "\n",
      "Step 223: val_rewards = 0.877514415794497 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 224: last loss = 0.19913\n",
      "eval step --\n",
      "\n",
      "Step 224: val_rewards = 0.960118906201787 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 225: last loss = -0.27727\n",
      "eval step --\n",
      "\n",
      "Step 225: val_rewards = 0.958455508370902 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 226: last loss = -0.19079\n",
      "eval step --\n",
      "\n",
      "Step 226: val_rewards = 0.8672114117480055 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 227: last loss = 0.15719\n",
      "eval step --\n",
      "\n",
      "Step 227: val_rewards = 0.8506990676666565 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 228: last loss = 0.02661\n",
      "eval step --\n",
      "\n",
      "Step 228: val_rewards = 0.8169172676672308 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 229: last loss = 0.15720\n",
      "eval step --\n",
      "\n",
      "Step 229: val_rewards = 0.787202295512006 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 230: last loss = -0.35546\n",
      "eval step --\n",
      "\n",
      "Step 230: val_rewards = 0.7827070562964004 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 231: last loss = -0.16052\n",
      "eval step --\n",
      "\n",
      "Step 231: val_rewards = 0.7827016272746081 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 232: last loss = -0.15233\n",
      "eval step --\n",
      "\n",
      "Step 232: val_rewards = 0.8638835058374236 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 233: last loss = 0.14508\n",
      "eval step --\n",
      "\n",
      "Step 233: val_rewards = 0.8058290042975339 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 234: last loss = -0.50950\n",
      "eval step --\n",
      "\n",
      "Step 234: val_rewards = 0.8114670257812319 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 235: last loss = 0.03522\n",
      "eval step --\n",
      "\n",
      "Step 235: val_rewards = 0.7751478365265873 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 236: last loss = -0.53544\n",
      "eval step --\n",
      "\n",
      "Step 236: val_rewards = 0.9077160216345912 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 237: last loss = -0.63394\n",
      "eval step --\n",
      "\n",
      "Step 237: val_rewards = 0.8320106449809994 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 238: last loss = -1.05811\n",
      "eval step --\n",
      "\n",
      "Step 238: val_rewards = 0.7882565647449181 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 239: last loss = -0.10044\n",
      "eval step --\n",
      "\n",
      "Step 239: val_rewards = 0.7742530026800974 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 240: last loss = -0.15660\n",
      "eval step --\n",
      "\n",
      "Step 240: val_rewards = 0.8224027151866016 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 241: last loss = -0.80632\n",
      "eval step --\n",
      "\n",
      "Step 241: val_rewards = 0.8380307369251153 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 242: last loss = -0.45013\n",
      "eval step --\n",
      "\n",
      "Step 242: val_rewards = 0.802886225482322 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 243: last loss = -0.44935\n",
      "eval step --\n",
      "\n",
      "Step 243: val_rewards = 0.7466544503958075 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 244: last loss = -0.24178\n",
      "eval step --\n",
      "\n",
      "Step 244: val_rewards = 0.7294864146020765 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 245: last loss = -0.00509\n",
      "eval step --\n",
      "\n",
      "Step 245: val_rewards = 0.6440529806289753 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 246: last loss = 0.01845\n",
      "eval step --\n",
      "\n",
      "Step 246: val_rewards = 0.6784093253616666 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 247: last loss = 0.05276\n",
      "eval step --\n",
      "\n",
      "Step 247: val_rewards = 0.6747741043043471 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 248: last loss = -0.11241\n",
      "eval step --\n",
      "\n",
      "Step 248: val_rewards = 0.881634486836252 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 249: last loss = -0.21313\n",
      "eval step --\n",
      "\n",
      "Step 249: val_rewards = 0.7350687525306537 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 250: last loss = 0.17705\n",
      "eval step --\n",
      "\n",
      "Step 250: val_rewards = 0.7614272786841879 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 251: last loss = -0.73450\n",
      "eval step --\n",
      "\n",
      "Step 251: val_rewards = 0.8036295714986136 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 252: last loss = -1.16027\n",
      "eval step --\n",
      "\n",
      "Step 252: val_rewards = 0.7503403249344128 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 253: last loss = 0.19083\n",
      "eval step --\n",
      "\n",
      "Step 253: val_rewards = 0.7459603621350251 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 254: last loss = -0.03298\n",
      "eval step --\n",
      "\n",
      "Step 254: val_rewards = 0.7284619564638583 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 255: last loss = -0.21079\n",
      "eval step --\n",
      "\n",
      "Step 255: val_rewards = 0.5479817175029674 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 256: last loss = -0.92842\n",
      "eval step --\n",
      "\n",
      "Step 256: val_rewards = 0.5889937903744963 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 257: last loss = -0.29674\n",
      "eval step --\n",
      "\n",
      "Step 257: val_rewards = 0.44953128406478 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 258: last loss = -0.47469\n",
      "eval step --\n",
      "\n",
      "Step 258: val_rewards = 0.3627361026127548 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 259: last loss = -0.43321\n",
      "eval step --\n",
      "\n",
      "Step 259: val_rewards = 0.3144701234452633 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 260: last loss = -0.19339\n",
      "eval step --\n",
      "\n",
      "Step 260: val_rewards = 0.3122634164957495 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 261: last loss = 0.02059\n",
      "eval step --\n",
      "\n",
      "Step 261: val_rewards = 0.3645817320659392 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 262: last loss = -0.04653\n",
      "eval step --\n",
      "\n",
      "Step 262: val_rewards = 0.30050079419610476 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 263: last loss = 0.11550\n",
      "eval step --\n",
      "\n",
      "Step 263: val_rewards = 0.30019074069072577 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 264: last loss = -0.08136\n",
      "eval step --\n",
      "\n",
      "Step 264: val_rewards = 0.3464974587792273 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 265: last loss = -0.48480\n",
      "eval step --\n",
      "\n",
      "Step 265: val_rewards = 0.3341988439678713 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 266: last loss = 0.01565\n",
      "eval step --\n",
      "\n",
      "Step 266: val_rewards = 0.26016898171514896 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 267: last loss = -0.16662\n",
      "eval step --\n",
      "\n",
      "Step 267: val_rewards = 0.16410008757780392 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 268: last loss = -0.19945\n",
      "eval step --\n",
      "\n",
      "Step 268: val_rewards = 0.18780021511716757 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 269: last loss = -0.10701\n",
      "eval step --\n",
      "\n",
      "Step 269: val_rewards = 0.18769575927044593 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 270: last loss = -0.15325\n",
      "eval step --\n",
      "\n",
      "Step 270: val_rewards = 0.18860270780862368 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 271: last loss = -0.11326\n",
      "eval step --\n",
      "\n",
      "Step 271: val_rewards = 0.19923294691030552 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 272: last loss = -0.99110\n",
      "eval step --\n",
      "\n",
      "Step 272: val_rewards = 0.26830933867849005 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 273: last loss = -0.20057\n",
      "eval step --\n",
      "\n",
      "Step 273: val_rewards = 0.27650300386570076 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 274: last loss = -0.61181\n",
      "eval step --\n",
      "\n",
      "Step 274: val_rewards = 0.2778373097716901 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 275: last loss = -0.63890\n",
      "eval step --\n",
      "\n",
      "Step 275: val_rewards = 0.254186274266834 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 276: last loss = -0.56135\n",
      "eval step --\n",
      "\n",
      "Step 276: val_rewards = 0.2271679625521815 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 277: last loss = -0.69859\n",
      "eval step --\n",
      "\n",
      "Step 277: val_rewards = 0.22816451330248497 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 278: last loss = -0.75732\n",
      "eval step --\n",
      "\n",
      "Step 278: val_rewards = 0.21521387423058258 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 279: last loss = -0.85304\n",
      "eval step --\n",
      "\n",
      "Step 279: val_rewards = 0.2886158932158124 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 280: last loss = -0.04719\n",
      "eval step --\n",
      "\n",
      "Step 280: val_rewards = 0.2765402702345741 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 281: last loss = 0.00550\n",
      "eval step --\n",
      "\n",
      "Step 281: val_rewards = 0.26771831053399975 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 282: last loss = -0.89830\n",
      "eval step --\n",
      "\n",
      "Step 282: val_rewards = 0.31039959172268805 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 283: last loss = -0.06296\n",
      "eval step --\n",
      "\n",
      "Step 283: val_rewards = 0.32993690101381207 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 284: last loss = -0.17526\n",
      "eval step --\n",
      "\n",
      "Step 284: val_rewards = 0.2807991055879304 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 285: last loss = -0.43566\n",
      "eval step --\n",
      "\n",
      "Step 285: val_rewards = 0.3151918417273335 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 286: last loss = 0.03783\n",
      "eval step --\n",
      "\n",
      "Step 286: val_rewards = 0.3030740414015562 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 287: last loss = -1.06717\n",
      "eval step --\n",
      "\n",
      "Step 287: val_rewards = 0.2913019065213368 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 288: last loss = 0.04063\n",
      "eval step --\n",
      "\n",
      "Step 288: val_rewards = 0.30626505461337494 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 289: last loss = -0.23596\n",
      "eval step --\n",
      "\n",
      "Step 289: val_rewards = 0.29835886750691964 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 290: last loss = -0.09131\n",
      "eval step --\n",
      "\n",
      "Step 290: val_rewards = 0.3132542721737578 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 291: last loss = -0.60821\n",
      "eval step --\n",
      "\n",
      "Step 291: val_rewards = 0.313272185524299 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 292: last loss = -0.05871\n",
      "eval step --\n",
      "\n",
      "Step 292: val_rewards = 0.3334432427395769 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 293: last loss = -0.39082\n",
      "eval step --\n",
      "\n",
      "Step 293: val_rewards = 0.34049787730378317 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 294: last loss = -0.02791\n",
      "eval step --\n",
      "\n",
      "Step 294: val_rewards = 0.3405156999153219 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 295: last loss = -0.17064\n",
      "eval step --\n",
      "\n",
      "Step 295: val_rewards = 0.32593490525798313 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 296: last loss = -0.22123\n",
      "eval step --\n",
      "\n",
      "Step 296: val_rewards = 0.3445133948687449 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 297: last loss = -0.70652\n",
      "eval step --\n",
      "\n",
      "Step 297: val_rewards = 0.36390689471051496 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 298: last loss = -0.33403\n",
      "eval step --\n",
      "\n",
      "Step 298: val_rewards = 0.3696386039256653 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 299: last loss = -0.92625\n",
      "eval step --\n",
      "\n",
      "Step 299: val_rewards = 0.3609854922063354 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 300: last loss = -0.07805\n",
      "eval step --\n",
      "\n",
      "Step 300: val_rewards = 0.3846903630162796 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 301: last loss = 0.02779\n",
      "eval step --\n",
      "\n",
      "Step 301: val_rewards = 0.37556829178219464 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 302: last loss = -1.01106\n",
      "eval step --\n",
      "\n",
      "Step 302: val_rewards = 0.3775219865500519 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 303: last loss = -0.43093\n",
      "eval step --\n",
      "\n",
      "Step 303: val_rewards = 0.3579685600011747 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 304: last loss = -0.02754\n",
      "eval step --\n",
      "\n",
      "Step 304: val_rewards = 0.3248208854831076 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 305: last loss = -0.01765\n",
      "eval step --\n",
      "\n",
      "Step 305: val_rewards = 0.448342530625758 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 306: last loss = -0.90984\n",
      "eval step --\n",
      "\n",
      "Step 306: val_rewards = 0.46205626241826975 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 307: last loss = -0.74659\n",
      "eval step --\n",
      "\n",
      "Step 307: val_rewards = 0.4571345117417503 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 308: last loss = 0.00095\n",
      "eval step --\n",
      "\n",
      "Step 308: val_rewards = 0.48024709967032264 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 309: last loss = -0.86663\n",
      "eval step --\n",
      "\n",
      "Step 309: val_rewards = 0.5218969028788097 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 310: last loss = -0.52622\n",
      "eval step --\n",
      "\n",
      "Step 310: val_rewards = 0.5326927367401623 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 311: last loss = -0.90987\n",
      "eval step --\n",
      "\n",
      "Step 311: val_rewards = 0.6042856759526277 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 312: last loss = -0.58864\n",
      "eval step --\n",
      "\n",
      "Step 312: val_rewards = 0.6167745800362193 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 313: last loss = -0.11669\n",
      "eval step --\n",
      "\n",
      "Step 313: val_rewards = 0.6623128105806081 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 314: last loss = -0.57203\n",
      "eval step --\n",
      "\n",
      "Step 314: val_rewards = 0.58756318610332 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 315: last loss = -0.15269\n",
      "eval step --\n",
      "\n",
      "Step 315: val_rewards = 0.5811968434302722 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 316: last loss = 0.03977\n",
      "eval step --\n",
      "\n",
      "Step 316: val_rewards = 0.5962450601824666 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 317: last loss = -0.71838\n",
      "eval step --\n",
      "\n",
      "Step 317: val_rewards = 0.7406410808700166 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 318: last loss = 0.00461\n",
      "eval step --\n",
      "\n",
      "Step 318: val_rewards = 0.6973086417740127 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 319: last loss = -0.40538\n",
      "eval step --\n",
      "\n",
      "Step 319: val_rewards = 0.6628920591695768 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 320: last loss = -0.49582\n",
      "eval step --\n",
      "\n",
      "Step 320: val_rewards = 0.674106658781427 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 321: last loss = 0.02479\n",
      "eval step --\n",
      "\n",
      "Step 321: val_rewards = 0.5903596976186211 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 322: last loss = -0.53149\n",
      "eval step --\n",
      "\n",
      "Step 322: val_rewards = 0.7311201340340114 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 323: last loss = -0.61081\n",
      "eval step --\n",
      "\n",
      "Step 323: val_rewards = 0.6397847368854844 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 324: last loss = -0.40725\n",
      "eval step --\n",
      "\n",
      "Step 324: val_rewards = 0.6593221041711418 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 325: last loss = -0.27925\n",
      "eval step --\n",
      "\n",
      "Step 325: val_rewards = 0.6896517952101705 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 326: last loss = -0.38064\n",
      "eval step --\n",
      "\n",
      "Step 326: val_rewards = 0.6254418551432632 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 327: last loss = 0.05659\n",
      "eval step --\n",
      "\n",
      "Step 327: val_rewards = 0.7606842753623124 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 328: last loss = -0.31318\n",
      "eval step --\n",
      "\n",
      "Step 328: val_rewards = 0.5129958471261391 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 329: last loss = -0.42363\n",
      "eval step --\n",
      "\n",
      "Step 329: val_rewards = 0.5310263402088543 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 330: last loss = -0.25038\n",
      "eval step --\n",
      "\n",
      "Step 330: val_rewards = 0.5626766220266607 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 331: last loss = 0.07979\n",
      "eval step --\n",
      "\n",
      "Step 331: val_rewards = 0.5353073673800429 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 332: last loss = -0.13378\n",
      "eval step --\n",
      "\n",
      "Step 332: val_rewards = 0.5458682831434811 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 333: last loss = -0.01312\n",
      "eval step --\n",
      "\n",
      "Step 333: val_rewards = 0.5520144311946343 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 334: last loss = -0.02812\n",
      "eval step --\n",
      "\n",
      "Step 334: val_rewards = 0.5523021738778883 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 335: last loss = -0.12025\n",
      "eval step --\n",
      "\n",
      "Step 335: val_rewards = 0.4629050326513254 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 336: last loss = -0.33273\n",
      "eval step --\n",
      "\n",
      "Step 336: val_rewards = 0.4542379611787865 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 337: last loss = -0.36261\n",
      "eval step --\n",
      "\n",
      "Step 337: val_rewards = 0.4628995631242957 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 338: last loss = -0.82288\n",
      "eval step --\n",
      "\n",
      "Step 338: val_rewards = 0.503569383976119 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 339: last loss = -0.72034\n",
      "eval step --\n",
      "\n",
      "Step 339: val_rewards = 0.5582015707477408 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 340: last loss = -0.28164\n",
      "eval step --\n",
      "\n",
      "Step 340: val_rewards = 0.5892520990179196 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 341: last loss = -0.46642\n",
      "eval step --\n",
      "\n",
      "Step 341: val_rewards = 0.6320494882093023 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 342: last loss = -0.03376\n",
      "eval step --\n",
      "\n",
      "Step 342: val_rewards = 0.6538824275110515 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 343: last loss = -0.35798\n",
      "eval step --\n",
      "\n",
      "Step 343: val_rewards = 0.5371247439052559 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 344: last loss = -0.28643\n",
      "eval step --\n",
      "\n",
      "Step 344: val_rewards = 0.5485091240721834 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 345: last loss = -0.66781\n",
      "eval step --\n",
      "\n",
      "Step 345: val_rewards = 0.5302265748127372 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 346: last loss = -1.11907\n",
      "eval step --\n",
      "\n",
      "Step 346: val_rewards = 0.5944991395972534 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 347: last loss = -0.16903\n",
      "eval step --\n",
      "\n",
      "Step 347: val_rewards = 0.7746229616972848 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 348: last loss = -0.48967\n",
      "eval step --\n",
      "\n",
      "Step 348: val_rewards = 0.7331957540040341 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 349: last loss = -0.69647\n",
      "eval step --\n",
      "\n",
      "Step 349: val_rewards = 0.6755168757379724 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 350: last loss = -0.13285\n",
      "eval step --\n",
      "\n",
      "Step 350: val_rewards = 0.68962083474306 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 351: last loss = -0.18182\n",
      "eval step --\n",
      "\n",
      "Step 351: val_rewards = 0.7665848608230262 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 352: last loss = -0.00145\n",
      "eval step --\n",
      "\n",
      "Step 352: val_rewards = 0.7827088829080306 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 353: last loss = -0.64289\n",
      "eval step --\n",
      "\n",
      "Step 353: val_rewards = 0.6139496106370841 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 354: last loss = -0.39804\n",
      "eval step --\n",
      "\n",
      "Step 354: val_rewards = 0.5915040024826825 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 355: last loss = 0.03760\n",
      "eval step --\n",
      "\n",
      "Step 355: val_rewards = 0.7652690051332182 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 356: last loss = -0.73637\n",
      "eval step --\n",
      "\n",
      "Step 356: val_rewards = 0.9594472004893891 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 357: last loss = -0.08363\n",
      "eval step --\n",
      "\n",
      "Step 357: val_rewards = 0.9384399836450613 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 358: last loss = -0.91237\n",
      "eval step --\n",
      "\n",
      "Step 358: val_rewards = 0.706358191936182 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 359: last loss = -0.55823\n",
      "eval step --\n",
      "\n",
      "Step 359: val_rewards = 0.7366076508815315 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 360: last loss = -0.68044\n",
      "eval step --\n",
      "\n",
      "Step 360: val_rewards = 0.8191887125823134 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 361: last loss = -0.03701\n",
      "eval step --\n",
      "\n",
      "Step 361: val_rewards = 0.8624942501609677 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 362: last loss = -0.05010\n",
      "eval step --\n",
      "\n",
      "Step 362: val_rewards = 0.8454227521140542 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 363: last loss = -0.25866\n",
      "eval step --\n",
      "\n",
      "Step 363: val_rewards = 0.8658112975576066 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 364: last loss = -0.12029\n",
      "eval step --\n",
      "\n",
      "Step 364: val_rewards = 0.7846629376359258 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 365: last loss = -0.27043\n",
      "eval step --\n",
      "\n",
      "Step 365: val_rewards = 0.700183952978875 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 366: last loss = -0.63428\n",
      "eval step --\n",
      "\n",
      "Step 366: val_rewards = 0.6801816640615971 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 367: last loss = -0.03207\n",
      "eval step --\n",
      "\n",
      "Step 367: val_rewards = 0.6285256334959537 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 368: last loss = -0.09284\n",
      "eval step --\n",
      "\n",
      "Step 368: val_rewards = 0.6002083920165928 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 369: last loss = -0.53712\n",
      "eval step --\n",
      "\n",
      "Step 369: val_rewards = 0.5586800862706924 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 370: last loss = 0.08540\n",
      "eval step --\n",
      "\n",
      "Step 370: val_rewards = 0.6411783660254782 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 371: last loss = -1.04743\n",
      "eval step --\n",
      "\n",
      "Step 371: val_rewards = 0.4896974461584878 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 372: last loss = -0.15223\n",
      "eval step --\n",
      "\n",
      "Step 372: val_rewards = 0.45655848948807465 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 373: last loss = -0.67565\n",
      "eval step --\n",
      "\n",
      "Step 373: val_rewards = 0.40916614572911325 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 374: last loss = -0.56454\n",
      "eval step --\n",
      "\n",
      "Step 374: val_rewards = 0.5414128073078527 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 375: last loss = -0.35947\n",
      "eval step --\n",
      "\n",
      "Step 375: val_rewards = 0.696598069484794 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 376: last loss = 0.10032\n",
      "eval step --\n",
      "\n",
      "Step 376: val_rewards = 0.9912698566117748 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 377: last loss = -0.59663\n",
      "eval step --\n",
      "\n",
      "Step 377: val_rewards = 0.8850422978738499 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 378: last loss = 0.15342\n",
      "eval step --\n",
      "\n",
      "Step 378: val_rewards = 0.859677132421245 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 379: last loss = -0.00095\n",
      "eval step --\n",
      "\n",
      "Step 379: val_rewards = 0.5584625453648209 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 380: last loss = -0.25931\n",
      "eval step --\n",
      "\n",
      "Step 380: val_rewards = 0.3900422302853067 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 381: last loss = -0.01281\n",
      "eval step --\n",
      "\n",
      "Step 381: val_rewards = 0.6075624957409786 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 382: last loss = -0.25714\n",
      "eval step --\n",
      "\n",
      "Step 382: val_rewards = 0.5804087729371219 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 383: last loss = -0.82461\n",
      "eval step --\n",
      "\n",
      "Step 383: val_rewards = 0.6284156380031283 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 384: last loss = -0.01953\n",
      "eval step --\n",
      "\n",
      "Step 384: val_rewards = 0.6671830606258625 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 385: last loss = -0.10104\n",
      "eval step --\n",
      "\n",
      "Step 385: val_rewards = 0.48231254692778147 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 386: last loss = -0.56652\n",
      "eval step --\n",
      "\n",
      "Step 386: val_rewards = 0.47768266826993544 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 387: last loss = -0.67293\n",
      "eval step --\n",
      "\n",
      "Step 387: val_rewards = 0.391468404142364 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 388: last loss = -0.61453\n",
      "eval step --\n",
      "\n",
      "Step 388: val_rewards = 0.3802512733748763 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 389: last loss = -0.71020\n",
      "eval step --\n",
      "\n",
      "Step 389: val_rewards = 0.38183257779815666 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 390: last loss = -0.04538\n",
      "eval step --\n",
      "\n",
      "Step 390: val_rewards = 0.605602499539763 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 391: last loss = -0.54989\n",
      "eval step --\n",
      "\n",
      "Step 391: val_rewards = 0.7858398128539851 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 392: last loss = -0.63299\n",
      "eval step --\n",
      "\n",
      "Step 392: val_rewards = 0.5760095281235735 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 393: last loss = -0.08875\n",
      "eval step --\n",
      "\n",
      "Step 393: val_rewards = 0.4961449132573094 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 394: last loss = -0.81410\n",
      "eval step --\n",
      "\n",
      "Step 394: val_rewards = 0.453085430419146 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 395: last loss = 0.03887\n",
      "eval step --\n",
      "\n",
      "Step 395: val_rewards = 0.3809959119283818 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 396: last loss = -0.38171\n",
      "eval step --\n",
      "\n",
      "Step 396: val_rewards = 0.5522641482606516 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 397: last loss = -0.21407\n",
      "eval step --\n",
      "\n",
      "Step 397: val_rewards = 0.5450256361522455 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 398: last loss = -0.25849\n",
      "eval step --\n",
      "\n",
      "Step 398: val_rewards = 0.5076211269773085 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 399: last loss = -0.18729\n",
      "eval step --\n",
      "\n",
      "Step 399: val_rewards = 0.5449613078013478 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 400: last loss = -0.13339\n",
      "eval step --\n",
      "\n",
      "Step 400: val_rewards = 0.3509152191587727 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 401: last loss = 0.02428\n",
      "eval step --\n",
      "\n",
      "Step 401: val_rewards = 0.3426306738121173 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 402: last loss = 0.04535\n",
      "eval step --\n",
      "\n",
      "Step 402: val_rewards = 0.3270635621226516 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 403: last loss = -0.06008\n",
      "eval step --\n",
      "\n",
      "Step 403: val_rewards = 0.6836731239093679 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 404: last loss = -1.16878\n",
      "eval step --\n",
      "\n",
      "Step 404: val_rewards = 0.5624708941087291 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 405: last loss = 0.44021\n",
      "eval step --\n",
      "\n",
      "Step 405: val_rewards = 0.3394344496798408 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 406: last loss = -0.68077\n",
      "eval step --\n",
      "\n",
      "Step 406: val_rewards = 0.515394709384842 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 407: last loss = -0.12968\n",
      "eval step --\n",
      "\n",
      "Step 407: val_rewards = 0.548974892339917 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 408: last loss = -0.47288\n",
      "eval step --\n",
      "\n",
      "Step 408: val_rewards = 0.59694938448389 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 409: last loss = -1.04447\n",
      "eval step --\n",
      "\n",
      "Step 409: val_rewards = 0.5190921887311506 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 410: last loss = -0.28883\n",
      "eval step --\n",
      "\n",
      "Step 410: val_rewards = 0.5040947019722856 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 411: last loss = 0.00853\n",
      "eval step --\n",
      "\n",
      "Step 411: val_rewards = 0.7885491359428068 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 412: last loss = -0.46961\n",
      "eval step --\n",
      "\n",
      "Step 412: val_rewards = 0.7463133679273308 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 413: last loss = 0.47682\n",
      "eval step --\n",
      "\n",
      "Step 413: val_rewards = 0.6220967870207083 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 414: last loss = -0.02964\n",
      "eval step --\n",
      "\n",
      "Step 414: val_rewards = 0.5566599975736947 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 415: last loss = -0.70166\n",
      "eval step --\n",
      "\n",
      "Step 415: val_rewards = 0.6453584971166635 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 416: last loss = -1.42546\n",
      "eval step --\n",
      "\n",
      "Step 416: val_rewards = 0.15437418387230545 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 417: last loss = -0.48774\n",
      "eval step --\n",
      "\n",
      "Step 417: val_rewards = 0.28836970592776295 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 418: last loss = -0.05268\n",
      "eval step --\n",
      "\n",
      "Step 418: val_rewards = 0.18808060209428057 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 419: last loss = -0.06039\n",
      "eval step --\n",
      "\n",
      "Step 419: val_rewards = 0.27848898097127645 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 420: last loss = -0.21204\n",
      "eval step --\n",
      "\n",
      "Step 420: val_rewards = 0.44383293346473934 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 421: last loss = 0.11779\n",
      "eval step --\n",
      "\n",
      "Step 421: val_rewards = 0.5506100399524619 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 422: last loss = -0.51491\n",
      "eval step --\n",
      "\n",
      "Step 422: val_rewards = 0.5520716763043108 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 423: last loss = -1.06865\n",
      "eval step --\n",
      "\n",
      "Step 423: val_rewards = 0.8671863442904185 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 424: last loss = -0.61349\n",
      "eval step --\n",
      "\n",
      "Step 424: val_rewards = 0.8996039849418138 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 425: last loss = -0.62059\n",
      "eval step --\n",
      "\n",
      "Step 425: val_rewards = 0.9115254122561346 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 426: last loss = -0.15631\n",
      "eval step --\n",
      "\n",
      "Step 426: val_rewards = 0.8383962100971446 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 427: last loss = 0.08437\n",
      "eval step --\n",
      "\n",
      "Step 427: val_rewards = 0.7382479627413283 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 428: last loss = -1.49707\n",
      "eval step --\n",
      "\n",
      "Step 428: val_rewards = 0.822263171628136 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 429: last loss = -1.29194\n",
      "eval step --\n",
      "\n",
      "Step 429: val_rewards = 0.7872193807498983 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 430: last loss = -0.53596\n",
      "eval step --\n",
      "\n",
      "Step 430: val_rewards = 0.9633917226393948 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 431: last loss = -0.63696\n",
      "eval step --\n",
      "\n",
      "Step 431: val_rewards = 0.7594813137803355 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 432: last loss = -0.05889\n",
      "eval step --\n",
      "\n",
      "Step 432: val_rewards = 0.6097566758020511 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 433: last loss = -1.20039\n",
      "eval step --\n",
      "\n",
      "Step 433: val_rewards = 0.6524056630212145 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 434: last loss = -0.28180\n",
      "eval step --\n",
      "\n",
      "Step 434: val_rewards = 0.4106634749092559 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 435: last loss = -0.96168\n",
      "eval step --\n",
      "\n",
      "Step 435: val_rewards = 0.3279360595511723 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 436: last loss = -0.51129\n",
      "eval step --\n",
      "\n",
      "Step 436: val_rewards = 0.33063898553755844 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 437: last loss = -0.23843\n",
      "eval step --\n",
      "\n",
      "Step 437: val_rewards = 0.33952416812296055 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 438: last loss = 0.08488\n",
      "eval step --\n",
      "\n",
      "Step 438: val_rewards = 0.2772162269023912 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 439: last loss = -0.79049\n",
      "eval step --\n",
      "\n",
      "Step 439: val_rewards = 0.3431638613066791 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 440: last loss = 0.09324\n",
      "eval step --\n",
      "\n",
      "Step 440: val_rewards = 0.3736911268467778 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 441: last loss = -0.07266\n",
      "eval step --\n",
      "\n",
      "Step 441: val_rewards = 0.2400381733252756 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 442: last loss = -0.34238\n",
      "eval step --\n",
      "\n",
      "Step 442: val_rewards = 0.2869196091464465 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 443: last loss = -0.42952\n",
      "eval step --\n",
      "\n",
      "Step 443: val_rewards = 0.3457685867544327 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 444: last loss = -1.28226\n",
      "eval step --\n",
      "\n",
      "Step 444: val_rewards = 0.4526428581688525 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 445: last loss = -0.00138\n",
      "eval step --\n",
      "\n",
      "Step 445: val_rewards = 0.7908562750349084 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 446: last loss = -0.18529\n",
      "eval step --\n",
      "\n",
      "Step 446: val_rewards = 1.1588258799443962 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 447: last loss = -0.27928\n",
      "eval step --\n",
      "\n",
      "Step 447: val_rewards = 0.5537074772692433 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 448: last loss = -0.35033\n",
      "eval step --\n",
      "\n",
      "Step 448: val_rewards = 0.7051569111458299 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 449: last loss = -0.25626\n",
      "eval step --\n",
      "\n",
      "Step 449: val_rewards = 0.718707841777958 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 450: last loss = -0.38098\n",
      "eval step --\n",
      "\n",
      "Step 450: val_rewards = 0.8272415665115735 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 451: last loss = -0.41692\n",
      "eval step --\n",
      "\n",
      "Step 451: val_rewards = 0.4611497137676545 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 452: last loss = -0.38868\n",
      "eval step --\n",
      "\n",
      "Step 452: val_rewards = 0.5377786432764481 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 453: last loss = -0.10980\n",
      "eval step --\n",
      "\n",
      "Step 453: val_rewards = 0.39428364979880803 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 454: last loss = -0.18460\n",
      "eval step --\n",
      "\n",
      "Step 454: val_rewards = 0.4241124279764164 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 455: last loss = -0.14872\n",
      "eval step --\n",
      "\n",
      "Step 455: val_rewards = 0.35525419561129046 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 456: last loss = -0.63452\n",
      "eval step --\n",
      "\n",
      "Step 456: val_rewards = 0.36710441248406744 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 457: last loss = -0.05867\n",
      "eval step --\n",
      "\n",
      "Step 457: val_rewards = 0.44731308501623346 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 458: last loss = -0.32107\n",
      "eval step --\n",
      "\n",
      "Step 458: val_rewards = 0.3350056171548136 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 459: last loss = -0.13027\n",
      "eval step --\n",
      "\n",
      "Step 459: val_rewards = 0.5722621155063518 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 460: last loss = -0.80615\n",
      "eval step --\n",
      "\n",
      "Step 460: val_rewards = 0.5956014028059273 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 461: last loss = -0.12604\n",
      "eval step --\n",
      "\n",
      "Step 461: val_rewards = 0.5462921823824284 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 462: last loss = -0.26386\n",
      "eval step --\n",
      "\n",
      "Step 462: val_rewards = 0.3940930995879772 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 463: last loss = -0.25312\n",
      "eval step --\n",
      "\n",
      "Step 463: val_rewards = 0.35969345467843283 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 464: last loss = -0.25284\n",
      "eval step --\n",
      "\n",
      "Step 464: val_rewards = 0.37551499567928737 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 465: last loss = -0.70828\n",
      "eval step --\n",
      "\n",
      "Step 465: val_rewards = 0.39355568165765764 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 466: last loss = -0.18779\n",
      "eval step --\n",
      "\n",
      "Step 466: val_rewards = 0.4009069311723485 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 467: last loss = -0.23041\n",
      "eval step --\n",
      "\n",
      "Step 467: val_rewards = 0.39524046416831604 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 468: last loss = -0.55120\n",
      "eval step --\n",
      "\n",
      "Step 468: val_rewards = 0.3503274353359523 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 469: last loss = -0.27561\n",
      "eval step --\n",
      "\n",
      "Step 469: val_rewards = 0.3654788616357279 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 470: last loss = -0.42603\n",
      "eval step --\n",
      "\n",
      "Step 470: val_rewards = 0.29708402384074073 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 471: last loss = -0.05919\n",
      "eval step --\n",
      "\n",
      "Step 471: val_rewards = 0.27041420941643246 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 472: last loss = -0.13713\n",
      "eval step --\n",
      "\n",
      "Step 472: val_rewards = 0.3084623072629011 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 473: last loss = -0.28997\n",
      "eval step --\n",
      "\n",
      "Step 473: val_rewards = 0.5225568714055638 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 474: last loss = -0.99946\n",
      "eval step --\n",
      "\n",
      "Step 474: val_rewards = 0.29715280534912253 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 475: last loss = -1.32575\n",
      "eval step --\n",
      "\n",
      "Step 475: val_rewards = 0.13809261121872188 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 476: last loss = -1.34760\n",
      "eval step --\n",
      "\n",
      "Step 476: val_rewards = 0.124968942665694 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 477: last loss = -1.07483\n",
      "eval step --\n",
      "\n",
      "Step 477: val_rewards = 0.2432127126460245 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 478: last loss = -0.27122\n",
      "eval step --\n",
      "\n",
      "Step 478: val_rewards = 0.6608713669715243 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 479: last loss = -0.22616\n",
      "eval step --\n",
      "\n",
      "Step 479: val_rewards = 1.0401404477330798 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 480: last loss = -0.31581\n",
      "eval step --\n",
      "\n",
      "Step 480: val_rewards = 0.7314484940421351 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 481: last loss = 0.10004\n",
      "eval step --\n",
      "\n",
      "Step 481: val_rewards = 0.4833994659832112 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 482: last loss = -0.36762\n",
      "eval step --\n",
      "\n",
      "Step 482: val_rewards = 0.4362982828060025 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 483: last loss = -0.25186\n",
      "eval step --\n",
      "\n",
      "Step 483: val_rewards = 0.4125645773381479 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 484: last loss = -0.29854\n",
      "eval step --\n",
      "\n",
      "Step 484: val_rewards = 0.37323088191495274 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 485: last loss = -0.77228\n",
      "eval step --\n",
      "\n",
      "Step 485: val_rewards = 0.31286534198274707 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 486: last loss = -1.04799\n",
      "eval step --\n",
      "\n",
      "Step 486: val_rewards = 0.28624437111593526 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 487: last loss = -0.78097\n",
      "eval step --\n",
      "\n",
      "Step 487: val_rewards = 0.26523270964707363 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 488: last loss = -0.20703\n",
      "eval step --\n",
      "\n",
      "Step 488: val_rewards = 0.23324277332184537 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 489: last loss = -0.03229\n",
      "eval step --\n",
      "\n",
      "Step 489: val_rewards = 0.25936126109433916 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 490: last loss = -0.22176\n",
      "eval step --\n",
      "\n",
      "Step 490: val_rewards = 0.3048001359598573 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 491: last loss = -0.63972\n",
      "eval step --\n",
      "\n",
      "Step 491: val_rewards = 0.3012755588641906 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 492: last loss = -0.59665\n",
      "eval step --\n",
      "\n",
      "Step 492: val_rewards = 0.2914048925297357 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 493: last loss = -0.18987\n",
      "eval step --\n",
      "\n",
      "Step 493: val_rewards = 0.36650263381564807 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 494: last loss = -0.67234\n",
      "eval step --\n",
      "\n",
      "Step 494: val_rewards = 0.4003691796396263 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 495: last loss = -0.05149\n",
      "eval step --\n",
      "\n",
      "Step 495: val_rewards = 0.4524884390319966 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 496: last loss = -0.52964\n",
      "eval step --\n",
      "\n",
      "Step 496: val_rewards = 0.530199597102734 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 497: last loss = -0.91260\n",
      "eval step --\n",
      "\n",
      "Step 497: val_rewards = 0.5234539784208561 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 498: last loss = -0.05236\n",
      "eval step --\n",
      "\n",
      "Step 498: val_rewards = 0.4583947520113467 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 499: last loss = -0.17227\n",
      "eval step --\n",
      "\n",
      "Step 499: val_rewards = 0.4166649927913886 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 500: last loss = -0.08389\n",
      "eval step --\n",
      "\n",
      "Step 500: val_rewards = 0.3934343975006086 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 501: last loss = -0.14930\n",
      "eval step --\n",
      "\n",
      "Step 501: val_rewards = 0.33787616471801923 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 502: last loss = -0.58301\n",
      "eval step --\n",
      "\n",
      "Step 502: val_rewards = 0.36806728847188636 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 503: last loss = -0.58176\n",
      "eval step --\n",
      "\n",
      "Step 503: val_rewards = 0.3639374646401014 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 504: last loss = -0.31332\n",
      "eval step --\n",
      "\n",
      "Step 504: val_rewards = 0.3450982188541879 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 505: last loss = -0.50162\n",
      "eval step --\n",
      "\n",
      "Step 505: val_rewards = 0.3505003691164334 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 506: last loss = -0.78969\n",
      "eval step --\n",
      "\n",
      "Step 506: val_rewards = 0.289669844154429 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 507: last loss = 0.06542\n",
      "eval step --\n",
      "\n",
      "Step 507: val_rewards = 0.2589495649382678 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 508: last loss = -1.02157\n",
      "eval step --\n",
      "\n",
      "Step 508: val_rewards = 0.3505007158545034 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 509: last loss = -0.09593\n",
      "eval step --\n",
      "\n",
      "Step 509: val_rewards = 0.4286623682325717 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 510: last loss = -0.22901\n",
      "eval step --\n",
      "\n",
      "Step 510: val_rewards = 0.5560879230241427 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 511: last loss = -0.86361\n",
      "eval step --\n",
      "\n",
      "Step 511: val_rewards = 0.5819357977203564 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 512: last loss = -0.15603\n",
      "eval step --\n",
      "\n",
      "Step 512: val_rewards = 0.574134932003525 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 513: last loss = -0.77399\n",
      "eval step --\n",
      "\n",
      "Step 513: val_rewards = 0.559913177433913 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 514: last loss = -0.95493\n",
      "eval step --\n",
      "\n",
      "Step 514: val_rewards = 0.6279709096683467 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 515: last loss = -0.21091\n",
      "eval step --\n",
      "\n",
      "Step 515: val_rewards = 0.7476076169757162 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 516: last loss = -0.04290\n",
      "eval step --\n",
      "\n",
      "Step 516: val_rewards = 0.84143489448868 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 517: last loss = -0.64214\n",
      "eval step --\n",
      "\n",
      "Step 517: val_rewards = 0.821757119856182 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 518: last loss = -0.62378\n",
      "eval step --\n",
      "\n",
      "Step 518: val_rewards = 0.8272723413027322 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 519: last loss = -1.01832\n",
      "eval step --\n",
      "\n",
      "Step 519: val_rewards = 0.8126542812124128 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 520: last loss = -0.37054\n",
      "eval step --\n",
      "\n",
      "Step 520: val_rewards = 0.737693946026205 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 521: last loss = -0.32544\n",
      "eval step --\n",
      "\n",
      "Step 521: val_rewards = 0.7100001363442139 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 522: last loss = -0.42422\n",
      "eval step --\n",
      "\n",
      "Step 522: val_rewards = 0.7190784050364831 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 523: last loss = -0.24326\n",
      "eval step --\n",
      "\n",
      "Step 523: val_rewards = 0.7401505784960989 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 524: last loss = -0.57980\n",
      "eval step --\n",
      "\n",
      "Step 524: val_rewards = 0.7643074187817421 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 525: last loss = -0.34182\n",
      "eval step --\n",
      "\n",
      "Step 525: val_rewards = 0.7356019718697815 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 526: last loss = -0.68352\n",
      "eval step --\n",
      "\n",
      "Step 526: val_rewards = 0.5184159997680571 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 527: last loss = -0.97210\n",
      "eval step --\n",
      "\n",
      "Step 527: val_rewards = 0.49336564161118474 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 528: last loss = -0.65026\n",
      "eval step --\n",
      "\n",
      "Step 528: val_rewards = 0.41187596664544374 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 529: last loss = -0.02535\n",
      "eval step --\n",
      "\n",
      "Step 529: val_rewards = 0.025407643275335203 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 530: last loss = -0.08485\n",
      "eval step --\n",
      "\n",
      "Step 530: val_rewards = 0.09858364014151429 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 531: last loss = -0.08797\n",
      "eval step --\n",
      "\n",
      "Step 531: val_rewards = 0.35713795613442023 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 532: last loss = 0.03730\n",
      "eval step --\n",
      "\n",
      "Step 532: val_rewards = 0.5830769597696551 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 533: last loss = -0.04016\n",
      "eval step --\n",
      "\n",
      "Step 533: val_rewards = 0.39872957793705366 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 534: last loss = 0.04459\n",
      "eval step --\n",
      "\n",
      "Step 534: val_rewards = 0.07523105487811664 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 535: last loss = -1.34907\n",
      "eval step --\n",
      "\n",
      "Step 535: val_rewards = 0.28086982675122185 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 536: last loss = -0.63025\n",
      "eval step --\n",
      "\n",
      "Step 536: val_rewards = 0.22710542152644728 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 537: last loss = -0.31778\n",
      "eval step --\n",
      "\n",
      "Step 537: val_rewards = 0.5015937335029468 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 538: last loss = 0.15353\n",
      "eval step --\n",
      "\n",
      "Step 538: val_rewards = 0.2693671658819552 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 539: last loss = 0.14528\n",
      "eval step --\n",
      "\n",
      "Step 539: val_rewards = 0.617064487448771 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 540: last loss = -0.04812\n",
      "eval step --\n",
      "\n",
      "Step 540: val_rewards = 0.665639933704413 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 541: last loss = -0.02147\n",
      "eval step --\n",
      "\n",
      "Step 541: val_rewards = 0.6816001538685271 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 542: last loss = -0.26303\n",
      "eval step --\n",
      "\n",
      "Step 542: val_rewards = 0.4153570288273543 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 543: last loss = 0.13399\n",
      "eval step --\n",
      "\n",
      "Step 543: val_rewards = 0.39193861756888393 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 544: last loss = -0.88533\n",
      "eval step --\n",
      "\n",
      "Step 544: val_rewards = 0.3379157314587495 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 545: last loss = -0.13371\n",
      "eval step --\n",
      "\n",
      "Step 545: val_rewards = 0.4975379328849575 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 546: last loss = -0.05110\n",
      "eval step --\n",
      "\n",
      "Step 546: val_rewards = 0.7026850651682803 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 547: last loss = -0.49205\n",
      "eval step --\n",
      "\n",
      "Step 547: val_rewards = 0.7619852338919234 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 548: last loss = -1.14104\n",
      "eval step --\n",
      "\n",
      "Step 548: val_rewards = 0.4938586521571052 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 549: last loss = -0.15812\n",
      "eval step --\n",
      "\n",
      "Step 549: val_rewards = 0.7251514872208444 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 550: last loss = -0.30085\n",
      "eval step --\n",
      "\n",
      "Step 550: val_rewards = 0.4038910914865089 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 551: last loss = 0.21352\n",
      "eval step --\n",
      "\n",
      "Step 551: val_rewards = 0.6287921988426304 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 552: last loss = -0.26094\n",
      "eval step --\n",
      "\n",
      "Step 552: val_rewards = 0.5695642726465318 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 553: last loss = -0.12548\n",
      "eval step --\n",
      "\n",
      "Step 553: val_rewards = 0.5817788584371282 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 554: last loss = -0.17561\n",
      "eval step --\n",
      "\n",
      "Step 554: val_rewards = 0.5518818660938509 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 555: last loss = -0.00171\n",
      "eval step --\n",
      "\n",
      "Step 555: val_rewards = 0.5789969855208733 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 556: last loss = -0.29572\n",
      "eval step --\n",
      "\n",
      "Step 556: val_rewards = 0.5476140873105402 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 557: last loss = -0.12753\n",
      "eval step --\n",
      "\n",
      "Step 557: val_rewards = 0.4117498936612647 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 558: last loss = -0.51205\n",
      "eval step --\n",
      "\n",
      "Step 558: val_rewards = 0.3735100981626889 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 559: last loss = -0.99974\n",
      "eval step --\n",
      "\n",
      "Step 559: val_rewards = 0.39873103678155847 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 560: last loss = -1.28542\n",
      "eval step --\n",
      "\n",
      "Step 560: val_rewards = 0.16056431941924446 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 561: last loss = -0.59304\n",
      "eval step --\n",
      "\n",
      "Step 561: val_rewards = 0.36588194376500155 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 562: last loss = 0.21917\n",
      "eval step --\n",
      "\n",
      "Step 562: val_rewards = 0.4916022633710578 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 563: last loss = -0.63223\n",
      "eval step --\n",
      "\n",
      "Step 563: val_rewards = 0.5241624826398432 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 564: last loss = -0.02581\n",
      "eval step --\n",
      "\n",
      "Step 564: val_rewards = 0.43214735938529975 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 565: last loss = -0.64705\n",
      "eval step --\n",
      "\n",
      "Step 565: val_rewards = 0.4995214722080198 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 566: last loss = -0.55275\n",
      "eval step --\n",
      "\n",
      "Step 566: val_rewards = 0.5869806080277582 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 567: last loss = -1.03708\n",
      "eval step --\n",
      "\n",
      "Step 567: val_rewards = 0.5392404941621607 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 568: last loss = -0.70599\n",
      "eval step --\n",
      "\n",
      "Step 568: val_rewards = 0.5656135016717836 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 569: last loss = -0.55908\n",
      "eval step --\n",
      "\n",
      "Step 569: val_rewards = 0.5968518418896354 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 570: last loss = 0.06409\n",
      "eval step --\n",
      "\n",
      "Step 570: val_rewards = 0.8187738500426096 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 571: last loss = -0.15760\n",
      "eval step --\n",
      "\n",
      "Step 571: val_rewards = 1.230371400508649 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 572: last loss = -0.19595\n",
      "eval step --\n",
      "\n",
      "Step 572: val_rewards = 0.6760422655294326 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 573: last loss = -0.24724\n",
      "eval step --\n",
      "\n",
      "Step 573: val_rewards = 0.27109125557362757 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 574: last loss = -0.10751\n",
      "eval step --\n",
      "\n",
      "Step 574: val_rewards = 0.5028137307622174 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 575: last loss = -0.62022\n",
      "eval step --\n",
      "\n",
      "Step 575: val_rewards = 0.6795757822223509 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 576: last loss = -0.21905\n",
      "eval step --\n",
      "\n",
      "Step 576: val_rewards = 0.5053957709105078 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 577: last loss = -0.25091\n",
      "eval step --\n",
      "\n",
      "Step 577: val_rewards = 0.44275965768940334 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 578: last loss = -0.24029\n",
      "eval step --\n",
      "\n",
      "Step 578: val_rewards = 0.2844657648850157 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 579: last loss = 0.05493\n",
      "eval step --\n",
      "\n",
      "Step 579: val_rewards = 0.22099470391030956 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 580: last loss = -0.55142\n",
      "eval step --\n",
      "\n",
      "Step 580: val_rewards = 0.16403633952577903 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 581: last loss = 0.08286\n",
      "eval step --\n",
      "\n",
      "Step 581: val_rewards = 0.2032554956930974 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 582: last loss = -0.14957\n",
      "eval step --\n",
      "\n",
      "Step 582: val_rewards = 0.2218553160137484 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 583: last loss = -0.02886\n",
      "eval step --\n",
      "\n",
      "Step 583: val_rewards = 0.22006099327458553 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 584: last loss = -0.09932\n",
      "eval step --\n",
      "\n",
      "Step 584: val_rewards = 0.19388369439296677 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 585: last loss = 0.00164\n",
      "eval step --\n",
      "\n",
      "Step 585: val_rewards = 0.19387535434715494 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 586: last loss = -0.14376\n",
      "eval step --\n",
      "\n",
      "Step 586: val_rewards = 0.20234920763711545 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 587: last loss = -0.03001\n",
      "eval step --\n",
      "\n",
      "Step 587: val_rewards = 0.22774335053957429 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 588: last loss = -0.19354\n",
      "eval step --\n",
      "\n",
      "Step 588: val_rewards = 0.2652135342275019 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 589: last loss = -0.23592\n",
      "eval step --\n",
      "\n",
      "Step 589: val_rewards = 0.3283112224928684 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 590: last loss = -0.01832\n",
      "eval step --\n",
      "\n",
      "Step 590: val_rewards = 0.2487059764559141 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 591: last loss = -0.74485\n",
      "eval step --\n",
      "\n",
      "Step 591: val_rewards = 0.3488301541594941 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 592: last loss = 0.06480\n",
      "eval step --\n",
      "\n",
      "Step 592: val_rewards = 0.4187696072072113 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 593: last loss = -0.14804\n",
      "eval step --\n",
      "\n",
      "Step 593: val_rewards = 0.45841316115984854 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 594: last loss = -0.73159\n",
      "eval step --\n",
      "\n",
      "Step 594: val_rewards = 0.460726958469758 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 595: last loss = -0.85787\n",
      "eval step --\n",
      "\n",
      "Step 595: val_rewards = 0.442520568080797 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 596: last loss = -0.35199\n",
      "eval step --\n",
      "\n",
      "Step 596: val_rewards = 0.4685010305986281 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 597: last loss = -0.04334\n",
      "eval step --\n",
      "\n",
      "Step 597: val_rewards = 0.4857813568748075 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 598: last loss = 0.04316\n",
      "eval step --\n",
      "\n",
      "Step 598: val_rewards = 0.565652290904476 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 599: last loss = -0.41494\n",
      "eval step --\n",
      "\n",
      "Step 599: val_rewards = 0.5240697165175718 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 600: last loss = -0.25123\n",
      "eval step --\n",
      "\n",
      "Step 600: val_rewards = 0.5579959499267324 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 601: last loss = -0.30536\n",
      "eval step --\n",
      "\n",
      "Step 601: val_rewards = 0.5351355085370674 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 602: last loss = -0.56605\n",
      "eval step --\n",
      "\n",
      "Step 602: val_rewards = 0.6006520594935835 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 603: last loss = -0.17580\n",
      "eval step --\n",
      "\n",
      "Step 603: val_rewards = 0.725133731671173 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 604: last loss = -0.41164\n",
      "eval step --\n",
      "\n",
      "Step 604: val_rewards = 0.7471387876485373 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 605: last loss = -1.02358\n",
      "eval step --\n",
      "\n",
      "Step 605: val_rewards = 1.1587658823766582 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 606: last loss = -0.35267\n",
      "eval step --\n",
      "\n",
      "Step 606: val_rewards = 0.8846882788776733 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 607: last loss = -1.00648\n",
      "eval step --\n",
      "\n",
      "Step 607: val_rewards = 0.5999599170381127 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 608: last loss = 0.04749\n",
      "eval step --\n",
      "\n",
      "Step 608: val_rewards = 0.486792068335859 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 609: last loss = -0.28096\n",
      "eval step --\n",
      "\n",
      "Step 609: val_rewards = 0.5869137256253933 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 610: last loss = 0.00110\n",
      "eval step --\n",
      "\n",
      "Step 610: val_rewards = 0.6884786936783276 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 611: last loss = -0.95367\n",
      "eval step --\n",
      "\n",
      "Step 611: val_rewards = 0.7581077867360357 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 612: last loss = -0.25976\n",
      "eval step --\n",
      "\n",
      "Step 612: val_rewards = 0.7559895301440754 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 613: last loss = -0.90049\n",
      "eval step --\n",
      "\n",
      "Step 613: val_rewards = 0.8201169969851816 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 614: last loss = -1.85368\n",
      "eval step --\n",
      "\n",
      "Step 614: val_rewards = 0.5890469228138304 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 615: last loss = -0.03208\n",
      "eval step --\n",
      "\n",
      "Step 615: val_rewards = 0.2514084058456457 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 616: last loss = -0.38208\n",
      "eval step --\n",
      "\n",
      "Step 616: val_rewards = 0.12480109532086137 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 617: last loss = -0.73894\n",
      "eval step --\n",
      "\n",
      "Step 617: val_rewards = 0.20708407115275415 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 618: last loss = -0.09480\n",
      "eval step --\n",
      "\n",
      "Step 618: val_rewards = 0.2889037644010353 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 619: last loss = -0.22398\n",
      "eval step --\n",
      "\n",
      "Step 619: val_rewards = 0.48254210347569365 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 620: last loss = -0.83612\n",
      "eval step --\n",
      "\n",
      "Step 620: val_rewards = 0.47637718296145015 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 621: last loss = -0.20000\n",
      "eval step --\n",
      "\n",
      "Step 621: val_rewards = 0.6489143864108707 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 622: last loss = -0.14288\n",
      "eval step --\n",
      "\n",
      "Step 622: val_rewards = 0.5800334851252874 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 623: last loss = -0.18637\n",
      "eval step --\n",
      "\n",
      "Step 623: val_rewards = 0.41543492315698843 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 624: last loss = -0.15802\n",
      "eval step --\n",
      "\n",
      "Step 624: val_rewards = 0.4730151261113957 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 625: last loss = 0.02860\n",
      "eval step --\n",
      "\n",
      "Step 625: val_rewards = 0.3172656721648774 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 626: last loss = -0.13222\n",
      "eval step --\n",
      "\n",
      "Step 626: val_rewards = 0.4924149340838149 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 627: last loss = -0.05945\n",
      "eval step --\n",
      "\n",
      "Step 627: val_rewards = 0.49881299005941604 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 628: last loss = -0.05301\n",
      "eval step --\n",
      "\n",
      "Step 628: val_rewards = 0.3646586508864065 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 629: last loss = -0.83561\n",
      "eval step --\n",
      "\n",
      "Step 629: val_rewards = 0.6367404485148596 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 630: last loss = -0.67675\n",
      "eval step --\n",
      "\n",
      "Step 630: val_rewards = 0.4832896601597154 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 631: last loss = -0.89759\n",
      "eval step --\n",
      "\n",
      "Step 631: val_rewards = 0.283237478260585 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 632: last loss = -0.15756\n",
      "eval step --\n",
      "\n",
      "Step 632: val_rewards = 0.24491871485415803 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 633: last loss = -0.31150\n",
      "eval step --\n",
      "\n",
      "Step 633: val_rewards = 0.36477609515803217 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 634: last loss = -0.12132\n",
      "eval step --\n",
      "\n",
      "Step 634: val_rewards = 0.3540190185382357 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 635: last loss = -0.97688\n",
      "eval step --\n",
      "\n",
      "Step 635: val_rewards = 0.527251512109868 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 636: last loss = -0.10581\n",
      "eval step --\n",
      "\n",
      "Step 636: val_rewards = 0.747454417946036 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 637: last loss = -0.51958\n",
      "eval step --\n",
      "\n",
      "Step 637: val_rewards = 0.7453565745645024 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 638: last loss = 0.21064\n",
      "eval step --\n",
      "\n",
      "Step 638: val_rewards = 0.6532032991310149 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 639: last loss = -0.06943\n",
      "eval step --\n",
      "\n",
      "Step 639: val_rewards = 0.4494215533402251 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 640: last loss = -0.17814\n",
      "eval step --\n",
      "\n",
      "Step 640: val_rewards = 0.5896638908543269 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 641: last loss = -0.04503\n",
      "eval step --\n",
      "\n",
      "Step 641: val_rewards = 0.5924144419983715 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 642: last loss = -0.92234\n",
      "eval step --\n",
      "\n",
      "Step 642: val_rewards = 0.6762544796217635 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 643: last loss = 0.26773\n",
      "eval step --\n",
      "\n",
      "Step 643: val_rewards = 0.6762139809894984 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 644: last loss = 0.05703\n",
      "eval step --\n",
      "\n",
      "Step 644: val_rewards = 0.6528719540571485 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 645: last loss = -0.47063\n",
      "eval step --\n",
      "\n",
      "Step 645: val_rewards = 0.6493852768437167 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 646: last loss = -0.84988\n",
      "eval step --\n",
      "\n",
      "Step 646: val_rewards = 0.5588304167352327 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 647: last loss = -0.17382\n",
      "eval step --\n",
      "\n",
      "Step 647: val_rewards = 0.5588083627982983 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 648: last loss = 0.03453\n",
      "eval step --\n",
      "\n",
      "Step 648: val_rewards = 0.558787291583501 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 649: last loss = 0.17305\n",
      "eval step --\n",
      "\n",
      "Step 649: val_rewards = 0.5587820642718364 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 650: last loss = 0.34006\n",
      "eval step --\n",
      "\n",
      "Step 650: val_rewards = 0.6312730918922926 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 651: last loss = -0.17927\n",
      "eval step --\n",
      "\n",
      "Step 651: val_rewards = 0.6558288801011253 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 652: last loss = -0.05221\n",
      "eval step --\n",
      "\n",
      "Step 652: val_rewards = 0.6558282757892567 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 653: last loss = -0.11901\n",
      "eval step --\n",
      "\n",
      "Step 653: val_rewards = 0.65447326466318 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 654: last loss = 0.01015\n",
      "eval step --\n",
      "\n",
      "Step 654: val_rewards = 0.6544677233543671 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 655: last loss = 0.03932\n",
      "eval step --\n",
      "\n",
      "Step 655: val_rewards = 0.6544660279569481 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 656: last loss = -0.60434\n",
      "eval step --\n",
      "\n",
      "Step 656: val_rewards = 0.6544659135143266 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 657: last loss = 0.21852\n",
      "eval step --\n",
      "\n",
      "Step 657: val_rewards = 0.6544717555933198 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 658: last loss = -0.09365\n",
      "eval step --\n",
      "\n",
      "Step 658: val_rewards = 0.654475672750269 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 659: last loss = -0.06866\n",
      "eval step --\n",
      "\n",
      "Step 659: val_rewards = 0.6967785005937549 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 660: last loss = 0.08300\n",
      "eval step --\n",
      "\n",
      "Step 660: val_rewards = 0.6967820413246004 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 661: last loss = 0.09478\n",
      "eval step --\n",
      "\n",
      "Step 661: val_rewards = 0.696774054177658 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 662: last loss = -1.05539\n",
      "eval step --\n",
      "\n",
      "Step 662: val_rewards = 0.5816164423689646 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 663: last loss = -0.02911\n",
      "eval step --\n",
      "\n",
      "Step 663: val_rewards = 0.5764289234679121 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 664: last loss = -0.29185\n",
      "eval step --\n",
      "\n",
      "Step 664: val_rewards = 0.5537065319546557 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 665: last loss = -0.12411\n",
      "eval step --\n",
      "\n",
      "Step 665: val_rewards = 0.4941755303384135 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 666: last loss = -0.29763\n",
      "eval step --\n",
      "\n",
      "Step 666: val_rewards = 0.708537692735059 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 667: last loss = -0.70610\n",
      "eval step --\n",
      "\n",
      "Step 667: val_rewards = 0.7575571859501202 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 668: last loss = 0.10165\n",
      "eval step --\n",
      "\n",
      "Step 668: val_rewards = 0.3215124092525905 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 669: last loss = -0.26609\n",
      "eval step --\n",
      "\n",
      "Step 669: val_rewards = 0.40164889965276696 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 670: last loss = -0.73865\n",
      "eval step --\n",
      "\n",
      "Step 670: val_rewards = 0.4788329350011989 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 671: last loss = -1.03068\n",
      "eval step --\n",
      "\n",
      "Step 671: val_rewards = 0.38307552095431274 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 672: last loss = 0.14143\n",
      "eval step --\n",
      "\n",
      "Step 672: val_rewards = 0.45095921250391346 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 673: last loss = -0.80659\n",
      "eval step --\n",
      "\n",
      "Step 673: val_rewards = 0.5510920117054346 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 674: last loss = 0.06638\n",
      "eval step --\n",
      "\n",
      "Step 674: val_rewards = 0.44164958118862613 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 675: last loss = -0.77368\n",
      "eval step --\n",
      "\n",
      "Step 675: val_rewards = 0.5143541596624064 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 676: last loss = -0.17380\n",
      "eval step --\n",
      "\n",
      "Step 676: val_rewards = 0.5143940677439415 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 677: last loss = -0.07547\n",
      "eval step --\n",
      "\n",
      "Step 677: val_rewards = 0.5144277565691404 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 678: last loss = 0.22257\n",
      "eval step --\n",
      "\n",
      "Step 678: val_rewards = 0.5144569117535506 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 679: last loss = -0.74116\n",
      "eval step --\n",
      "\n",
      "Step 679: val_rewards = 0.5144862198410182 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 680: last loss = -0.20220\n",
      "eval step --\n",
      "\n",
      "Step 680: val_rewards = 0.507737141499354 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 681: last loss = -0.34061\n",
      "eval step --\n",
      "\n",
      "Step 681: val_rewards = 0.49663786239014074 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 682: last loss = -0.23405\n",
      "eval step --\n",
      "\n",
      "Step 682: val_rewards = 0.5113339203716323 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 683: last loss = -0.55248\n",
      "eval step --\n",
      "\n",
      "Step 683: val_rewards = 0.542525349838868 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 684: last loss = 0.40111\n",
      "eval step --\n",
      "\n",
      "Step 684: val_rewards = 0.49237456804725943 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 685: last loss = -0.21310\n",
      "eval step --\n",
      "\n",
      "Step 685: val_rewards = 0.49237412163442257 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 686: last loss = -0.96795\n",
      "eval step --\n",
      "\n",
      "Step 686: val_rewards = 0.47283197595306 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 687: last loss = -0.89605\n",
      "eval step --\n",
      "\n",
      "Step 687: val_rewards = 0.39100284459400864 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 688: last loss = -0.27226\n",
      "eval step --\n",
      "\n",
      "Step 688: val_rewards = 0.4509116951918533 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 689: last loss = -0.91851\n",
      "eval step --\n",
      "\n",
      "Step 689: val_rewards = 0.4189376225918829 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 690: last loss = 0.12837\n",
      "eval step --\n",
      "\n",
      "Step 690: val_rewards = 0.5857057719855636 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 691: last loss = -0.28882\n",
      "eval step --\n",
      "\n",
      "Step 691: val_rewards = 0.6193590445139658 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 692: last loss = -0.23345\n",
      "eval step --\n",
      "\n",
      "Step 692: val_rewards = 0.4891802124931811 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 693: last loss = -1.43154\n",
      "eval step --\n",
      "\n",
      "Step 693: val_rewards = 0.5445895733999383 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 694: last loss = -0.10163\n",
      "eval step --\n",
      "\n",
      "Step 694: val_rewards = 0.5489823100816132 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 695: last loss = 0.10815\n",
      "eval step --\n",
      "\n",
      "Step 695: val_rewards = 0.646279506241958 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 696: last loss = -0.86264\n",
      "eval step --\n",
      "\n",
      "Step 696: val_rewards = 0.6214666645217781 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 697: last loss = -0.49630\n",
      "eval step --\n",
      "\n",
      "Step 697: val_rewards = 0.6266758894829225 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 698: last loss = -0.53190\n",
      "eval step --\n",
      "\n",
      "Step 698: val_rewards = 0.626539978548963 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 699: last loss = -0.90556\n",
      "eval step --\n",
      "\n",
      "Step 699: val_rewards = 0.6144517898730534 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 700: last loss = -0.22287\n",
      "eval step --\n",
      "\n",
      "Step 700: val_rewards = 0.5603978288452561 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 701: last loss = 0.03331\n",
      "eval step --\n",
      "\n",
      "Step 701: val_rewards = 0.4631353338155649 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 702: last loss = -0.16177\n",
      "eval step --\n",
      "\n",
      "Step 702: val_rewards = 0.5316315732030561 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 703: last loss = -0.14571\n",
      "eval step --\n",
      "\n",
      "Step 703: val_rewards = 0.47715784805177036 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 704: last loss = -1.65207\n",
      "eval step --\n",
      "\n",
      "Step 704: val_rewards = 0.4788782766172618 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 705: last loss = -0.99190\n",
      "eval step --\n",
      "\n",
      "Step 705: val_rewards = 0.44779429000677445 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 706: last loss = -0.48990\n",
      "eval step --\n",
      "\n",
      "Step 706: val_rewards = 0.4866144050374706 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 707: last loss = -0.65604\n",
      "eval step --\n",
      "\n",
      "Step 707: val_rewards = 0.4926875673833896 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 708: last loss = 0.14570\n",
      "eval step --\n",
      "\n",
      "Step 708: val_rewards = 0.492698687600165 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 709: last loss = -1.04627\n",
      "eval step --\n",
      "\n",
      "Step 709: val_rewards = 0.3017023354602412 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 710: last loss = -0.93470\n",
      "eval step --\n",
      "\n",
      "Step 710: val_rewards = 0.25499247202836384 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 711: last loss = -0.89822\n",
      "eval step --\n",
      "\n",
      "Step 711: val_rewards = 0.24303332196033037 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 712: last loss = -0.31347\n",
      "eval step --\n",
      "\n",
      "Step 712: val_rewards = 0.22774429937506113 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 713: last loss = -0.25627\n",
      "eval step --\n",
      "\n",
      "Step 713: val_rewards = 0.26930897446982094 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 714: last loss = -0.89843\n",
      "eval step --\n",
      "\n",
      "Step 714: val_rewards = 0.25732589975007664 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 715: last loss = 0.06649\n",
      "eval step --\n",
      "\n",
      "Step 715: val_rewards = 0.2645037068132355 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 716: last loss = 0.00496\n",
      "eval step --\n",
      "\n",
      "Step 716: val_rewards = 0.2921587798202467 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 717: last loss = -1.11309\n",
      "eval step --\n",
      "\n",
      "Step 717: val_rewards = 0.3406185509336019 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 718: last loss = -0.91649\n",
      "eval step --\n",
      "\n",
      "Step 718: val_rewards = 0.366639193836267 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 719: last loss = -0.82766\n",
      "eval step --\n",
      "\n",
      "Step 719: val_rewards = 0.33886125076566515 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 720: last loss = -0.91785\n",
      "eval step --\n",
      "\n",
      "Step 720: val_rewards = 0.3391141919967729 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 721: last loss = -0.35552\n",
      "eval step --\n",
      "\n",
      "Step 721: val_rewards = 0.33910797018274497 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 722: last loss = -0.12408\n",
      "eval step --\n",
      "\n",
      "Step 722: val_rewards = 0.34004358763386955 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 723: last loss = -0.14993\n",
      "eval step --\n",
      "\n",
      "Step 723: val_rewards = 0.3586407439661999 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 724: last loss = -0.90559\n",
      "eval step --\n",
      "\n",
      "Step 724: val_rewards = 0.33574088813904657 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 725: last loss = -0.18669\n",
      "eval step --\n",
      "\n",
      "Step 725: val_rewards = 0.29766140599647256 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 726: last loss = -0.00495\n",
      "eval step --\n",
      "\n",
      "Step 726: val_rewards = 0.4881762575224438 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 727: last loss = -0.16377\n",
      "eval step --\n",
      "\n",
      "Step 727: val_rewards = 0.4813693383000438 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 728: last loss = 0.25803\n",
      "eval step --\n",
      "\n",
      "Step 728: val_rewards = 0.4813635658003297 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 729: last loss = 0.05010\n",
      "eval step --\n",
      "\n",
      "Step 729: val_rewards = 0.43728264209143886 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 730: last loss = -0.59022\n",
      "eval step --\n",
      "\n",
      "Step 730: val_rewards = 0.4372815694404919 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 731: last loss = -0.05970\n",
      "eval step --\n",
      "\n",
      "Step 731: val_rewards = 0.44407028356303924 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 732: last loss = -0.26067\n",
      "eval step --\n",
      "\n",
      "Step 732: val_rewards = 0.46830541511539897 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 733: last loss = -0.36281\n",
      "eval step --\n",
      "\n",
      "Step 733: val_rewards = 0.46804628010353605 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 734: last loss = -0.56738\n",
      "eval step --\n",
      "\n",
      "Step 734: val_rewards = 0.5120136176658278 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 735: last loss = 0.04638\n",
      "eval step --\n",
      "\n",
      "Step 735: val_rewards = 0.5775959698441064 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 736: last loss = -0.27515\n",
      "eval step --\n",
      "\n",
      "Step 736: val_rewards = 0.6533719819316763 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 737: last loss = -0.16014\n",
      "eval step --\n",
      "\n",
      "Step 737: val_rewards = 0.5701080714948256 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 738: last loss = -0.09861\n",
      "eval step --\n",
      "\n",
      "Step 738: val_rewards = 0.5701447399061309 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 739: last loss = -0.15881\n",
      "eval step --\n",
      "\n",
      "Step 739: val_rewards = 0.5701854740026355 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 740: last loss = -0.11568\n",
      "eval step --\n",
      "\n",
      "Step 740: val_rewards = 0.5763475815612065 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 741: last loss = -0.06853\n",
      "eval step --\n",
      "\n",
      "Step 741: val_rewards = 0.5763791256401152 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 742: last loss = -0.12640\n",
      "eval step --\n",
      "\n",
      "Step 742: val_rewards = 0.5860966647227744 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 743: last loss = -0.75803\n",
      "eval step --\n",
      "\n",
      "Step 743: val_rewards = 0.6023561432294896 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 744: last loss = -0.20590\n",
      "eval step --\n",
      "\n",
      "Step 744: val_rewards = 0.6023747691507725 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 745: last loss = -0.06491\n",
      "eval step --\n",
      "\n",
      "Step 745: val_rewards = 0.6023880701202717 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 746: last loss = 0.19752\n",
      "eval step --\n",
      "\n",
      "Step 746: val_rewards = 0.6024262599777986 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 747: last loss = -0.45758\n",
      "eval step --\n",
      "\n",
      "Step 747: val_rewards = 0.5265643607759573 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 748: last loss = 0.03273\n",
      "eval step --\n",
      "\n",
      "Step 748: val_rewards = 0.5266045998460632 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 749: last loss = -0.62118\n",
      "eval step --\n",
      "\n",
      "Step 749: val_rewards = 0.5266425191352361 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 750: last loss = -0.15762\n",
      "eval step --\n",
      "\n",
      "Step 750: val_rewards = 0.5701943682623103 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 751: last loss = 0.15820\n",
      "eval step --\n",
      "\n",
      "Step 751: val_rewards = 0.5642909682743021 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 752: last loss = 0.08419\n",
      "eval step --\n",
      "\n",
      "Step 752: val_rewards = 0.564320035746789 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 753: last loss = -0.13680\n",
      "eval step --\n",
      "\n",
      "Step 753: val_rewards = 0.564332391157 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 754: last loss = -0.09513\n",
      "eval step --\n",
      "\n",
      "Step 754: val_rewards = 0.5643267066988417 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 755: last loss = 0.03007\n",
      "eval step --\n",
      "\n",
      "Step 755: val_rewards = 0.5643279769618001 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 756: last loss = -0.04073\n",
      "eval step --\n",
      "\n",
      "Step 756: val_rewards = 0.587409206977561 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 757: last loss = -0.79282\n",
      "eval step --\n",
      "\n",
      "Step 757: val_rewards = 0.3417812586582748 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 758: last loss = -0.24595\n",
      "eval step --\n",
      "\n",
      "Step 758: val_rewards = 0.423886431741071 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 759: last loss = -0.30786\n",
      "eval step --\n",
      "\n",
      "Step 759: val_rewards = 0.2967021421015236 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 760: last loss = -1.13042\n",
      "eval step --\n",
      "\n",
      "Step 760: val_rewards = 0.30688677345068704 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 761: last loss = -0.27328\n",
      "eval step --\n",
      "\n",
      "Step 761: val_rewards = 0.3510852761879365 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 762: last loss = 0.03953\n",
      "eval step --\n",
      "\n",
      "Step 762: val_rewards = 0.4178136632581144 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 763: last loss = -0.64400\n",
      "eval step --\n",
      "\n",
      "Step 763: val_rewards = 0.5184130888583287 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 764: last loss = -1.11950\n",
      "eval step --\n",
      "\n",
      "Step 764: val_rewards = 0.33635660721208255 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 765: last loss = -0.07425\n",
      "eval step --\n",
      "\n",
      "Step 765: val_rewards = 0.35497373465720217 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 766: last loss = -0.16215\n",
      "eval step --\n",
      "\n",
      "Step 766: val_rewards = 0.4332418697979203 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 767: last loss = 0.03284\n",
      "eval step --\n",
      "\n",
      "Step 767: val_rewards = 0.2707506030256742 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 768: last loss = 0.20177\n",
      "eval step --\n",
      "\n",
      "Step 768: val_rewards = 0.24055819043423715 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 769: last loss = 0.05204\n",
      "eval step --\n",
      "\n",
      "Step 769: val_rewards = 0.26052983338082797 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 770: last loss = -0.15692\n",
      "eval step --\n",
      "\n",
      "Step 770: val_rewards = 0.3610955360523631 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 771: last loss = -0.50415\n",
      "eval step --\n",
      "\n",
      "Step 771: val_rewards = 0.3256519783174953 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 772: last loss = 0.22357\n",
      "eval step --\n",
      "\n",
      "Step 772: val_rewards = 0.37967421056916995 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 773: last loss = -0.75485\n",
      "eval step --\n",
      "\n",
      "Step 773: val_rewards = 0.34534908650601204 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 774: last loss = -0.07310\n",
      "eval step --\n",
      "\n",
      "Step 774: val_rewards = 0.3026565173790773 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 775: last loss = -0.10496\n",
      "eval step --\n",
      "\n",
      "Step 775: val_rewards = 0.318324875076725 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 776: last loss = -0.09934\n",
      "eval step --\n",
      "\n",
      "Step 776: val_rewards = 0.5450855211089181 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 777: last loss = 0.07485\n",
      "eval step --\n",
      "\n",
      "Step 777: val_rewards = 0.5372552494212046 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 778: last loss = -0.10018\n",
      "eval step --\n",
      "\n",
      "Step 778: val_rewards = 0.5151687201482297 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 779: last loss = -0.26577\n",
      "eval step --\n",
      "\n",
      "Step 779: val_rewards = 0.54848120305422 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 780: last loss = 0.13486\n",
      "eval step --\n",
      "\n",
      "Step 780: val_rewards = 0.5856804914650331 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 781: last loss = -0.93576\n",
      "eval step --\n",
      "\n",
      "Step 781: val_rewards = 0.511218454267514 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 782: last loss = -0.04498\n",
      "eval step --\n",
      "\n",
      "Step 782: val_rewards = 0.5235787930108421 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 783: last loss = -0.70922\n",
      "eval step --\n",
      "\n",
      "Step 783: val_rewards = 0.46180125077640316 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 784: last loss = 0.28592\n",
      "eval step --\n",
      "\n",
      "Step 784: val_rewards = 0.40052339834373624 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 785: last loss = -0.76275\n",
      "eval step --\n",
      "\n",
      "Step 785: val_rewards = 0.30920814618266884 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 786: last loss = -0.53307\n",
      "eval step --\n",
      "\n",
      "Step 786: val_rewards = 0.26601913380842385 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 787: last loss = -0.18818\n",
      "eval step --\n",
      "\n",
      "Step 787: val_rewards = 0.37810942888959875 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 788: last loss = 0.00873\n",
      "eval step --\n",
      "\n",
      "Step 788: val_rewards = 0.33412520754476566 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 789: last loss = -0.32977\n",
      "eval step --\n",
      "\n",
      "Step 789: val_rewards = 0.3815524397019525 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 790: last loss = -0.00839\n",
      "eval step --\n",
      "\n",
      "Step 790: val_rewards = 0.32550659927513015 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 791: last loss = 0.00302\n",
      "eval step --\n",
      "\n",
      "Step 791: val_rewards = 0.20960167720429082 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 792: last loss = -0.71986\n",
      "eval step --\n",
      "\n",
      "Step 792: val_rewards = 0.20951613703887265 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 793: last loss = -0.24008\n",
      "eval step --\n",
      "\n",
      "Step 793: val_rewards = 0.29366921554584335 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 794: last loss = 0.10378\n",
      "eval step --\n",
      "\n",
      "Step 794: val_rewards = 0.3119611821623267 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 795: last loss = -0.36315\n",
      "eval step --\n",
      "\n",
      "Step 795: val_rewards = 0.31189124527834555 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 796: last loss = 0.04259\n",
      "eval step --\n",
      "\n",
      "Step 796: val_rewards = 0.28730640426162124 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 797: last loss = -1.19300\n",
      "eval step --\n",
      "\n",
      "Step 797: val_rewards = 0.34213007837081183 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 798: last loss = -0.11826\n",
      "eval step --\n",
      "\n",
      "Step 798: val_rewards = 0.410370239978983 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 799: last loss = 0.24066\n",
      "eval step --\n",
      "\n",
      "Step 799: val_rewards = 0.35375524301095523 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 800: last loss = -0.18890\n",
      "eval step --\n",
      "\n",
      "Step 800: val_rewards = 0.21273471839710772 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 801: last loss = 0.10446\n",
      "eval step --\n",
      "\n",
      "Step 801: val_rewards = 0.4668840961671869 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 802: last loss = -0.05365\n",
      "eval step --\n",
      "\n",
      "Step 802: val_rewards = 0.47129290434282084 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 803: last loss = -0.43497\n",
      "eval step --\n",
      "\n",
      "Step 803: val_rewards = 0.4934732422520872 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 804: last loss = -0.13141\n",
      "eval step --\n",
      "\n",
      "Step 804: val_rewards = 0.4542121060664922 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 805: last loss = -0.03060\n",
      "eval step --\n",
      "\n",
      "Step 805: val_rewards = 0.5160978266828209 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 806: last loss = 0.14571\n",
      "eval step --\n",
      "\n",
      "Step 806: val_rewards = 0.6435656797488948 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 807: last loss = 0.14088\n",
      "eval step --\n",
      "\n",
      "Step 807: val_rewards = 0.6602075407161717 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 808: last loss = -0.43446\n",
      "eval step --\n",
      "\n",
      "Step 808: val_rewards = 0.39277578600437857 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 809: last loss = 0.06483\n",
      "eval step --\n",
      "\n",
      "Step 809: val_rewards = 0.36731696169465433 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 810: last loss = -0.61214\n",
      "eval step --\n",
      "\n",
      "Step 810: val_rewards = 0.44013189896429744 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 811: last loss = -0.14902\n",
      "eval step --\n",
      "\n",
      "Step 811: val_rewards = 0.4219718268924024 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 812: last loss = -0.97353\n",
      "eval step --\n",
      "\n",
      "Step 812: val_rewards = 0.43998101899699293 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 813: last loss = -0.72154\n",
      "eval step --\n",
      "\n",
      "Step 813: val_rewards = 0.3760356332911867 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 814: last loss = -0.20762\n",
      "eval step --\n",
      "\n",
      "Step 814: val_rewards = 0.37598670265462125 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 815: last loss = -0.20059\n",
      "eval step --\n",
      "\n",
      "Step 815: val_rewards = 0.4294797642780609 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 816: last loss = -0.53725\n",
      "eval step --\n",
      "\n",
      "Step 816: val_rewards = 0.36008850927242086 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 817: last loss = -0.08811\n",
      "eval step --\n",
      "\n",
      "Step 817: val_rewards = 0.3568262036734647 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 818: last loss = -0.07646\n",
      "eval step --\n",
      "\n",
      "Step 818: val_rewards = 0.3029126433505221 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 819: last loss = 0.11501\n",
      "eval step --\n",
      "\n",
      "Step 819: val_rewards = 0.27318061056682985 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 820: last loss = 0.04013\n",
      "eval step --\n",
      "\n",
      "Step 820: val_rewards = 0.25519450147725853 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 821: last loss = -0.02118\n",
      "eval step --\n",
      "\n",
      "Step 821: val_rewards = 0.25482092998952227 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 822: last loss = -0.94951\n",
      "eval step --\n",
      "\n",
      "Step 822: val_rewards = 0.3652455440705239 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 823: last loss = -0.25685\n",
      "eval step --\n",
      "\n",
      "Step 823: val_rewards = 0.47473057750216524 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 824: last loss = -0.79077\n",
      "eval step --\n",
      "\n",
      "Step 824: val_rewards = 0.3838781548548024 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 825: last loss = -0.25846\n",
      "eval step --\n",
      "\n",
      "Step 825: val_rewards = 0.2870024155168421 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 826: last loss = -0.83128\n",
      "eval step --\n",
      "\n",
      "Step 826: val_rewards = 0.2676940673651604 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 827: last loss = -0.12751\n",
      "eval step --\n",
      "\n",
      "Step 827: val_rewards = 0.2853031155649093 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 828: last loss = -0.03375\n",
      "eval step --\n",
      "\n",
      "Step 828: val_rewards = 0.32443960943373773 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 829: last loss = -0.07255\n",
      "eval step --\n",
      "\n",
      "Step 829: val_rewards = 0.3255780683168013 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 830: last loss = -1.30733\n",
      "eval step --\n",
      "\n",
      "Step 830: val_rewards = 0.37488574006138864 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 831: last loss = -0.04895\n",
      "eval step --\n",
      "\n",
      "Step 831: val_rewards = 0.37506972934144256 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 832: last loss = -0.16223\n",
      "eval step --\n",
      "\n",
      "Step 832: val_rewards = 0.2697240774828888 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 833: last loss = -0.40749\n",
      "eval step --\n",
      "\n",
      "Step 833: val_rewards = 0.2550450348406054 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 834: last loss = 0.07764\n",
      "eval step --\n",
      "\n",
      "Step 834: val_rewards = 0.24705063956059312 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 835: last loss = -1.25881\n",
      "eval step --\n",
      "\n",
      "Step 835: val_rewards = 0.25872125920031813 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 836: last loss = -0.88451\n",
      "eval step --\n",
      "\n",
      "Step 836: val_rewards = 0.18270283217703104 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 837: last loss = 0.04278\n",
      "eval step --\n",
      "\n",
      "Step 837: val_rewards = 0.18295974851694785 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 838: last loss = -0.06972\n",
      "eval step --\n",
      "\n",
      "Step 838: val_rewards = 0.2996292538737858 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 839: last loss = 0.18677\n",
      "eval step --\n",
      "\n",
      "Step 839: val_rewards = 0.25554618160012604 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 840: last loss = -0.01024\n",
      "eval step --\n",
      "\n",
      "Step 840: val_rewards = 0.2563917501171332 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 841: last loss = -0.42686\n",
      "eval step --\n",
      "\n",
      "Step 841: val_rewards = 0.38067873326832846 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 842: last loss = -0.00214\n",
      "eval step --\n",
      "\n",
      "Step 842: val_rewards = 0.2619598478077816 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 843: last loss = -1.14786\n",
      "eval step --\n",
      "\n",
      "Step 843: val_rewards = 0.31963502644721564 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 844: last loss = -0.06522\n",
      "eval step --\n",
      "\n",
      "Step 844: val_rewards = 0.2845597883099539 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 845: last loss = -0.59451\n",
      "eval step --\n",
      "\n",
      "Step 845: val_rewards = 0.2707098874786767 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 846: last loss = -0.02135\n",
      "eval step --\n",
      "\n",
      "Step 846: val_rewards = 0.3771201036649343 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 847: last loss = -0.26022\n",
      "eval step --\n",
      "\n",
      "Step 847: val_rewards = 0.33282106250574994 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 848: last loss = -0.07895\n",
      "eval step --\n",
      "\n",
      "Step 848: val_rewards = 0.36708992150806397 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 849: last loss = -0.04796\n",
      "eval step --\n",
      "\n",
      "Step 849: val_rewards = 0.3442451051602875 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 850: last loss = -0.75482\n",
      "eval step --\n",
      "\n",
      "Step 850: val_rewards = 0.35770938303482175 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 851: last loss = -0.09546\n",
      "eval step --\n",
      "\n",
      "Step 851: val_rewards = 0.321409943021259 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 852: last loss = -0.38437\n",
      "eval step --\n",
      "\n",
      "Step 852: val_rewards = 0.2206220996332787 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 853: last loss = -0.17331\n",
      "eval step --\n",
      "\n",
      "Step 853: val_rewards = 0.2207047740922902 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 854: last loss = 0.09131\n",
      "eval step --\n",
      "\n",
      "Step 854: val_rewards = 0.2242544832338341 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 855: last loss = -0.63859\n",
      "eval step --\n",
      "\n",
      "Step 855: val_rewards = 0.2618924648243694 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 856: last loss = -1.02536\n",
      "eval step --\n",
      "\n",
      "Step 856: val_rewards = 0.2778980652840529 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 857: last loss = -0.40363\n",
      "eval step --\n",
      "\n",
      "Step 857: val_rewards = 0.25328448996053776 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 858: last loss = -0.49743\n",
      "eval step --\n",
      "\n",
      "Step 858: val_rewards = 0.18246689827554197 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 859: last loss = -0.03576\n",
      "eval step --\n",
      "\n",
      "Step 859: val_rewards = 0.15113943817042302 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 860: last loss = -0.09840\n",
      "eval step --\n",
      "\n",
      "Step 860: val_rewards = 0.16515429513272534 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 861: last loss = -0.49173\n",
      "eval step --\n",
      "\n",
      "Step 861: val_rewards = 0.1912205975171461 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 862: last loss = -0.15746\n",
      "eval step --\n",
      "\n",
      "Step 862: val_rewards = 0.23084842152069934 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 863: last loss = -0.56481\n",
      "eval step --\n",
      "\n",
      "Step 863: val_rewards = 0.2312830426682001 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 864: last loss = -0.10388\n",
      "eval step --\n",
      "\n",
      "Step 864: val_rewards = 0.2571775248622317 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 865: last loss = -0.83429\n",
      "eval step --\n",
      "\n",
      "Step 865: val_rewards = 0.24180077398814495 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 866: last loss = -0.14933\n",
      "eval step --\n",
      "\n",
      "Step 866: val_rewards = 0.21325510965771116 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 867: last loss = -0.42275\n",
      "eval step --\n",
      "\n",
      "Step 867: val_rewards = 0.2053798037842201 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 868: last loss = -0.77283\n",
      "eval step --\n",
      "\n",
      "Step 868: val_rewards = 0.2304620654296494 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 869: last loss = -0.22183\n",
      "eval step --\n",
      "\n",
      "Step 869: val_rewards = 0.3956563828423988 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 870: last loss = 0.10418\n",
      "eval step --\n",
      "\n",
      "Step 870: val_rewards = 0.35065812431629945 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 871: last loss = -0.57217\n",
      "eval step --\n",
      "\n",
      "Step 871: val_rewards = 0.3875904604784318 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 872: last loss = 0.04111\n",
      "eval step --\n",
      "\n",
      "Step 872: val_rewards = 0.33405409927819785 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 873: last loss = -0.82376\n",
      "eval step --\n",
      "\n",
      "Step 873: val_rewards = 0.35975336188554496 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 874: last loss = -0.49368\n",
      "eval step --\n",
      "\n",
      "Step 874: val_rewards = 0.22702754902256683 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 875: last loss = -1.13205\n",
      "eval step --\n",
      "\n",
      "Step 875: val_rewards = 0.28342986812428794 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 876: last loss = -0.30330\n",
      "eval step --\n",
      "\n",
      "Step 876: val_rewards = 0.3200592871066465 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 877: last loss = 0.00271\n",
      "eval step --\n",
      "\n",
      "Step 877: val_rewards = 0.23725635660891028 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 878: last loss = -0.21921\n",
      "eval step --\n",
      "\n",
      "Step 878: val_rewards = 0.14687202014667297 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 879: last loss = -1.17742\n",
      "eval step --\n",
      "\n",
      "Step 879: val_rewards = 0.14598454951924986 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 880: last loss = -0.21824\n",
      "eval step --\n",
      "\n",
      "Step 880: val_rewards = 0.17744861994014013 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 881: last loss = -0.82598\n",
      "eval step --\n",
      "\n",
      "Step 881: val_rewards = 0.123590927443733 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 882: last loss = -0.09783\n",
      "eval step --\n",
      "\n",
      "Step 882: val_rewards = 0.17541081424217936 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 883: last loss = -0.73108\n",
      "eval step --\n",
      "\n",
      "Step 883: val_rewards = 0.21989638055831087 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 884: last loss = -0.67638\n",
      "eval step --\n",
      "\n",
      "Step 884: val_rewards = 0.16411938179752436 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 885: last loss = -0.29517\n",
      "eval step --\n",
      "\n",
      "Step 885: val_rewards = 0.17474292482601297 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 886: last loss = -0.09941\n",
      "eval step --\n",
      "\n",
      "Step 886: val_rewards = 0.23175605394226811 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 887: last loss = 0.08442\n",
      "eval step --\n",
      "\n",
      "Step 887: val_rewards = 0.2591008861892683 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 888: last loss = -0.04548\n",
      "eval step --\n",
      "\n",
      "Step 888: val_rewards = 0.25485785860035876 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 889: last loss = -0.82540\n",
      "eval step --\n",
      "\n",
      "Step 889: val_rewards = 0.3082781517374667 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 890: last loss = -0.07891\n",
      "eval step --\n",
      "\n",
      "Step 890: val_rewards = 0.28551313572814024 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 891: last loss = -1.32042\n",
      "eval step --\n",
      "\n",
      "Step 891: val_rewards = 0.26267712660576886 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 892: last loss = -1.35517\n",
      "eval step --\n",
      "\n",
      "Step 892: val_rewards = 0.23269384780454275 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 893: last loss = -0.78361\n",
      "eval step --\n",
      "\n",
      "Step 893: val_rewards = 0.25222412581805836 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 894: last loss = -0.58883\n",
      "eval step --\n",
      "\n",
      "Step 894: val_rewards = 0.2938784837405971 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 895: last loss = -0.10591\n",
      "eval step --\n",
      "\n",
      "Step 895: val_rewards = 0.27572420121172747 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 896: last loss = 0.05639\n",
      "eval step --\n",
      "\n",
      "Step 896: val_rewards = 0.2757051486839904 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 897: last loss = -0.11513\n",
      "eval step --\n",
      "\n",
      "Step 897: val_rewards = 0.24912618662267869 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 898: last loss = -0.13245\n",
      "eval step --\n",
      "\n",
      "Step 898: val_rewards = 0.2944140116203138 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 899: last loss = -0.29234\n",
      "eval step --\n",
      "\n",
      "Step 899: val_rewards = 0.294396696215818 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 900: last loss = -0.69285\n",
      "eval step --\n",
      "\n",
      "Step 900: val_rewards = 0.2943976311247448 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 901: last loss = -0.10247\n",
      "eval step --\n",
      "\n",
      "Step 901: val_rewards = 0.2575436846983439 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 902: last loss = 0.17714\n",
      "eval step --\n",
      "\n",
      "Step 902: val_rewards = 0.19061740346252315 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 903: last loss = -0.66282\n",
      "eval step --\n",
      "\n",
      "Step 903: val_rewards = 0.21533745124500053 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 904: last loss = -0.14101\n",
      "eval step --\n",
      "\n",
      "Step 904: val_rewards = 0.2242635368743919 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 905: last loss = 0.06749\n",
      "eval step --\n",
      "\n",
      "Step 905: val_rewards = 0.22423123348498764 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 906: last loss = -0.00982\n",
      "eval step --\n",
      "\n",
      "Step 906: val_rewards = 0.22827357469462922 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 907: last loss = -0.00708\n",
      "eval step --\n",
      "\n",
      "Step 907: val_rewards = 0.20771105367585171 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 908: last loss = -0.06960\n",
      "eval step --\n",
      "\n",
      "Step 908: val_rewards = 0.2052629075173689 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 909: last loss = -0.10347\n",
      "eval step --\n",
      "\n",
      "Step 909: val_rewards = 0.21968599795264665 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 910: last loss = 0.35082\n",
      "eval step --\n",
      "\n",
      "Step 910: val_rewards = 0.22462028209961873 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 911: last loss = 0.11086\n",
      "eval step --\n",
      "\n",
      "Step 911: val_rewards = 0.2921392255137902 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 912: last loss = -1.05636\n",
      "eval step --\n",
      "\n",
      "Step 912: val_rewards = 0.292316569600024 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 913: last loss = -0.77054\n",
      "eval step --\n",
      "\n",
      "Step 913: val_rewards = 0.33391593706642947 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 914: last loss = -0.04560\n",
      "eval step --\n",
      "\n",
      "Step 914: val_rewards = 0.30267513828601816 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 915: last loss = -0.22058\n",
      "eval step --\n",
      "\n",
      "Step 915: val_rewards = 0.29635227197692104 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 916: last loss = -0.56774\n",
      "eval step --\n",
      "\n",
      "Step 916: val_rewards = 0.2988403566056797 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 917: last loss = -0.18007\n",
      "eval step --\n",
      "\n",
      "Step 917: val_rewards = 0.26500737348983056 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 918: last loss = -0.87789\n",
      "eval step --\n",
      "\n",
      "Step 918: val_rewards = 0.2578336400612798 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 919: last loss = -0.20236\n",
      "eval step --\n",
      "\n",
      "Step 919: val_rewards = 0.22838462211037164 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 920: last loss = -0.97775\n",
      "eval step --\n",
      "\n",
      "Step 920: val_rewards = 0.2642616482327425 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 921: last loss = -0.38465\n",
      "eval step --\n",
      "\n",
      "Step 921: val_rewards = 0.2597487081541979 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 922: last loss = -1.14236\n",
      "eval step --\n",
      "\n",
      "Step 922: val_rewards = 0.2684356861803749 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 923: last loss = -0.10051\n",
      "eval step --\n",
      "\n",
      "Step 923: val_rewards = 0.1065594927463666 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 924: last loss = -0.86176\n",
      "eval step --\n",
      "\n",
      "Step 924: val_rewards = 0.25122274337167383 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 925: last loss = -0.16807\n",
      "eval step --\n",
      "\n",
      "Step 925: val_rewards = 0.1545264441436742 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 926: last loss = -0.11431\n",
      "eval step --\n",
      "\n",
      "Step 926: val_rewards = 0.09848127591163466 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 927: last loss = -1.06228\n",
      "eval step --\n",
      "\n",
      "Step 927: val_rewards = 0.5608966669220977 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 928: last loss = -0.73319\n",
      "eval step --\n",
      "\n",
      "Step 928: val_rewards = 1.2920660857784656 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 929: last loss = -0.26157\n",
      "eval step --\n",
      "\n",
      "Step 929: val_rewards = 0.64073414843601 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 930: last loss = -0.04271\n",
      "eval step --\n",
      "\n",
      "Step 930: val_rewards = 0.3246894954500527 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 931: last loss = -0.66168\n",
      "eval step --\n",
      "\n",
      "Step 931: val_rewards = 0.6204064391320873 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 932: last loss = -0.29512\n",
      "eval step --\n",
      "\n",
      "Step 932: val_rewards = 0.5725598516038921 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 933: last loss = -0.34081\n",
      "eval step --\n",
      "\n",
      "Step 933: val_rewards = 0.6713353738186055 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 934: last loss = -0.15705\n",
      "eval step --\n",
      "\n",
      "Step 934: val_rewards = 0.5045241697669329 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 935: last loss = -1.00054\n",
      "eval step --\n",
      "\n",
      "Step 935: val_rewards = 0.41381876286726604 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 936: last loss = -1.02923\n",
      "eval step --\n",
      "\n",
      "Step 936: val_rewards = 0.17390577408080468 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 937: last loss = -0.31576\n",
      "eval step --\n",
      "\n",
      "Step 937: val_rewards = -0.1315434768369181 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 938: last loss = 0.00953\n",
      "eval step --\n",
      "\n",
      "Step 938: val_rewards = -0.12896523525899958 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 939: last loss = -0.45562\n",
      "eval step --\n",
      "\n",
      "Step 939: val_rewards = 0.14884033145927159 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 940: last loss = -0.43577\n",
      "eval step --\n",
      "\n",
      "Step 940: val_rewards = -0.21818336452678402 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 941: last loss = -0.93333\n",
      "eval step --\n",
      "\n",
      "Step 941: val_rewards = -0.10033717901619425 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 942: last loss = 0.04449\n",
      "eval step --\n",
      "\n",
      "Step 942: val_rewards = -0.18617562995920467 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 943: last loss = -0.21346\n",
      "eval step --\n",
      "\n",
      "Step 943: val_rewards = -0.15288406837831608 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 944: last loss = -0.77977\n",
      "eval step --\n",
      "\n",
      "Step 944: val_rewards = -0.23292080485835073 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 945: last loss = -0.84623\n",
      "eval step --\n",
      "\n",
      "Step 945: val_rewards = 0.49724606631400176 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 946: last loss = -0.10524\n",
      "eval step --\n",
      "\n",
      "Step 946: val_rewards = 0.47749859059958233 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 947: last loss = -0.17362\n",
      "eval step --\n",
      "\n",
      "Step 947: val_rewards = 0.45482687014238676 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 948: last loss = -0.53306\n",
      "eval step --\n",
      "\n",
      "Step 948: val_rewards = 0.6292460047967197 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 949: last loss = -0.73290\n",
      "eval step --\n",
      "\n",
      "Step 949: val_rewards = 0.7060616559512192 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 950: last loss = 0.01599\n",
      "eval step --\n",
      "\n",
      "Step 950: val_rewards = 0.8553290119906392 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 951: last loss = 0.07256\n",
      "eval step --\n",
      "\n",
      "Step 951: val_rewards = 0.16123553922028225 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 952: last loss = -0.08405\n",
      "eval step --\n",
      "\n",
      "Step 952: val_rewards = 0.9502767260276233 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 953: last loss = -0.63400\n",
      "eval step --\n",
      "\n",
      "Step 953: val_rewards = 0.6882584605840658 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 954: last loss = -0.59730\n",
      "eval step --\n",
      "\n",
      "Step 954: val_rewards = 0.4476295258718967 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 955: last loss = 0.01150\n",
      "eval step --\n",
      "\n",
      "Step 955: val_rewards = 0.3203483808431652 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 956: last loss = -0.88740\n",
      "eval step --\n",
      "\n",
      "Step 956: val_rewards = 0.38492826674892533 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 957: last loss = -0.74225\n",
      "eval step --\n",
      "\n",
      "Step 957: val_rewards = 0.39921635419918294 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 958: last loss = -0.05013\n",
      "eval step --\n",
      "\n",
      "Step 958: val_rewards = 0.4437840168894452 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 959: last loss = 0.19722\n",
      "eval step --\n",
      "\n",
      "Step 959: val_rewards = 0.39845734998976806 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 960: last loss = 0.10954\n",
      "eval step --\n",
      "\n",
      "Step 960: val_rewards = 0.7532659277232786 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 961: last loss = -0.02268\n",
      "eval step --\n",
      "\n",
      "Step 961: val_rewards = 0.9197136411483738 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 962: last loss = -0.09834\n",
      "eval step --\n",
      "\n",
      "Step 962: val_rewards = 0.996729045399965 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 963: last loss = -0.17318\n",
      "eval step --\n",
      "\n",
      "Step 963: val_rewards = 0.8027181479111712 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 964: last loss = -0.00439\n",
      "eval step --\n",
      "\n",
      "Step 964: val_rewards = 0.7978385334902244 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 965: last loss = -0.57995\n",
      "eval step --\n",
      "\n",
      "Step 965: val_rewards = 0.774150418816588 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 966: last loss = -0.47145\n",
      "eval step --\n",
      "\n",
      "Step 966: val_rewards = 0.7409466190883477 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 967: last loss = -0.61029\n",
      "eval step --\n",
      "\n",
      "Step 967: val_rewards = 0.7783939244025704 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 968: last loss = -0.57374\n",
      "eval step --\n",
      "\n",
      "Step 968: val_rewards = 0.799732510064519 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 969: last loss = -0.15412\n",
      "eval step --\n",
      "\n",
      "Step 969: val_rewards = 0.9222644376160477 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 970: last loss = 0.03870\n",
      "eval step --\n",
      "\n",
      "Step 970: val_rewards = 0.9486166806481723 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 971: last loss = -0.48428\n",
      "eval step --\n",
      "\n",
      "Step 971: val_rewards = 0.8467091823965086 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 972: last loss = 0.00339\n",
      "eval step --\n",
      "\n",
      "Step 972: val_rewards = 0.8470239456748128 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 973: last loss = -0.33522\n",
      "eval step --\n",
      "\n",
      "Step 973: val_rewards = 0.826148555173563 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 974: last loss = -0.54357\n",
      "eval step --\n",
      "\n",
      "Step 974: val_rewards = 0.8247988520623266 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 975: last loss = -0.06720\n",
      "eval step --\n",
      "\n",
      "Step 975: val_rewards = 0.8236951802407433 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 976: last loss = -0.06560\n",
      "eval step --\n",
      "\n",
      "Step 976: val_rewards = 0.8207206659223899 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 977: last loss = -0.00810\n",
      "eval step --\n",
      "\n",
      "Step 977: val_rewards = 0.8201379936651204 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 978: last loss = -0.64314\n",
      "eval step --\n",
      "\n",
      "Step 978: val_rewards = 0.8194943183299741 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 979: last loss = -0.48294\n",
      "eval step --\n",
      "\n",
      "Step 979: val_rewards = 0.7952190132928321 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 980: last loss = 0.00293\n",
      "eval step --\n",
      "\n",
      "Step 980: val_rewards = 0.7947581797525187 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 981: last loss = -0.01440\n",
      "eval step --\n",
      "\n",
      "Step 981: val_rewards = 0.7947477975273262 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 982: last loss = 0.05571\n",
      "eval step --\n",
      "\n",
      "Step 982: val_rewards = 0.7947813419086545 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 983: last loss = -0.27790\n",
      "eval step --\n",
      "\n",
      "Step 983: val_rewards = 0.7946231340494956 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 984: last loss = -0.03436\n",
      "eval step --\n",
      "\n",
      "Step 984: val_rewards = 0.7675457892041061 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 985: last loss = -0.12744\n",
      "eval step --\n",
      "\n",
      "Step 985: val_rewards = 0.7674118654112796 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 986: last loss = -0.02543\n",
      "eval step --\n",
      "\n",
      "Step 986: val_rewards = 0.7672944713889993 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 987: last loss = -0.27431\n",
      "eval step --\n",
      "\n",
      "Step 987: val_rewards = 0.7671866788371535 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 988: last loss = 0.06836\n",
      "eval step --\n",
      "\n",
      "Step 988: val_rewards = 0.7670579488976369 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 989: last loss = -0.08511\n",
      "eval step --\n",
      "\n",
      "Step 989: val_rewards = 0.7649066598462009 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 990: last loss = -0.71555\n",
      "eval step --\n",
      "\n",
      "Step 990: val_rewards = 0.7648090498051773 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 991: last loss = -0.00080\n",
      "eval step --\n",
      "\n",
      "Step 991: val_rewards = 0.7647276602129557 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 992: last loss = -0.21493\n",
      "eval step --\n",
      "\n",
      "Step 992: val_rewards = 0.764659356857236 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 993: last loss = -0.48922\n",
      "eval step --\n",
      "\n",
      "Step 993: val_rewards = 0.7645774861592104 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 994: last loss = -0.22602\n",
      "eval step --\n",
      "\n",
      "Step 994: val_rewards = 0.7645229812964643 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 995: last loss = -0.05952\n",
      "eval step --\n",
      "\n",
      "Step 995: val_rewards = 0.764480939553867 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 996: last loss = -0.51498\n",
      "eval step --\n",
      "\n",
      "Step 996: val_rewards = 0.7644472887046325 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 997: last loss = 0.01489\n",
      "eval step --\n",
      "\n",
      "Step 997: val_rewards = 0.764399655655221 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 998: last loss = -0.67348\n",
      "eval step --\n",
      "\n",
      "Step 998: val_rewards = 0.819841832609436 | baseline_reward = 1.1209681893688832\n",
      "\n",
      "-------------------------------------\n",
      "training model --\n",
      "Step 999: last loss = -0.02121\n",
      "eval step --\n",
      "\n",
      "Step 999: val_rewards = 0.8196751884907942 | baseline_reward = 1.1209681893688832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 14:04:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = f\"v4_training_{tid}\") as run:\n",
    "    params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"train_step\": train_step,\n",
    "            \"eval_step\": eval_step,\n",
    "            \"metric_function\": 'sharpe',\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \n",
    "            \"symbol_universe\" : symbol_universe,\n",
    "            \"feature_set\" : feature_set,\n",
    "            \"d_model\" : d_model,\n",
    "            \"nheads\" : nheads,\n",
    "            \"num_transformer_layers\" : num_transformer_layers,\n",
    "\n",
    "            \"episode_duration\" : 12,    \n",
    "            \"holding_period\" : 1,\n",
    "            \"train_test_split\" : 0.8,\n",
    "            \"symbol_universe\" : symbol_universe,\n",
    "            \"feature_set\" : feature_set,\n",
    "\n",
    "        }\n",
    "    # Log training parameters.\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    portfolio_constructor = PortfolioConstructor(\n",
    "        device = device,\n",
    "        symbol_universe= params['symbol_universe'],\n",
    "        num_features= len(params['feature_set']),\n",
    "        d_model = params['d_model'],\n",
    "        nheads = params['nheads'],\n",
    "        num_transformer_layers = params['num_transformer_layers'],\n",
    "    )\n",
    "\n",
    "    market_env = MarketEnvironment(\n",
    "        device = device,\n",
    "        data_path = data_path,\n",
    "        holding_period = params['holding_period'],\n",
    "        episode_duration = params['episode_duration'],\n",
    "        train_test_split = params['train_test_split'],\n",
    "        symbol_universe = params['symbol_universe'],\n",
    "        feature_set = params['feature_set']\n",
    "        )\n",
    "\n",
    "    portfolio_constructor.cuda()\n",
    "    portfolio_constructor.train()\n",
    "    market_env.reset(mode = \"train\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(portfolio_constructor.parameters(), lr = learning_rate)\n",
    "\n",
    "    max_reward = -1\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        is_end = False\n",
    "        returns = []\n",
    "        tran_costs = []\n",
    "        nlls = []\n",
    "        all_allocations = []\n",
    "\n",
    "        market_env.reset(mode = \"train\", transaction_cost= 1e-7)\n",
    "        state = market_env.get_state()\n",
    "        \n",
    "        while not is_end:\n",
    "            symbol_idx, allocations = portfolio_constructor(state)\n",
    "            state, return_, _, is_end, tran_cost = market_env.step(allocations)\n",
    "\n",
    "            all_allocations.append(allocations)\n",
    "            returns.append(return_)\n",
    "            tran_costs.append(tran_cost)\n",
    "\n",
    "        sharp_ratio = sharp_ratio_(returns, tran_costs)\n",
    "\n",
    "        loss = -sharp_ratio\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        if (episode + 1) % train_step == 0:\n",
    "\n",
    "                    print(\"-------------------------------------\")\n",
    "                    print(\"training model --\")\n",
    "                    print('Step {}: last loss = {:.5f}\\r'.format(episode, loss), end='')\n",
    "                    print()\n",
    "                    mlflow.log_metric(\"train loss\", f\"{loss:2f}\", step=episode)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = 0\n",
    "                    \n",
    "        if (episode + 1) % eval_step == 0:\n",
    "            print(\"eval step --\")\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                portfolio_constructor.eval()\n",
    "                reward_val, baseline_val, portfolio_constructor = evaluate(portfolio_constructor, market_env)\n",
    "                portfolio_constructor.train()\n",
    "\n",
    "                print('Step {}: val_rewards = {} | baseline_reward = {}'.format(episode, reward_val, baseline_val))\n",
    "                mlflow.log_metric(\"eval_sharpe\", f\"{reward_val:2f}\", step=episode)\n",
    "                mlflow.log_metric(\"baseline_sharpe\", f\"{baseline_val:2f}\", step=episode)\n",
    "\n",
    "                if max_reward < reward_val:\n",
    "                    max_reward = reward_val\n",
    "\n",
    "                    print(\"*** found better model ***\")\n",
    "                print()\n",
    "    mlflow.pytorch.log_model(portfolio_constructor, f\"portfolio_constructor_{tid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1013, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1002, 0.0000, 0.0000, 0.0000, 0.0987, 0.0000, 0.1000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0997, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0994, 0.0000, 0.0979, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1006, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1027, 0.0000, 0.0994, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0996, 0.0000, 0.0000, 0.0000, 0.0992, 0.0000, 0.0995, 0.0000,\n",
       "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0990, 0.0000, 0.0000, 0.0000, 0.0998, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1006, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1011, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1013], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0998, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0999, 0.0000, 0.0000, 0.0000, 0.0996, 0.0000, 0.0997, 0.0000,\n",
       "         0.0000, 0.0000, 0.0998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0994, 0.0000, 0.0000, 0.0000, 0.1002, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1007, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1003], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0999, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0996, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1001, 0.0997,\n",
       "         0.0000, 0.0000, 0.1001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1006, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0995, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1004, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1000], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0999, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0995, 0.0000, 0.0000, 0.0000, 0.0992, 0.0000, 0.0995, 0.0000,\n",
       "         0.0000, 0.0000, 0.0998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1015, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1001, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0992, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1001], device='cuda:0', grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_allocations[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naradaw/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/11 10:47:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'portfolio-constructor-v3'.\n",
      "Created version '1' of model 'portfolio-constructor-v3'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7fc41191ac70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.pytorch.log_model(\n",
    "        pytorch_model=portfolio_constructor,\n",
    "        artifact_path = \"portfolio_constructor_{tid}\",\n",
    "        # input_example = market_env.get_random_state(),\n",
    "        registered_model_name=\"portfolio-constructor-v3\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
